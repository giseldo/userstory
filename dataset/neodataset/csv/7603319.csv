issuekey,created,title,description,storypoints
80284911,2021-03-04 21:55:13.432,Meltano Invoke/Select does not invalidate catalog cache after reinstalling a tap,"<!---
Please read this!

Before opening a new issue, make sure to search for keywords in the issues
filtered by the ""regression"" or ""bug"" label and verify the issue you're about to submit isn't a duplicate.

If you are submitting an issue with a tap, please include:

- account details
- target details
- entities selected  with meltano select (if you have selected any entities), as the bug may be related to a specific entity
- the full elt command you are running
- full output of the meltano elt command. Logs can get pretty long, so you can add the full log as a snippet in the Meltano project and add a link in the issue.
--->

### What is the current *bug* behavior?
Taps are using cached properties for `meltano invoke/select` even if the tap's properties have changed after `meltano install`.



### What is the expected *correct* behavior?
When running `meltano install`, a tap's properties cache should be invalidated so that `meltano invoke/select` should effectively ""rediscover"" the tap's properties.



### Steps to reproduce
```
# install all taps
1. meltano install
2. meltano invoke tap-custom-tap
# update tap-custom-tap repo with some new schema properties
3. meltano install tap-custom-tap
4. meltano invoke tap-custom-tap
```


### Relevant logs and/or screenshots
After running `meltano install extractor tap-s3-toast`
```
> meltano --log-level=debug invoke tap-s3-toast
[2021-03-04 21:28:53,618] [1|MainThread|root] [DEBUG] Creating engine <meltano.core.project.Project object at 0x7f510102d160>@sqlite:////projects/.meltano/meltano.db
[2021-03-04 21:28:54,642] [1|MainThread|root] [DEBUG] Created configuration at /projects/.meltano/run/tap-s3-toast/tap.config.json
[2021-03-04 21:28:54,643] [1|MainThread|root] [DEBUG] Could not find tap.properties.json in /projects/.meltano/extractors/tap-s3-toast/tap.properties.json, skipping.
[2021-03-04 21:28:54,643] [1|MainThread|root] [DEBUG] Could not find tap.properties.cache_key in /projects/.meltano/extractors/tap-s3-toast/tap.properties.cache_key, skipping.
[2021-03-04 21:28:54,643] [1|MainThread|root] [DEBUG] Could not find state.json in /projects/.meltano/extractors/tap-s3-toast/state.json, skipping.
[2021-03-04 21:28:54,643] [1|MainThread|meltano.core.plugin.singer.tap] [DEBUG] Using cached catalog file
```

### Possible fixes
According to @DouweM and my [Slack conversation](https://meltano.slack.com/archives/C013EKWA2Q1/p1614892532050300), `meltano invoke/select` should have some logic to invalidate caches. Other options are removing the cache when running `meltano install` either by default or with an optional cli flag.



### Further regression test
_Ensure we automatically catch similar issues in the future_

- [ ] Write additional adequate test cases and submit test results
- [ ] Test results should be reviewed by a person from the team",4
79729839,2021-02-24 16:52:39.171,Add Superset as a plugin,"- https://superset.apache.org/
- https://github.com/apache/superset
- https://pypi.org/project/apache-superset

```bash
meltano add analyzer superset

meltano invoke superset db upgrade # Run automatically?
meltano invoke superset fab create-admin
meltano invoke superset init # Run automatically?
meltano invoke superset run
```

Meltano can manage Superset configuration (https://superset.apache.org/docs/installation/configuring-superset) by allowing values to be set for the keys in https://github.com/apache/superset/blob/master/superset/config.py, automatically generating `superset_config.py`, and pointing Superset there by using the `SUPERSET_CONFIG_PATH` env var. Users should also be able to set `SUPERSET_CONFIG_PATH` (or `meltano config superset set config_path <path>`) themselves to use their own config file.

Ideally, Meltano would also be able to inject database connection strings corresponding to loaders directly into Superset so that these don't need to be managed in two places: https://superset.apache.org/docs/databases/installing-database-drivers, https://superset.apache.org/docs/databases/postgres. Possibly through the `DB_CONNECTION_MUTATOR` setting? https://github.com/apache/superset/issues/9045",8
79000530,2021-02-12 20:43:52.805,Let plugin `pip_url` take into account Python version,"The `pip_url` for `airflow` is currently `apache-airflow==1.10.14 --constraint https://raw.githubusercontent.com/apache/airflow/constraints-1.10.14/constraints-3.6.txt`, using a constraints file built specifically for Python 3.6, regardless of the version that's actually used.

With Airflow 1.10.14, this is OK, because the constraints file is actually valid on 3.7 and 3.8 as well, but this is not the case for Airflow 2.0.1's https://raw.githubusercontent.com/apache/airflow/constraints-2.0.1/constraints-3.6.txt.

This can be seen in https://gitlab.com/meltano/meltano/-/merge_requests/2032 and pipelines https://gitlab.com/michelrado/meltano/-/jobs/1026252711 (3.7) and https://gitlab.com/michelrado/meltano/-/jobs/1025856502 (3.8).

One way to solve this would be to let a `PYTHON_VERSION` environment variable to referenced from `pip_url`, like so:

```yaml
pip_url: 'apache-airflow==2.0.1 --constraint https://raw.githubusercontent.com/apache/airflow/constraints-2.0.1/constraints-${PYTHON_VERSION}.txt'
```",4
78832542,2021-02-10 16:46:47.822,Document how to manage incremental replication state without a persistent system database,"As I wrote on Slack:

>>>
Note that there's already a workaround for state management with ephemeral system databases: since `meltano elt` (and `meltano schedule run`) can take a `--state` argument to run with a specific state, and can be run with `--dump=state` to dump the current state instead of running the underlying pipeline, a serverless ELT wrapper can:
1. download the state from S3
2. run the pipeline with `--state=<downloaded state>`
3. dump the current state (from the local ephemeral system database) to a file with `--dump=state > new-state.json`
4. upload the new state to S3
>>>",4
77710021,2021-01-22 22:00:04.111,Pipelines UI should let an arbitrary cron expression be chosen as a schedule interval,"`meltano schedule` already supports arbitrary cron expressions, e.g. `0 */6 * * *` to mean ""every 6 hours"": https://meltano.com/docs/command-line-interface.html#schedule

The UI should support this as well, both when creating a new pipeline and editing an existing one.

At the same time, we should show these intervals correctly: https://gitlab.com/meltano/meltano/-/issues/2527

Update (2021-11-29): To provide a good user experience, I (AJ) think we should also preview the ""next 10 occurrences"" (or similar) so the user can ensure the cron expression is valid and meets their intended schedule.",8
77189415,2021-01-13 22:09:47.612,Remove flakehell now that it's no longer being maintained,"https://github.com/life4/flakehell was added in https://gitlab.com/meltano/meltano/-/merge_requests/1970, but the maintainer has now archived the project: https://github.com/wemake-services/wemake-python-styleguide/issues/1817.

Since it's no longer being maintained, we should move away from it and start using `flake8 --diff` directly.
The docs at https://wemake-python-stylegui.de/en/latest/pages/usage/integrations/legacy.html will likely come in handy as well.",4
76599958,2020-12-30 23:23:54.256,Reuse parent plugin installation directory using `inherit_from` and matching `pip_url` (prevent venv duplication),"Original Description

<details><summary>Click to expand</summary>

When a plugin inherits from another, both get their packages installed into their own venvs at `.meltano/{type}s/{name}` (e.g. `.meltano/extractors/tap-gitlab` and `.meltano/extractors/tap-gitlab--inherited`), even though they could reuse the same package on-disk, which would speed up `meltano install` and reduce the installation footprint.

One way to accomplish this would be to have plugins that share a `pip_url` share an installation directory, by using `.meltano/{type}s/{pip_url}` as the path, with `pip_url` hashed or transformed to a valid directory name. The resulting dirnames are pretty unwieldy though, and this would make it much harder to find a given plugin's installation directory just by browsing the filesystem and looking for its name. Adding a command to print the path (https://gitlab.com/meltano/meltano/-/issues/2337) would help, but there's a cost.

Alternatively, we could have plugins fall back on their parent's installation dir if their `pip_url`s match, so that we'd only have `.meltano/extractors/tap-gitlab` until `tap-gitlab--inherited` changes its `pip_url` and gets a `.meltano/extractors/tap-gitlab--inherited`. Note that the directory names should always correspond to plugins _in the project_, so that names are known to be unique and map 1:1 to `pip_url`s. Plugins inheriting directly from a discoverable plugin (e.g. `tap-postgres--db-one` and `tap-postgres--db-two` both inheriting from `tap-postgres`, without a plugin by that name existing in the project) would not use their parent plugin's name for the installation directory, since the name could refer to multiple variants with their own `pip_url`s.

This approach is consistent with the current behavior, where plugin dir names match project plugin names, with the addition of inheritance (unless `pip_url` is overridden). I think the limitation that it doesn't apply to plugins that don't share a common ancestor in the project, but do share a `pip_url`, is acceptable.

</details>

## Updated Summary (2022-03-03)

There are a few paths forward on this which each have their own pros/cons.

There are also a couple related issues that make the matter overall more complex:

- #3068+
- #2701+ 

## Simplest solution, using `inherits_from`

The simplest solution is to use the `inherits_from` option as a trigger for leveraging the same installation `venv_path` internally within Meltano. 

This rule would apply _only_ if the plugin inheriting from does not also define `pip_url`. If `pip_url` is also defined _and is different_, the `venv_path` for that plugin would be based only on its own name rather than its parent (meaing, a unique venv).

Pseudocode:

```py
    @property
    def venv_name(self) -> string:
        """"""Get the venv name this plugin should use.""""""
        if not self.inherits_from:
            # No parent. Use a unique venv per plugin.
            return self.plugin_name

        if not self.pip_url or self.inherits_from.pip_url == self.pip_url:
            # Use the parent's venv. Plugin is inheriting and there's no difference in pip_url.
            return self.parent.name

        # Default to unique venv per plugin.
        return self.plugin_name
```

Pros:

- It works automatically with users' existing project definitions.
- It speeds up the installation for most projects if they use `inherits_from` at least once in their project.

Cons:

- This would not solve for #3068 or #2701.
  - It doesn't solve for detecting changes in `pip_url`. (#2701)
  - It may or may not be able to solve for inheritance between plugins of different types, for instance: the scenario described in #3068 where it is desirable for `dbt` and `sqlfluff` to share a venv.
    - This is tricky because `inherits_from` was never imagined to solve for inheritance across a `transformer` plugin, for instance, and a `utility` plugin.
    - In theory, this too could be made to work, but even so, the other implications of `inherits_from` seem broader than would applicable to diverse utilities of fundamentally different functions and configuration options.
- It doesn't solve for reconciling plugin names with their venvs if a plugin is renamed within `meltano.yml`.
- It doesn't solve for `pip_url` which just _happen_ to be identical. (Not a common use case anyway.)


## Possible resolution strategy

In sequence, this might get us where we want to go:

1. Solve for the simplest `inherits_from` cases, as described above. This would probably solve for 90% of the venv duplication issues for an average project.
2. To solve for the ""pip_url drift"" issue described in #2701, leave a marker file within the venv page during installation with the text of `pip_url` text. Then check the pip_url against the marker file on each execution. If it differs from the current value of `pip_url`, raise a warning or error that prompts the user to reinstall.
3. To resolve complex venv sharing use cases described in #3068, expand the spec so that plugins can ""inherit"" or ""share"" _only_ the venv of another plugin, without having to inherit all the other types of functionality, such as config, commands, etc. (More details on alternatives are in #3608, and presumably those solutions would stack with the `inherits_from` and 'pip_url drift detection' in the above two issues.)",8
76434934,2020-12-23 19:06:21.486,Health endpoint requires auth,The healthcheck endpoint (api/v1/health) always returns a 401 when requested from curl.,4
76109014,2020-12-16 00:40:22.011,Environment Variable Substitution doesn't work with array values,"### What is the current *bug* behavior?
Environment variables can't be used in the `meltano.yml` file for arrays.



### What is the expected *correct* behavior?
The yaml parser should traverse arrays to substitute environment variables. It would also be nice to add custom yaml variable support to the parser. (e.g. `custom_var: var` -> `database: '{{ custom_var }}'`)



### Steps to reproduce
- In a `meltano.yml` file try to use an environement variable in an array.


### Relevant logs and/or screenshots
```
plugins:
  extractors:
  - name: tap-mysql
    variant: transferwise
    pip_url: pipelinewise-tap-mysql
    config:
      host: host
      user: root
      database: ${DB_NAME}
    select:
    - ${DB_NAME}-users.id
    - ${DB_NAME}-users.email
    metadata:
      ${DB_NAME}-users:
        replication-method: FULL_TABLE
```
`database: ${DB_NAME}` works because it's an object. But it doesn't work in the select because it's an array.

### Possible fixes
Not sure, sorry.



### Further regression test
_Ensure we automatically catch similar issues in the future_

- [ ] Write additional adequate test cases and submit test results
- [ ] Test results should be reviewed by a person from the team",8
74578751,2020-11-18 16:40:53.090,Let project plugin definitions and config be defined in multiple individual YAML files instead of `meltano.yml`,"As I wrote in https://gitlab.com/meltano/meltano/-/issues/2206#note_388496082:

>>>
I think we could also allow users to define their project's taps and targets in their own YAML files, instead of having everything live in `meltano.yml`, since the `select`, `metadata`, `schema`, and `config` properties can grow quite large.

These files could live at `<type>/<name>.yml` in the project, e.g. `extractors/tap-example.yml`:

```yaml
variant: meltano
pip_url: git+https://gitlab.com/meltano/tap-example.git
select: ...
metadata: ...
schema: ...
config: ...
```

We wouldn't need to support variant subdirectories in this case, since only a single variant of a plugin should exist in a project anyway, which also goes for custom plugins.
>>>

------------------

## [Update] Latest spec description:

In `meltano.yml` specifying an include directory:

```yaml
project_id: ...
include_paths:
  - connectors/*.yml
  - schedules/*.yml
...
plugins: ...
schedules: ...
```

Or explicitly with exact file paths:

```yaml
project_id: ...
include_paths:
  - connectors/data-team.yml
  - connectors/business-team.yml
  - schedules/prod-refreshes.yml
...
plugins: ...
schedules: ...
```

And the companion files can be defined as:

```yaml
# connectors/data-team.yml
plugins:
  extractors:
    - name:...

  loaders:
    - name:...
```

```yml
# connectors/business-team.yml
plugins:
  extractors:
    - name:...

  loaders:
    - name:...
```

```yml
# schedules/prod-refreshes.yml
schedules:
  ...
```

Additional info:
- The included files have the same spec as `meltano.yml`, with the exception that some top-level entries like `project_id` might not be permitted.
- Rather than establish a priority between file declarations, collisions would be treated as errors, for instance if two files both declare an `extractor` with the name `tap-gitlab`.",8
74453850,2020-11-16 19:17:23.490,Support for data validation with Great Expectations,"### Problem to solve
This started with a conversation with @DouweM: We were wondering if it was possible to integrate data validation with Great Expectations more tightly into a Meltano workflow. This could be as simple as a ""meltano add validator"" type wrapper and integration into the elt runs, or some other form of integration

It would also be neat for Great Expectations to show up in the Meltano UI, maybe with Data Docs/validation results integrated into the UI, potentially even with a ""configure"" interface to the great_expectations.yml config file (although that is definitely a stretch).

One example for a UI integration of GE validation results is Dagster: https://dagster.io/blog/great-expectations-for-dagster

### Target audience
The target audience would be two-fold: 
1. Data engineers/owners of the Meltano workflow who run the pipelines and want to see whether all source data was correctly extracted and whether data was correctly transformed, and 
2. data consumers (stakeholders) that want to use Data Docs for data documentation and to see the most recent validation status.

### Further details
Not much to add - data validation is crucial to pipelines, and I think making it more accessible/part of Meltano would be really beneficial for users!

### Proposal (older)

<details><summary>Click to expand</summary>

I would love to hear from existing GE users how they envision this could work - here are some of my thoughts:
- At a very high level, maybe we could add another key concept like a ""validator"" that can be added to a Meltano project. This could potentially even be a wrapper for ""great_expectations init"" to initialize a GE data context in the Meltano project. 
- A user would then either configure a GE datasource manually, or all data connections could be inherited from Meltano. 
- The user would then create Expectation Suites and Checkpoints. Running ""meltano elt"" (possibly with a --run-validation flag?) could then trigger running all configured Checkpoints as part of a pipeline. I'm not quite clear yet how or whether a user would specify when exactly to run validation.

</details>


### What does success look like, and how can we measure that?
Acceptance criteria: a user can run validation with GE as part of a Meltano pipeline run without having to invoke GE separately, and can see the results either in Data Docs or directly in the Meltano UI.

Success: people actually run validation with GE in Meltano!


### Links / references

* GE homepage: greatexpectations.io
* Dagster integration: https://dagster.io/blog/great-expectations-for-dagster
* Airflow operator (as an example of how to invoke validation): https://github.com/great-expectations/airflow-provider-great-expectations

### Updated Proposal (2022-01-05)

1. Using the recently released Meltano features `dbt test` and `test commands`, Meltano users should be able to add Great Expectations as a utility plugin.
    1. First phase: Great Expectations can be added manually to `meltano.yml` by users familiar with Great Expectations (or in Meltano-owned projects like the Hub).
    2. Second phase: Assuming positive results from ""first phase"" above, Great Expectations will be added to `discovery.yml` so that users can add it using the command `meltano add utility great-expectations` (with no `--custom` flag).
2. Within `meltano.yml`, users will add `commands` with names starting with a `test*` prefix.
3. Tests will be runnable using any of these:
    - `meltano test --all`
    - `meltano test great-expectations`
    - `meltano test great-expectations:test-foo great-expectations:test-bar`
    - `meltano run great-expectations:test-foo great-expectations:test-bar` (because tests are also commands)

### Special commands and functions

Docs/UI:

We may also need to come up with a `great-expectations:ui` command which would build and launch

Init:

We may also want a predefined `init` command and perhaps commands for other administrative operations.",4
74172740,2020-11-10 23:31:34.201,"Consider renaming extractor ""entities"" and ""attributes"" to ""streams"" and ""properties"" for consistency with Singer","This would prevent confusion and inconsistent/interchangeable use of the Meltano-specific and Singer-specific term in docs or conversation.

Meltano users don't technically need to be familiar with underlying Singer concepts, and ""entities"" and ""attributes"" are arguably more user-friendly and less implementation-specific, but especially at this stage this is unrealistic since users will still often need to debug (and fix) taps and read their repositories and documentation, which use the Singer terms.

If Meltano were to at one point add support for a different open source EL standard that uses different terms, using ""streams"" and ""properties"" may no longer make sense, but that's a bridge we can cross then.",4
74168158,2020-11-10 20:39:58.958,`docs` job fails with vague `ERROR: Job failed: exit code 1`,"See https://gitlab.com/meltano/meltano/-/jobs/841670357:

```bash
<snip>
/home/u894-pruvsekvb4qh/www/meltano.com/public_html/mstile-144x144.png
/home/u894-pruvsekvb4qh/www/meltano.com/public_html/meltano-diagram.png
/home/u894-pruvsekvb4qh/www/meltano.com/public_html/apple-touch-icon.png
/home/u894-pruvsekvb4qh/www/meltano.com/public_html/mstile-310x150.png
Cleaning up file based variables
ERROR: Job failed: exit code 1
```

The failing command is [`$REMOTE_EXEC ""mkdir -p $SSH_BACKUP_DIRECTORY; tar zcvf $SSH_BACKUP_DIRECTORY-$(date +%Y-%m-%dT%H:%M).tar.gz $SSH_DIRECTORY/$SSH_WWW_DIRECTORY""`](https://gitlab.com/meltano/meltano/-/blob/master/.gitlab/ci/docs.gitlab-ci.yml#L25), where `$REMOTE_EXEC` is `ssh -o StrictHostKeyChecking=no $SSH_USER_DOMAIN -p$SSH_PORT`.

An earlier run (https://gitlab.com/meltano/meltano/-/jobs/841365856) logged those exact same output lines, but then was able to continue on to the next command:

```bash
<snip>
/home/u894-pruvsekvb4qh/www/meltano.com/public_html/mstile-144x144.png
/home/u894-pruvsekvb4qh/www/meltano.com/public_html/meltano-diagram.png
/home/u894-pruvsekvb4qh/www/meltano.com/public_html/apple-touch-icon.png
/home/u894-pruvsekvb4qh/www/meltano.com/public_html/mstile-310x150.png
$ $REMOTE_EXEC ""cd $SSH_DIRECTORY && find ./$SSH_WWW_DIRECTORY -mindepth 1 -maxdepth 1 -not -name blog -not -name '.' -exec rm -rf  '{}' \;""
$ scp -o stricthostkeychecking=no -P$SSH_PORT -r public/* $SSH_USER_DOMAIN:$SSH_DIRECTORY/$SSH_WWW_DIRECTORY
Saving cache for successful job
Creating cache default...
docs/node_modules/: found 25033 matching files and directories 
Uploading cache.zip to https://storage.googleapis.com/gitlab-com-runners-cache/project/7603319/default 
Created cache
Cleaning up file based variables
Job succeeded
```",2
72420621,2020-10-08 22:38:43.746,Nested properties in discovered catalog are interpreted as `inclusion: automatic` even if their parent property is `inclusion: available`,"See https://gitlab.com/meltano/meltano/-/blob/master/src/meltano/core/plugin/singer/catalog.py#L284.

As a result, these nested properties will always show up as `automatic` in `meltano select --list --all`, cannot be deselected using `meltano select --except`, and end up having metadata `inclusion: automatic, selected: true` in the generated catalog file that's passed to the tap.

In reality, if a parent property is discovered as `inclusion: available`, changing `selected` or `inclusion` on an individual subproperty usually won't make a difference since taps typically only check the `selected` metadata for top-level properties, not nested ones.

To resolve this, then, we can have Meltano assume `inclusion: automatic` on top-level properties that lack metadata, but not on nested properties, (assuming their parent property does have metadata). In `meltano select --list --all`, nested properties without discovered(!) `inclusion` metadata shouldn't be listed at all, since they can't actually be individually selected. If a nested property does have discovered `inclusion` metadata, we can list it. 

We could automatically have a top-level `selected` rule cascade down to its properties, but let's not get into that for now.

---

As a temporary workaround, the following `metadata` rule can be added to a plugin definition to flip these nested `inclusion: automatic` metadatas to `inclusion: available`:

```yaml
extractors:
- name: tap-facebook
  # ...
  metadata:
    '*':
      '*.*':
        inclusion: available
```",8
70124985,2020-08-21 18:40:56.781,Add `meltano run [<plugin>...]` to run arbitrary plugins in a pipeline,"As meltano run evolves we may opt to expand/change the invocation syntax but

>  a simple first iteration will be to focus on just the chain use case: we do a bit of work to validate blocks, but otherwise we run left to right and find ExtractLoad Blocks as appropriate. @tayloramurphy 

It's simply `meltano run tap-gitlab target-jsonl tap-gitlab target-csv dbt:run` with blocks run in series (with ExtractLoad blocks parsed and linked as needed). 

Other sample valid invocations:

- `meltano run tap-gitlab map-remove-nulls target-msyql dbt:run`
- `meltano run tap-gitlab target-msyql dbt:run tap-msqyl target-json`
- `meltano run tap-gitlab target-msyql dbt:run dbt:test superset:build`

Invalid invocations:

- command block between IOBlocks: `meltano run tap-gitlab dbt:run target-msyql`
- starting IOBlock (tap-peloton) with no ending IOBlock (a target): `meltano run tap-peloton tap-gitlab target-mysql` 
- ending IOBlock (target-jsonl) with no starting IOBlock (a tap): `meltano run tap-gitlab target-mysql target-jsonl` 
- tap/target used as command block: `meltano run tap-gitlab:discovery`

Job ID generation and STATE support:

- If no environment is provided: job ID is not generated and STATE is not supported.
- If environment is provided: job ID is automatically generated with a format such as `{environment_name}:{tap_name}-to-{target_name}`.

Explicitly out of scope in the first version:

- Running taps/targets as commands is out of scope at least in this first version. 
- Retries, permissive or selective failures is out of scope at least in this first version. A failure at any point halts execution and no further blocks are executed. 


Fully out of scope:

- There's no plan as of now to support individual block-level arg passing - and it will likely not be available in future revisions. For instance, this will not be possible: `meltano run dbt:run dbt:test[--verbose]`. Instead you'd need to use `meltano invoke` or create a custom `test-verbose` command and execute like this `meltano run dbt:run dbt:test-verbose`.


<details>
  <summary>Original issue body</summary>
  
As in:

- `meltano elt <extractor> <loader> [--transform=skip]` -> `meltano run <extractor> <loader>`, e.g. `meltano run tap-foo target-bar`
- `meltano elt <extractor> <loader> --transform=run` -> `meltano run <extractor> <loader> <transformer>`, e.g. `meltano run tap-foo target-bar dbt`
- `meltano elt <extractor> <loader> --transform=only` -> `meltano run <transformer> --with <extractor> --with <loader>`, e.g. `meltano run dbt --with tap-foo target-bar` (see https://gitlab.com/meltano/meltano/-/issues/2546)

And instead of `meltano schedule gitlab-to-jsonl tap-gitlab target-jsonl @hourly --transform=run:

```yaml
schedules:
- name: gitlab-to-jsonl
  extractor: tap-gitlab
  loader: target-jsonl
  transform: run
  interval: '@hourly'
```

We get `meltano schedule gitlab-to-jsonl tap-gitlab target-jsonl dbt @hourly`:

```yml
- name: gitlab-to-jsonl
  interval: '@hourly'
  pipeline: # or `run`?
  - tap-gitlab
  - target-jsonl
  - dbt
```
</details>",12
70124963,2020-08-21 18:39:37.722,Allow extractor Singer messages to be transformed using Python before passing them to loader (Stream Maps),"We could support [`transformer` plugins](https://meltano.com/docs/plugins.html#transformers) other than `dbt`, that would follow extraction rather than loading, and would transform an extractor's output stream of Singer messages (`SCHEMA`, `RECORD`, etc), before they're streamed into the loader.

The two types of transformers could be distinguished using an extra, e.g. `type: {etl,elt}` or `{follows,acts_on,transforms}: {extractor,loader}`

We could support both pip packages, and local executable files, using either the `pip_url` or `executable` plugin setting.",12
70124790,2020-08-21 18:32:43.092,Allow stream-level and property-level transformations to be defined in `meltano.yml` (Stream Maps),"<details><summary>Expand Original Issue Description</summary>

Inspired by https://www.dropbase.io/, PipelineWise transformations (https://transferwise.github.io/pipelinewise/user_guide/transformations.html), and our [existing extractor extras](https://meltano.com/docs/plugins.html#metadata-extra), I'm imagining something like:

```yaml
extractors:
- name: tap-example
  # ...
  transform:
    <entity>:
      <function>: <args>
      <attribute>:
        <function>: <args>
```

Since these transformations would act on an extractor's output: a stream of Singer `SCHEMA` and `RECORD` messages, we could relatively easily support functions for:
- renaming (prefixing, suffixing) entity (stream, table) or attribute (property, column) names
- dropping entities or attributes, in cases where a tap doesn't support discovery mode and entity selection
- adding attributes with predefined or dynamic values, like PipelineWise's metadata columns: https://transferwise.github.io/pipelinewise/user_guide/metadata_columns.html
- filtering records based on one or more attribute values, keeping only those that do (or don't!) match (`drop_if`, `drop_unless`?)
- replacing text in attribute values
- replacing empty strings with nulls
- replacing nulls with a string
- changing attribute types and casting values, which can go beyond overriding the JSON schema using the [`schema` extra](https://meltano.com/docs/plugins.html#schema-extra)

Functions could take arguments of any type: a simple string, an array of values, or an object with additional keys. If a function doesn't take any attribute (like `drop`), it could just take a `true` boolean.

Since functions could have object arguments, Meltano would not immediately be able to distinguish between `{entity: {attribute: {function: scalar_value}}}` and `{entity: {function: {nested_key: nested_value}}}`, where the key nested under an entity identifier could either be an attribute identifier or a function name. We don't have this issue with [`metadata`](https://meltano.com/docs/plugins.html#metadata-extra) and [`schema`](https://meltano.com/docs/plugins.html#schema-extra) extras, because metadata values cannot (so far) be objects, and since schema info can only be specified for attributes, not entities as a whole. Perhaps we can add a special `_` or `_self` or `_entity` key at the attribute level to nest entity-level transformation functions under, in cases where they need object values.

</details>

## Background (Updated 2021-12-15)

There would be a large advantage to being able to enable transformations like those from [pipelinewise-transform-field](https://github.com/transferwise/pipelinewise-transform-field) and Meltano SDK's [Inline Stream Maps](https://sdk.meltano.com/en/latest/stream_maps.html) to be defined natively in `meltano.yml` config.

This opens up a large number of use cases defined on the SDK docs site:

> ### Stream-Level Mapping Applications
>
> - Stream aliasing: streams can be aliased to provide custom naming downstream.
> - Stream filtering: streams records can be filtered based on any user-defined logic.
> - Stream duplication: streams can be split or duplicated and then sent as multiple distinct streams to the downstream target.
> 
> ### Property-Level Mapping Applications
>
> - Property-level aliasing: properties can be renamed in the resulting stream.
> - Property-level transformations: properties can be transformed inline.
> - Property-level exclusions: properties can be removed from the resulting stream.
> - Property-level additions: new properties can be created based on inline user-defined expressions.

As well as fixes for these common issues:

- Applying selection rules to taps that don't support selection.
- Resolve issues caused by taps that use selection rules to filter `RECORD` messages but not the `SCHEMA` messages that is used to create target tables.
- Resolve compatibility issues from taps that send data types that the chosen target cannot understand.
- Resolve compatibility issues from taps that send `ACTIVATE_VERSION` messages to targets that don't understand them.
- Need for ""record flattening"" when neither the tap or target supports this feature natively.

## Proposal: Map transforms as properties of extractors and loaders (Updated 2021-12-15)

After running the following...

```bash
meltano add mapper meltano-map-transform
meltano add mapper pipelinewise-transform-field
```

... you would be able to provide a config such as:


```yaml
mappers:
- name: meltano-map-transform
  pip_url: meltano-map-transform
  config:                         # Optionally, a default config.
    # ...
- name: pipelinewise-transform-field
  pip_url: pipelinewise-transform-field
- name: no-activate-version.      # A fictional mapper that removes ACTIVATE_VERSION messages
  pip_url: no-activate-version
  config:                         # Optionally, a default config.
    # ...
extractors:
- name: tap-gitlab
  # ...
  mappings:
  - name: pii-hasher               # The name of the map transform to apply.
    mapper: meltano-map-transform  # The mapper plugin to use.
    config:                        # What will be sent to the transformer in a config.json file.
      stream_maps:
        customers:
          id_hashed: md5(record['id'])
          id: None
loaders:
- name: target-salesforce
  # ...
  mappings:
  - name: add-global-guid          # The name of the map transform to apply.
    mapper: meltano-map-transform  # The mapper plugin to use.
    config:                        # What will be sent to the transformer in a config.json file.
      stream_maps:
        customers:
          guid: md5(record['id'])
- name: target-csv
  # ...
  mappings:
  - name: flatten-records          # The name of the map transform to apply.
    mapper: meltano-map-transform  # The mapper plugin to use.
    default: true                  # Transform prepended automatically if default=true.
    config:                        # What will be sent to the transformer in a config.json file.
      flatten_records: true
  - name: compat-fix               # The name of the map transform to apply.
    mapper: no-activate-version    # The mapper plugin to use.
                                   # config is omitted if the generic config is sufficient
```

One or more map transforms may be placed between tap and target:

```bash
meltano run tap-gitlab pii-hasher target-salesfoce
meltano run tap-gitlab pii-hasher flatten-records target-csv
```

Since `default=true` for `flatten-records` and `compat-fix` on `target-csv`, these are all equivalent:

```bash
meltano run tap-gitlab flatten-records target-csv
meltano run tap-gitlab target-csv
```

Note: For the example config above, this command would fail because `flatten-records` is only defined for `target-csv` and is not defined for `tap-gitlab` nor `target-salesforce`: `meltano run tap-gitlab flatten-records target-salesforce`.",12
69769623,2020-08-12 16:13:00.636,Add new doc for `airflow` plugin and its settings,"It should be made clear that all of Airflow's settings can be managed through `meltano config` and `meltano.yml`.

(Parts of) https://meltano.com/docs/production.html#airflow-orchestrator should be moved here too.",2
69183865,2020-07-29 17:55:52.517,Add `container_spec` for containerized plugin commands,"The `tap-mssql` plugin is a good example of a non-standard, non-python tap which would nevertheless be desirable for mainstream support. One way to support this is by adding a docker_image option in discovery.yml and work through whatever issues come up in that process.

Originally started as a discussion here: https://meltano.slack.com/archives/C013Z450LCD/p1596043957041600?thread_ts=1595992122.035100&cid=C013Z450LCD

--------------

## Update (2021-08-24)

-----------------

The original use case for this issue (java-based `singer/tap-mssql`) is no longer a priority since we now have a python-based `tap-mssql` implementation and there just haven't been many (any?) other taps or targets requested which were not pip installable.

A newer use case is Lightdash, which has a complex set of requirements and is recommended to be run via docker - or specifically via docker compose.

## Update ~~(2021-10-05)~~ ~~(2021-11-29)~~ (2021-12-07)

In discussions around this topic, it appears clear we need to provide docker and non-docker invocation methods for plugins so that a `--container=True` flag (or similar) could toggle the behavior to dockerized vs native invocation methods. The docker config is then additive to existing `executable` and `pip_url` inputs.

### New `container_spec` option for plugin commands

This feature would add a structured `container_spec` config option to each plugin command:

```yml
plugins:
    utilities:
        name: lightdash
        commands:
        - name: serve
          executable: ""lightdash serve""  # ignored when running via docker
          container_spec:
            image: lightdash
            command: lightdash serve
            entrypoint: 
            env:
                ENVVAR1: foo
                ENVVAR2: bar
            ports:
            - 5000
            - 8080
            volumes:
            - "".:/project/myproject""
        - name: build
          executable: ""lightdash build""  # ignored when running via docker
          container_spec:
            image: lightdash
            command: lightdash build
            entrypoint: 
            env:
                ENVVAR1: foo
                ENVVAR2: bar
            ports: [] # None needed
            volumes:
            - "".:/project/myproject""
```

### Supported `container_spec` options in V1:

* `image`
* `vars`
* `ports`
* `command`
* `entrypoint`
* `volumes` - or *at least* a container path in which to mount the root project dir

Only `image` is required. All other options are optional.

### New `--containers=true|false` flag for CLI commands `elt`, `invoke`, and `run`

The CLI commands `elt`, `invoke`, and `run` would all have a new `--containers` CLI arg. This arg has the following behaviors:

- `true`: This uses `container_spec` whenever it is available.
    - Important: `true` setting does not guarantee that all invocations will be run containerized. Any plugin or command that does not have a `container_spec` will be attempted to be invoked natively. If native invocation fails, the command will fail - as is the existing behavior today.
- `false`: This users local invocation (non-containerized) for all plugins. (Assumption being that there is no container runtime available on the machine.)
    - Note: If a plugin _only_ has a `container_spec` config and does not have any other means of being invoked, then the command will fail.

Note that if multiple plugins need to be invoked, they will share the same preference setting across the entire meltano CLI command.

#### Sample invocation

For instance, taking the following sample invocation: `meltano --containers=true run tap-gitlab target-snowflake dbt:run dbt:test lightdash:build` and assuming that only `tap-gitlab` and `lightdash:build` have a defined `container_spec`, the behavior will be as follows:

- `tap-gitlab` will be invoked within a container and its data will be passed to the STDIN of `target-snowflake`, which will be running natively.
- `dbt:run` and `dbt:test` will both be run natively, since no `container_spec` exists for those commands.
- `lightdash:build` will be run within a container.

Assuming the same command is run with `--containers=false`, and assuming `lightdash:build` does not also have an non-containerized `executable` set, then meltano will fail to build an execution plan and will abort immediately with something like:

> Execution of `lightdash:build` is not defined when container runtimes are disabled. To proceed, add a native `executable` definition to `lightdash:build` command or run with `--containers=true` to enable containerized execution.

Lastly, if `executable` and `container_spec` are both provided for the `lightdash:build` command, but the command is invoked natively (`--containers=false`) without the proper prerequisites installed, then the command will fail at the very end of the job when `meltano run` reaches the final `lightdash:build` step.

#### Optional CLI conventions we could support:

- `--containers` is short for `--containers=true`
- `--no-containers` is short for `--containers=false`


## Out of scope

### Tap and Target containerization

We will probably build support for general plugins (`transformers`, `orchestrators`, and `utilities`) first - or more specifically for plugin `commands` first.

Due to the additional complexity, containerized extractors and loaders would likely be a fast-follow on top of the first MVC release.

Unlike for general utilities, the `container_spec` config for extractors and loaders needs to be at the plugin level, not the command level, and any `command` value will likely be ignored by Meltano at runtime, in favor of the ways that Meltano already knows to invoke a tap, especially: `tap-mytap --discover --config=/path/to/config.json` (discovery) and `tap-mytap --config=/path/to/config.json` (sync).

### Compose Syntax and docker-compose.yml files

Out of scope for now, but we _might_ also support docker-compose in future:

```yml
plugins:
    utilities:
        name: lighdash
        commands:
        - name: serve
          container_spec:
            compose: lightdash.docker-compose.yml
```",12
68280117,2020-07-20 20:39:16.224,"Add ""Configure"" buttons to extractor and loader in Pipelines list",,4
68279587,2020-07-20 20:38:22.127,"Add ""Full Refresh"" button to UI","- Only when pipeline actually has state
- Update text in ""ConnectorSettings"" component about changing start date",4
63846865,2020-07-17 16:23:10.920,Preserve comments and flow style when programmatically modifying `meltano.yml` (Ruamel),"PyYAML doesn't support this currently (https://github.com/yaml/pyyaml/issues/90), but https://pypi.org/project/ruamel.yaml does.

Note:

Our usage of the Canonical class may also be a factor we need to consider in our implementation. Noted by @kgpayne below: https://gitlab.com/meltano/meltano/-/issues/2177#note_708071594",20
54102788,2020-07-08 14:16:00.817,Create Helm chart to deploy Meltano to Kubernetes,"The Helm chart should ideally run the Meltano schedule via Kubernetes orchestration without Airflow if possible.

Minimum Goal:
The minimum goal will to be to provide a way to run a single `meltano elt` pipeline on a schedule with a Kubernetes job.

Ideal Outcome:
The ideal outcome would be to support both running a project specific docker image and the default Meltano docker image in both readonly, as well as read-write mode to support the broadest possible user base. Each scheduled pipeline created using `meltano schedule` and available through `meltano schedule list --format=json` would result in its own scheduled `meltano elt` job.",12
35189225,2020-05-29 15:15:14.135,"Rename of internal (OLTP) ""Job"" tables and columns","- [ ] Rename `job` table to `run(s)`
- [ ] Rename `job_id` column to `job_name`

Note: Since the concept of 'job' itself is changing, we should either wait until #2924+ is shipped - or else combine this with that.",4
34008395,2020-04-30 17:09:12.114,Allow extractor entities to be selected in UI,"We already had an ""Entity selection"" modal before, but it was removed in https://gitlab.com/meltano/meltano/-/merge_requests/1157:

![](https://gitlab.com/meltano/meltano/uploads/4d03378285cecd17fbb9e8e537838471/Screen_Shot_2019-08-14_at_8.30.04_AM.png)

I think we should:
- [ ] Restore the `EntitiesSelectorModal` component from that MR
- [ ] Add a new ""Select entities"" button to installed extractors on the Extract page, provided they support the `discovery` and `catalog`/`properties` capabilities
- [ ] Make it clear that with anything but the full set of properties, Meltano's built-in transforms may not work, as described in https://gitlab.com/meltano/meltano/-/issues/876#note_240275676",8
29724563,2020-01-18 12:19:16.515,"Add ""Retry"" button to pipeline logs view and list when run failed","### Problem to solve
(Summarize the problem we are trying to solve in the form of we need to do [A], so that [B] can [C])

We need to *add a pipeline retry button to the Meltano UI Pipeline logs view*, so that when I see that something went wrong I can retry it easily. 

![Screen_Shot_2020-01-18_at_7.15.57_AM](/uploads/328de49638e6d3d5e4285cd0b07d5827/Screen_Shot_2020-01-18_at_7.15.57_AM.jpg)

For example, I can see that ""Nothing to do"" happened, but something should have happened. 
I want to be able to make changes and then just hit ""retry"".

### Target audience
(For whom are we doing this? Include a persona)

Anyone who is editing or building their own analyses. 

### Further details
(Include use cases, benefits, and/or goals)

It will make it easier to trouble shoot. 
This helps make solve the ""Have you tried restarting it?"" problem.


### Proposal
(How are we going to solve the problem? Try to include the user journey)
I propose a Retry button in the bottom right of the modal.


### What does success look like, and how can we measure that?
(Define both the success metrics and acceptance criteria. Note that success metrics indicate the desired business outcomes, while acceptance criteria indicate when the solution is working correctly. If there is no way to measure success, link to an issue that will implement a way to measure this)

### Regression test
(Ensure the feature doesn't cause any regressions)
- [ ] Write adequate test cases and submit test results
- [ ] Test results should be reviewed by a person from the team

### Links / references

_Please note that this was taken from GitLab, to be changed accordingly_",4
19359027,2019-03-22 05:19:25.268,Combine with ML Flow to Scale Knowledge,"### Problem to solve

Data science/ML experiments are often difficult to communicate to fellow team members and executive. When communicating to team members, the experiment has to be **reproducible** in order for them to have more technical depth. When communicating with executive, the experiment has to be **communicated in a simple manner** in order to have investment from leadership. Not only do executives and team members are stake holders, but other people in the company might be too. Scaling knowledge of experiments is difficult within any company. 

### Target audience

Data Scientist

*  Frustrated at a company process designed for software engineers
*  Might able to communicate success, but unable to explain the failed experiments leading up to that success. Failed experiments consists of the core pieces of insight gained by the scientist. Stakeholders often wonder why the process take so long.
* .Might be able to communicate successful experiments to a few stakeholders, but unable to do so with all those that might be interested. Uable to scale their communication.
* Frustrated at the slow decision making when the number of stakeholders that needed to be communicated to are high
* Frustrated at the review process designed for software engineers -> Git code management system like Bitbucket does not have the necessary features to support the review process needed for high quality feedback for peers -> want to have high quality feedback on experiments to produce high quality results

 **As an organization grows, how do we make sure that an insight uncovered by one person effectively transfers beyond the targeted recipient?**

### Further details

One of the major problems is not only sharing reports and analytics but also ML experiments so that knowledge can be pervasive. There have been attempts like this such as [Airbnb Knowledge Repo](https://github.com/airbnb/knowledge-repo) but it's an underinvested area. It's still an unsolved problem within the data science community.

### Proposal
Combine it with MLflow Projects to have reproducible experiments.

### What does success look like, and how can we measure that?
(Define both the success metrics and acceptance criteria. Note that success metrics indicate the desired business outcomes, while acceptance criteria indicate when the solution is working correctly. If there is no way to measure success, link to an issue that will implement a way to measure this)

Have a solution that covers 5 main areas:
1.  Reproduciblity
2.  Quality
3.  Consumability of content
4.  Discoverability
5.  Learning/growth

Reference: [Scaling Knowledge at Airbnb](https://medium.com/airbnb-engineering/scaling-knowledge-at-airbnb-875d73eff091)

### Links / references

_Please note that this was taken from GitLab, to be changed accordingly_

https://github.com/airbnb/knowledge-repo

https://mlflow.org

https://medium.com/airbnb-engineering/scaling-knowledge-at-airbnb-875d73eff091",1
18781735,2019-03-04 15:20:31.688,Add Snowflake dialect to PyPIKA,"### Problem to solve
We are currently using PyPIKA to generate the SQL queries for the Meltano UI. As we want to use Snowflake as a data warehouse, we need to make sure our SQL generation engine supports it. We'll need to add a Snowflake dialect to PyPIKA, which might involve sending back as a contribution or as a separate package. Once the dialect in, PyPIKA should be able to generate queries compatible with Snowflake specifics.

### Target audience
Data Analyst


### Further details
Without this, the generated SQL might cause an error when queried.


### Proposal

  - [ ] Investigate Snowflake SQL support
  - [ ] Investigate PyPIKA dialect handling 
  - [ ] Investigate PyPIKA dialect integration
  - [ ] Build the dialect

### What does success look like, and how can we measure that?
Our SQL generation engine can output Snowflake compatible SQL.

### Links / references

Related to #428 

https://github.com/kayak/pypika/blob/master/pypika/dialects.py",3
18781583,2019-03-04 15:15:18.164,Add Snowflake dialect the database settings,"Related to #428 

### Problem to solve
We need to add a Snowflake dialect in the database settings, along with any dialect-specific configuration keys in the database settings so that Meltano API can connect to Snowflake for querying.


### Target audience
Audience: Data Analyst


### What does success look like, and how can we measure that?
We are able to add a Snowflake connection in the Meltano UI database settings.


### Links / references

_Please note that this was taken from GitLab, to be changed accordingly_",1
18696124,2019-02-28 21:23:07.690,Create a connector for Snowflake in the UI,"### Problem to solve
(Summarize the problem we are trying to solve in the form of we need to do [A], so that [B] can [C])
Currently, if we want to use snowflake as a target, we can run the ETL, but a user cannot use the UI to analyze data

### Target audience
(For whom are we doing this? Include a persona)
Anyone replicating the tutorials

### Further details
(Include use cases, benefits, and/or goals)
Have a full working demo with snowflake as a target

### Proposal
(How are we going to solve the problem? Try to include the user journey)
Create a connector for Snowflake - @iroussos, let's break down the problem if needed

### What does success look like, and how can we measure that?
(Define both the success metrics and acceptance criteria. Note that success metrics indicate the desired business outcomes, while acceptance criteria indicate when the solution is working correctly. If there is no way to measure success, link to an issue that will implement a way to measure this)
AC: We can run each demo e2e with target snowflake and play with the data in the UI

### Links / references

_Please note that this was taken from GitLab, to be changed accordingly_",4
18266934,2019-02-15 13:57:43.532,Planning/Exploration Sprint - should Meltano offer a hosted sandbox experience?,"### Full Opportunity Assessment

@valexieva please add link here once written: --> both links below

### Problem(s) to solve
Our current product adoption path assumes users launch Meltano in their local development environment (their computer), however due to the size of many data sets this can cause issues with memory, performance, etc. which can limit user adoption and recurring MAU

However, users also may be using Meltano on data that we do not want to responsible for hosting (PII, GDPR, etc.) so we also need to think about guiding them to use a production environment that makes sense for their needs.

### Proposed Solution (to Validate)
Offer the user a way to use Meltano while hosting their data somewhere that can handle a big data set. 

Will explore pros/cons of different options, including:
* Self-hosting Meltano on the user's existing infrastructure
* Self-hosting Meltano on a one-click install (e.g. Bitnami, Dreamhost, Amazon, etc.)
* Meltano hosted sandbox


### Tasks to Complete Planning/Exploration Sprint
* [x] Write an opportunity assessment for a hosted sandbox experience (template here: https://docs.google.com/document/d/1rKEihpSxzC_a499xzbuIh9Jd-MfteOaUTMVqQJBzAtA/edit# - go / no-go + recommendation
* [x] Write an opportunity assessment for docker (template here: https://docs.google.com/document/d/1OG2HzICuj0v2C1itznPoW4mI5KtroSDKSmDIPxet0OE/edit# - go / no-go + recommendation
* [x] Collect and discuss advantages / disadvantages of all options in the context of the product
* [ ] ~~Architectural design proposal, including development environment/ deployment to production~~
* [ ] ~~Feasibility discussion~~
* [ ] ~~Lightweight development design for a 1 week sprint experiment~~
* [x] Go / no-go for immediate action versus place in the backlog. **Decision is: NO GO**, we will execute on our Docker image for now and revisit later",4
18053807,2019-02-07 20:28:14.617,to be reused for something else,TBD,2
17667095,2019-01-25 15:57:36.900,Add workflow for custom taps and targets,"### Problem to solve
_Summarize the problem we are trying to solve in the form of we need to do [A], so that [B] can [C]_

We currently don't support many taps and targets and this poses risk to adoption. We would like to have a very clear explanation / workflow for anyone willing to contribute

### Target audience
(For whom are we doing this? Include a persona)

Developers

### Further details
(Include use cases, benefits, and/or goals)

By adding more taps and targets, we hope to be able to increase adoption by allowing companies to bring many different data sources in 1.
Developing taps and targets in Singer is painful and we believe that by making this process smoother, we can improve the Singer contributing community, which is currently a roadblock to adoption for us.


### Proposal
(How are we going to solve the problem? Try to include the user journey)

- [x] Add custom switch to `meltano add` - we should be able to add any tap/target bypassing discovery.yml from the CLOI
- [x] Document steps required to create a custom tap and target

### What does success look like, and how can we measure that?
(Define both the success metrics and acceptance criteria. Note that success metrics indicate the desired business outcomes, while acceptance criteria indicate when the solution is working correctly. If there is no way to measure success, link to an issue that will implement a way to measure this)


### Links / references

_Please note that this was taken from GitLab, to be changed accordingly_",1
17331949,2019-01-14 16:50:41.819,Silence pip unless there is an error,"As per the 2018-01-14 demo, this was a feedback:

> Pip version 10.0.1 red warning.
> Micael: Should we silence pip?
> Micael: We can either upgrade pip (I don’t like that) or silence the ouput unless there is an error (I like that)

I think we should simple silence pip unless there is a problem.

Make the error YELLOW and not RED when it's a warning.",1
17268497,2019-01-11 13:25:24.817,Remove `dbt` as a dependency in setup.py,"Meltano doesn't need **dbt** to be installed as a dependency.

Let's remove it.",1
17139395,2019-01-07 16:09:06.349,Settings with SQLite,- [x] Refactor the frontend of the setting page to accept connections for SQLite.,1
14522544,2018-09-27 13:53:09.751,Snowflake Target,"Because we are starting to move in the direction of using the singer taps and target protocol we will need a snowflake target. 

cc @iroussos",13
98557810,2021-12-06 22:40:00.303,Consider a private meltano managed `.meltano/dbt/profiles.yml` for dbt,"## Background

Currently Meltano creates a not-very robust `profiles.yml` file which attempts to be ""one size fits all"" but it only supports a few different config options and has several magic environment variables it expects. This is a barrier to new users onboarding and frequently we get ""not declared"" errors for those environment variables being missing from runtime context.

Pros of status quo:

1. Gives an out-of-box dbt experience for three different generic database configs: postgres, redshift, and snowflake
    - https://gitlab.com/meltano/files-dbt/-/blob/master/bundle/transform/profile/profiles.yml
2. Doesn't require a user to understand the `profiles.yml` format.

Cons of status quo:

1. Most users will end up deleting the extra profiles or modifying the file to make it their own. (Which feels wrong, like you are breaking the connection to Meltano.)
1. There's no support for other databases like Spark or Redshift.
1. Manual changes could get overridden if the files bundle is reinstalled.
1. Following from point 1, it's not clear at all if overriding/modifying the built-in `profiles.yml` is supported - or if there are other consequences of doing so.

## Proposal

1. Similarly to how Meltano pre-creates and manages config.json files and catalog.json files for taps, Meltano could auto-generate a hidden `profiles.yml` file internally to the `.meltano` directory.
1. Meltano can populate values from config in `meltano.yml`, rather than requiring special environment variables to be set by the user.
1. Meltano can pick what kind of dbt adapter to use based on the environment's `primary_loader` config (not yet built).
1. As users get more mature in their usage of dbt, they may _optionally_ provide a path to their own generated profiles.yml file. This could be an optional config option for the `dbt` transformer and/or of the meltano environment itself.
1. The user can optionally provide a profile name for dbt to use - as a config option for the `dbt` transformer.
    - If not set, the dbt profile target can default to the environment name (`prod`, `userdev`, etc.).
1. Since the hidden file would not be version controlled, it can be adapted without breaking existing code. For instance, we could add handling for a Spark profile and we wouldn't need users to reinstall `files-dbt` bundle.",8
98540152,2021-12-06 17:05:45.966,Document pyenv/poetry use in contributor docs,"Our getting started guide for contributors isn't super cohesive in how the setup steps for contributing to meltano, the meltano api, or the meltano ui are described, and what the expected dev environment looks like.

1. For example in the ""setting up your environment"" section we have folks clone/poetry install https://meltano.com/docs/contributor-guide.html#setting-up-your-environment
2. The api dev section makes no further mention of poetry and has you do a vanilla `meltano init` 
3. Similar the UI dev sections also makes no further mention of poetry.

Would it be useful to maybe to include a preamble on working with pyenv/poetry - and then providing general contributor, api, ui dev docs that take that into account? A lot of folks will already have experience with things like pyenv/virtualenv/poetry but it might still be nice to touch on them and clarify their potential use for managing your dev environments.",1
98479511,2021-12-05 18:03:20.565,meltano run ci/cd pipeline project,"Something similar to the `meltano/demo-project` running as part of our CI/CD suite but using `meltano run` would be handy. I think initially, it would probably be wise to make it an optional stage (i.e. `allow_failure: true`) - so that failures on that stage don't block our entire pipeline. Another option might be to initially have this be a scheduled pipeline - https://docs.gitlab.com/ee/ci/pipelines/schedules.html",4
98435750,2021-12-03 21:57:53.767,Command `dbt:run` should work without initializing extra environment variables,"When running `meltano elt tap target --transform=run` - things ""just work"" without little additional work if you're using one of the existing transform packages like in the gitlab demo project in a pipeline.

However, running `meltano invoke dbt:run` doesn't work because there are still several env vars we have to set. The big ones (like DBT_TARGET_SCHEMA) are covered very briefly in https://meltano.com/docs/transforms.html#running-a-transform-in-meltano but there's a lot more you need to potentially set if you're trying to use it from the env, namely: PG_DATABASE|PASSWORD|USERNAME|PORT|etc.

At first, I couldn't tell if those were all env vars we were just setting proactively every time you run `meltano elt` (because we can infer what we need there and what the target context is), or if there was another way I should configure this. i.e. is there a generated config file I should just reference? All the config files I found just seem to reference env vars though.

## Environment Variables currently expected by our bundled `profiles.yml`

| Variable            | Meaning                                                                                                                                        | Maps to                                                                                                                                                                                                                                                                                                                                  |
| ------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `DBT_TARGET`        | > This is the default target your dbt project will use. It must be one of the targets you define in your profile. Commonly it is set to `dev`. | Defaults to Meltano's `$MELTANO_LOAD__DIALECT`. With the `profiles.yml` currently packaged in the dbt file bundle, this means one of `snowflake`, `postgres` or `bigquery`. Like @tayloramurphy mentions in https://gitlab.com/meltano/meltano/-/issues/3104#note_804770702, this is confusing and not standard practice in dbt projects. |
| `DBT_TARGET_SCHEMA` | > The default schema that dbt will build objects in.                                                                                           | Defaults to `analytics`.                                                                                                                                                                                                                                                                                                                 |
| `SF_*`              | Snowflake connection parameters.                                                                                                               | These [leverage the `env_aliases`](https://gitlab.com/meltano/meltano/-/blob/544f7128b3efd783efe7ac7b7c109e80ef9f086c/src/meltano/core/bundle/discovery.yml#L2551-2578) injected at runtime by Meltano. So, they get the same values as `TARGET_SNOWFLAKE_*`.                                                                                                                                                                                                                   |
| `SF_ROLE`           | Snowflake Role.                                                                                                                                | This one doesn't use `env_aliases` so I'm not sure if it gets injected...                                                                                                                                                                                                                                                                |
| `PG_*`              | PostgreSQL connection parameters.                                                                                                              | These [leverage the `env_aliases`](https://gitlab.com/meltano/meltano/-/blob/544f7128b3efd783efe7ac7b7c109e80ef9f086c/src/meltano/core/bundle/discovery.yml#L2299-2333) injected at runtime by Meltano. So, they get the same values as `TARGET_POSTGRES_*`.                                                                                                                                                                                                                    |
| `TARGET_BIGQUERY_*` | BigQuery connection parameters.                                                                                                                | These use directly the same values as `TARGET_BIGQUERY_*`.                                                                                                                                                                                                                                                                               |",4
98429315,2021-12-03 18:09:15.750,tap-slack extractor token config setting mislabeled,The [tap-slack setting](https://github.com/Mashey/tap-slack#setup) requires the `token` key but we have it labeled `api_token` so configuration doesnt work through the CLI.,1
98425476,2021-12-03 16:37:59.441,Document Nested Select Examples,Based on https://gitlab.com/meltano/sdk/-/issues/285#note_750983833 where incorrectly configured select criteria for a complex nested case looked like an SDK bug to the user. We should update [the documentation](https://meltano.com/docs/command-line-interface.html#select) to show the example that @edgarrmondragon provided. I believe the solution was that you need to include the highest level attribute before you can select nested attributes (correct me if I'm wrong).,1
98375509,2021-12-02 20:58:36.452,meltano run improvements for closer elt parity,"For `meltano run` to be a full replacement of `meltano elt` there's a few small additions that we'll probably need to consider. This originally pop'd up in https://gitlab.com/meltano/meltano/-/issues/2301#note_750941505 during the initial `meltano run` build.

1. ~~Support for incremental jobs~~ (now logged as #3130):
    1. Job ID support via Job ID prefix sourced from a CLI flag or based on the activated meltano env. (https://gitlab.com/meltano/meltano/-/issues/2301#note_750941505)
    1. Top-level `--full-refresh` to ignore saved state and do a full refresh. (https://gitlab.com/meltano/meltano/-/merge_requests/2432#note_757893658)
    1. Top-level `--no-state-update` to not save state. (https://gitlab.com/meltano/meltano/-/merge_requests/2432#note_757893658)
1. Top-level `--force` to ignore locks or indications of already-running jobs. (https://gitlab.com/meltano/meltano/-/merge_requests/2432#note_757893658)
1. A ""dry run"" option ala `meltano run --dry-run ...` (https://gitlab.com/meltano/meltano/-/issues/2301#note_750941505)

Note: Behind the scenes - cleaning up the ELBContext -> ELTContext relationship and what singer plugins expect probably needs to be part of the Job support work.",12
98284875,2021-12-02 02:09:43.551,Document MacOS specific configuration step for contributing to the UI/API,"On MacOS BigSur (and up?) port 5000 is [used by Airplay/Control Center](https://developer.apple.com/forums/thread/682332). For new users setting up a development environment that can cause a somewhat hard to troubleshoot issue because `meltano ui` won't actually be able to start. It's easy enough to work around by [setting alternate bind ports](https://meltano.com/docs/settings.html#ui-bind-port), so we should just explicitly document this in the contributor guide - and flag it for MacOS users.

Long term we have https://gitlab.com/meltano/meltano/-/issues/3088 to try and auto-detect/warn if the port is unaivable, and to change the default port for new projects.",1
98280413,2021-12-01 23:35:50.436,Clarify venv usage by Meltano when inheriting with an environment,"Had a user ask today whether or not Meltano re-installed a plugin in a separate venv when a [Meltano Environment](https://meltano.com/docs/environments.html#inheritance) is used. I wasn't sure of the answer and the docs weren't clear.

@edgarrmondragon can you clarify this and make an MR to the docs?",1
98278569,2021-12-01 22:21:48.876,flakehell install failures,"Both @alex1126 and I are running into an issue where flakehell is failing to install while setting up the pre-commit hooks documented in the [contributor guide](https://meltano.com/docs/contributor-guide.html#setting-up-your-environment):

```shell
(melty-3.8.12) ➜  meltano git:(master) poetry run pre-commit install --install-hooks
pre-commit installed at .git/hooks/pre-commit
[INFO] Installing environment for https://github.com/life4/flakehell.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
An unexpected error has occurred: CalledProcessError: command: ('/Users/syn/.cache/pre-commit/repo72kapjci/py_env-python3.8/bin/python', '-mpip', 'install', '.')
return code: 1
expected return code: 0
stdout:
    Processing /Users/syn/.cache/pre-commit/repo72kapjci
      Installing build dependencies: started
      Installing build dependencies: finished with status 'done'
      Getting requirements to build wheel: started
      Getting requirements to build wheel: finished with status 'done'
      Installing backend dependencies: started
      Installing backend dependencies: finished with status 'done'
      Preparing metadata (pyproject.toml): started
      Preparing metadata (pyproject.toml): finished with status 'error'

stderr:
      ERROR: Command errored out with exit status 1:
       command: /Users/syn/.cache/pre-commit/repo72kapjci/py_env-python3.8/bin/python /Users/syn/.cache/pre-commit/repo72kapjci/py_env-python3.8/lib/python3.8/site-packages/pip/_vendor/pep517/in_process/_in_process.py prepare_metadata_for_build_wheel /var/folders/47/sydnn1xj5vv6krsn8n4krgjw0000gn/T/tmpet2njlov
           cwd: /Users/syn/.cache/pre-commit/repo72kapjci
      Complete output (36 lines):
      Traceback (most recent call last):
        File ""/Users/syn/.cache/pre-commit/repo72kapjci/py_env-python3.8/lib/python3.8/site-packages/pip/_vendor/pep517/in_process/_in_process.py"", line 363, in <module>
          main()
        File ""/Users/syn/.cache/pre-commit/repo72kapjci/py_env-python3.8/lib/python3.8/site-packages/pip/_vendor/pep517/in_process/_in_process.py"", line 345, in main
          json_out['return_val'] = hook(**hook_input['kwargs'])
        File ""/Users/syn/.cache/pre-commit/repo72kapjci/py_env-python3.8/lib/python3.8/site-packages/pip/_vendor/pep517/in_process/_in_process.py"", line 164, in prepare_metadata_for_build_wheel
          return hook(metadata_directory, config_settings)
        File ""/private/var/folders/47/sydnn1xj5vv6krsn8n4krgjw0000gn/T/pip-build-env-znhipti1/overlay/lib/python3.8/site-packages/flit_core/buildapi.py"", line 49, in prepare_metadata_for_build_wheel
          metadata = make_metadata(module, ini_info)
        File ""/private/var/folders/47/sydnn1xj5vv6krsn8n4krgjw0000gn/T/pip-build-env-znhipti1/overlay/lib/python3.8/site-packages/flit_core/common.py"", line 396, in make_metadata
          md_dict.update(get_info_from_module(module, ini_info.dynamic_metadata))
        File ""/private/var/folders/47/sydnn1xj5vv6krsn8n4krgjw0000gn/T/pip-build-env-znhipti1/overlay/lib/python3.8/site-packages/flit_core/common.py"", line 193, in get_info_from_module
          docstring, version = get_docstring_and_version_via_import(target)
        File ""/private/var/folders/47/sydnn1xj5vv6krsn8n4krgjw0000gn/T/pip-build-env-znhipti1/overlay/lib/python3.8/site-packages/flit_core/common.py"", line 169, in get_docstring_and_version_via_import
          m = sl.load_module()
        File ""<frozen importlib._bootstrap_external>"", line 522, in _check_name_wrapper
        File ""<frozen importlib._bootstrap_external>"", line 1022, in load_module
        File ""<frozen importlib._bootstrap_external>"", line 847, in load_module
        File ""<frozen importlib._bootstrap>"", line 265, in _load_module_shim
        File ""<frozen importlib._bootstrap>"", line 702, in _load
        File ""<frozen importlib._bootstrap>"", line 671, in _load_unlocked
        File ""<frozen importlib._bootstrap_external>"", line 843, in exec_module
        File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
        File ""/Users/syn/.cache/pre-commit/repo72kapjci/flakehell/__init__.py"", line 5, in <module>
          from ._cli import entrypoint, flake8_entrypoint
        File ""/Users/syn/.cache/pre-commit/repo72kapjci/flakehell/_cli.py"", line 9, in <module>
          from .commands import COMMANDS
        File ""/Users/syn/.cache/pre-commit/repo72kapjci/flakehell/commands/__init__.py"", line 5, in <module>
          from ._baseline import baseline_command
        File ""/Users/syn/.cache/pre-commit/repo72kapjci/flakehell/commands/_baseline.py"", line 6, in <module>
          from .._patched import FlakeHellApplication
        File ""/Users/syn/.cache/pre-commit/repo72kapjci/flakehell/_patched/__init__.py"", line 2, in <module>
          from ._app import FlakeHellApplication
        File ""/Users/syn/.cache/pre-commit/repo72kapjci/flakehell/_patched/_app.py"", line 10, in <module>
          from flake8.options.config import MergedConfigParser, get_local_plugins
      ImportError: cannot import name 'MergedConfigParser' from 'flake8.options.config' (/private/var/folders/47/sydnn1xj5vv6krsn8n4krgjw0000gn/T/pip-build-env-znhipti1/normal/lib/python3.8/site-packages/flake8/options/config.py)
```

This seems to be related to https://github.com/flakehell/flakehell/issues/22 but the versions don't actually match up:

```
(melty-3.8.12) ➜  meltano git:(master) poetry run flakehell --version
FlakeHell 0.7.1
Flake8    3.9.2
For plugins versions use flakehell plugins
```

Digging some more, @edgarrmondragon found that pre-commit actually creates it own venv's and doesn't take into account pyproject.toml - and so the latest versions of flakehell/flake8 are installed - which then trigger https://github.com/flakehell/flakehell/issues/22.",0
98196140,2021-11-30 19:32:18.459,Auto-detect UI port availability and possibly switch default for new projects,"On MacOS BigSur (and up?) port 5000 is [used by Airplay/Control Center](https://developer.apple.com/forums/thread/682332). For new users setting up a development environment that can cause a somewhat hard to troubleshoot issue because `meltano ui` won't actually be able to start. It's easy enough to work around by [setting alternate bind ports](https://meltano.com/docs/settings.html#ui-bind-port), so we could just explicitly document this in the contributor guide - and flagging it for MacOS users. 

Would it be worth considering changing the default api port from 5000 (and 8080 for the dev ui) to something like 8080 (and 8181 for the dev ui)?",4
98122488,2021-11-29 19:38:35.753,Update pinned version of target-bigquery,"Latest version of adswerve/target-bigquery is [0.11.3](https://github.com/adswerve/target-bigquery/releases), but the plugin in `discovery.yml` is pinned to an earlier version: `git+https://github.com/adswerve/target-bigquery.git@v0.10.2`.

There have been some important bug fixes since that, like https://github.com/adswerve/target-bigquery/issues/9.

cc @pnadolny13 @tayloramurphy",1
97586046,2021-11-18 22:29:16.071,Meltano version bumps and changelog grooming should not require AJ review,"Logging this issue to remove myself (AJ) from the weekly release process.

Related issue for SDK: https://gitlab.com/meltano/sdk/-/issues/277",2
97546181,2021-11-18 10:53:03.304,Document how STATE is handled in greater detail,"The specifics around handling of 'incomplete state' (state from previous failed jobs) is not included in the [relevant section of the docs.](https://meltano.com/docs/integration.html#incremental-replication-state).

Thread from [Slack](https://meltano.slack.com/archives/CMN8HELB0/p1637186511109700):

> I should know this already, but...  If a loader fails in an ELT job, what happens to the data and the state?  I reviewed the Meltano ELT reference architecture and it does not say, and I have not looked at the source.  Is the main Meltano process actually sitting in the middle and watching the messages from extractor stdout and sending them to loader stdin, and watching stdout of the target for that state to know that the target has processed, and stored, records up to that state?  I've had that assumption all along, but didn't really know.

> I know the feeling. It's one of those things that you really want to know is solid and understand, but I've also often had the feeling I didn't really understand what would happen in exceptional situations.
Taking the code is truth approach - these are the relevant places I think:
https://gitlab.com/meltano/meltano/-/blob/master/src/meltano/core/plugin/singer/target.py#L21
https://gitlab.com/meltano/meltano/-/blob/master/src/meltano/core/runner/singer.py#L90
As each STATE message flows through, meltano (or optionally delegated to the target it seems) writes the state.
Would be great to have one of the @meltano-engineering guys jump in and correct me, or even some links to detailed design docs! (edited)",2
98548093,2021-11-17 21:12:43.059,Allow users to name a default Environment in `meltano.yml`,"## Summary
[//]: # (Concisely summarize the feature you are proposing.)

Let users set the name of the default environment in `meltano.yml`.

## Proposed benefits
[//]: # (Concisely summarize the benefits this feature would bring to yourself and other users.)

Users don't need to pass `--environment` to every command or set the `MELTANO_ENVIRONMENT` variable.

## Proposal details
[//]: # (In as much detail as you are able, describe the feature you'd like to build or would like to see built.)

```yaml
version: 1
default_environment: dev

environments:
- name: dev
  ...
- name: prod
  ...
```

means commands will use `dev` if `--environment` is not passed.

## Best reasons not to build
[//]: # (Will this negatively affect any existing functionality? Do you anticipate any breaking changes versus what may already be working today? Make the counter-argument to your proposal here.)

Can confuse users about which environment will be activated if they use `--environment` and `default_environment` at the same time, but that can easily addressed in the docs and clearly giving precedence to the former.",4
97509233,2021-11-17 20:53:23.932,Support a --log-format short cut,"Per @rabidaudio in https://gitlab.com/meltano/meltano/-/merge_requests/2395#note_731245620

> Would it be possible to configure the log format from CLI args? It seems like the way to configure it as designed now is through the log handler files. It seems clunky to ask users to turn on structured logs everywhere just so they show up nicely in Dagster. I suppose our Dagster pipelines could generate an alternate `logging.yaml` file and pass with `--log-config`, would this be the recommended way?

We could definitely add a basic `--log-format=[json|keyvalue|uncolored-console]` flag that switches meltano to use one of our other built-in formatters for the console logger *if* there's no logging config present. 

It would be a nice shortcut for scenarios where folks don't want to supply a more elaborate logging config, and really just want to be able to switch from a human-friendly format to a machine-friendly format.",2
97423721,2021-11-16 13:22:57.314,Support `meltano shell` command to spawn terminal in plugin context,"## Change Log

- 2021-11-17: Added feedback from @edgarrmondragon and Docker ideas.

### Problem to solve

In order to improve productivity of developers using plugins within a Meltano Project, we could support a `meltano shell` command that spawns a terminal session inside the plugins context. This enables users to benefit from Meltanos config, virtual-env and environment management features in a simple and intuitive way (i.e. without having to prefix each plugin invocation with `meltano invoke`).

### Target audience

Developers, Data Engineers, Data Scientists and Analytics Engineers working within a Meltano Project.

### Further details

This would compliment the recently added `--environments` [feature](https://meltano.com/docs/environments.html#environments), allowing users to get on with _using_ wrapped tools without having to worry about configuration, virtual-env and dev/prod environment differences. It also cuts down on redundant setup/teardown operations on each `meltano invoke` invocation (e.g. Airflow `initdb` [here](https://gitlab.com/meltano/meltano/-/blob/master/src/meltano/core/plugin/airflow.py#L108)), and allows us to set up other convenience features like autocomplete etc. in the shell context. Users may wish to have control of the development environment for each plugin, including adding additional tools for tests, linting etc. (e.g. SQLFluff with dbt).

This may also be an opportunity to support Docker containers for development. `meltano shell` may just as easily implement an equivalent of `docker run -it fishtownanalytics/dbt` as `source /path/to/dbt/venv/bin/activate`. This may reduce the impact of cross-platform tooling support and configuration differences.

### Proposal

Example workflow for adding a new data integration using Meltano, dbt and Airflow:

```shell
$ git checkout -b 2345_add_gitlab_to_hub_pipeline

# find a tap that matches the source I want to integrate
$ meltano discover extractors
> tap-gitlab
> ...

# add and configure the tap
$ meltano add extractor tap-gitlab
$ meltano config tap-gitlab set projects ""meltano/meltano meltano/sdk""
$ meltano config tap-gitlab set start_date 2021-10-01T00:00:00Z

# now lets create a schedule
$ meltano schedule gitlab-to-postgres tap-gitlab target-postgres @daily

# and run a sample extract in my development environment, so dbt has data
$ meltano --environment=dev_kp elt tap-gitlab target-postgres --job_id=gitlab-to-postgres

# with a new extractor added and data extracted,
# lets update our dbt models for the new source...

# activate a console within the 'dbt' virtual env created by Meltano Project,
# using secrets/env vars from my personal dev environment
$ meltano shell dbt --environment=dev_kp

# luckily a postgres-compatible dbt package exists for the tap I just added 😅
$ nano packages.yml
> packages:
  - package: dbt-labs/gitlab
    version: 0.7.0

$ dbt deps

# run the new package using creds from my dev_kp environment
$ dbt run --select gitlab 

# done (optimistically)
$ exit
> back in parent project context

# finally, lets update my hub DAG in Meltano Project (doesn't exist yet - see [#2920](https://gitlab.com/meltano/meltano/-/issues/2920))
$ nano meltano.yml
> ...
dags:
  meltano_hub:
    interval: @daily
    steps:
      - name: gitlab
        cmd: 'meltano elt tap-gitlab target-postgres --job_id=gitlab-to-postgres'
        retries: 2
      - name: google_analytics
        cmd: 'meltano elt tap-google-analytics target-postgres --job_id=google-analytics-to-postgres'
      - name: product_mart
        cmd: 'meltano invoke dbt run --select models/marts/product'
        depends_on:
          - gitlab
          - google_analytics
...

# wonder if the new DAG actually works?
# start a terminal session in the Airflow context
$ meltano shell airflow --environment=dev_kp
# Note: this could launch a local instance of `airflow webserver`
# for developer convenience 🤔

$ airflow dags list
> meltano_hub

$ airflow dags trigger meltano_hub
> runs DAG locally using config specified in dev_kp environment

# *goes and checks the Airflow UI on http://localhost:8080 and the test warehouse for data*

# done
$ exit

# lets see what the team thinks
$ git commit -am ""added gitlab integration""
$ git push
$ glab mr create
# off it goes to CI ✉️

# profit ☕️
```

### What does success look like, and how can we measure that?

Users can work productively in a plugins context facilitated (and not obstructed) by Meltano Project. New users to a Project do not need to worry about the mechanics of installation and configuration; onboarding to an existing Project is as easy as `git clone` and `meltano shell <plugin name>` 🚀

### Regression test
(Ensure the feature doesn't cause any regressions)
- [ ] Write adequate test cases and submit test results
- [ ] Test results should be reviewed by a person from the team

### Links / references

- [#2920](https://gitlab.com/meltano/meltano/-/issues/2920) Meltano should output a DAG-like data structure with individual pipeline components
- [&82](https://gitlab.com/groups/meltano/-/epics/82) Improved dbt integration
- [#2210](https://gitlab.com/meltano/meltano/-/issues/2205) Add `docker_image` property to support non-Python/pip plugins

_Please note that this was taken from GitLab, to be changed accordingly_

## Option: Alternative framing around `meltano environment activate ...`

We should consider also if this could be an environment-level invocation, such as `meltano environment activate`. Which would be a much more robust version of sourcing the `.env` variables, but able to step into a context where all invocations are already contextualized and able to be invoked directly.

If an environment-level action, we get around issues of having to step separately into dbt and sqlfluff context separately, because all of the context for both would be set at time of activating the environment.",12
97392485,2021-11-16 01:58:12.557,State lost after ETL exception,"### What is the current *bug* behavior?
The incremental state seems to have been completely lost, forcing a full re-sync of the DB.


### What is the expected *correct* behavior?
Meltano should always use the stored state.


### Steps to reproduce
I don't have a repro I'm afraid.



### Relevant logs and/or screenshots

```
# meltano --version
meltano, version 1.69.0
```

Running in a Docker container under GKE.

The last logs I got were:

```
meltano                | Loading failed (2): CRITICAL ['Traceback (most recent call last):
', '  File ""/project/.meltano/loaders/target-bigquery/venv/lib/python3.8/site-packages/urllib3/connectionpool.py"", line 445, in _make_request
    six.raise_from(e, None)
', '  File ""<string>"", line 3, in raise_from
', '  File ""/project/.meltano/loaders/target-bigquery/venv/lib/python3.8/site-packages/urllib3/connectionpool.py"", line 440, in _make_request
    httplib_response = conn.getresponse()
', '  File ""/usr/local/lib/python3.8/http/client.py"", line 1347, in getresponse
    response.begin()
', '  File ""/usr/local/lib/python3.8/http/client.py"", line 307, in begin
    version, status, reason = self._read_status()
', '  File ""/usr/local/lib/python3.8/http/client.py"", line 268, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1"")
', '  File ""/usr/local/lib/python3.8/socket.py"", line 669, in readinto
    return self._sock.recv_into(b)
', '  File ""/usr/local/lib/python3.8/ssl.py"", line 1241, in recv_into
    return self.read(nbytes, buffer)
', '  File ""/usr/local/lib/python3.8/ssl.py"", line 1099, in read
    return self._sslobj.read(len, buffer)
', 'socket.timeout: The read operation timed out
', '
During handling of the above exception, another exception occurred:

', 'Traceback (most recent call last):
', '  File ""/project/.meltano/loaders/target-bigquery/venv/lib/python3.8/site-packages/requests/adapters.py"", line 439, in send
    resp = conn.urlopen(
', '  File ""/project/.meltano/loaders/target-bigquery/venv/lib/python3.8/site-packages/urllib3/connectionpool.py"", line 755, in urlopen
    retries = retries.increment(
', '  File ""/project/.meltano/loaders/target-bigquery/venv/lib/python3.8/site-packages/urllib3/util/retry.py"", line 532, in increment
    raise six.reraise(type(error), error, _stacktrace)
', '  File ""/project/.meltano/loaders/target-bigquery/venv/lib/python3.8/site-packages/urllib3/packages/six.py"", line 770, in reraise
    raise value
', '  File ""/project/.meltano/loaders/target-bigquery/venv/lib/python3.8/site-packages/urllib3/connectionpool.py"", line 699, in urlopen
    httplib_response = self._make_request(
', '  File ""/project/.meltano/loaders/target-bigquery/venv/lib/python3.8/site-packages/urllib3/connectionpool.py"", line 447, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
', '  File ""/project/.meltano/loaders/target-bigquery/venv/lib/python3.8/site-packages/urllib3/connectionpool.py"", line 336, in _raise_timeout
    raise ReadTimeoutError(
', ""urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='bigquery.googleapis.com', port=443): Read timed out. (read timeout=60)
"", '
During handling of the above exception, another exception occurred:

', 'Traceback (most recent call last):
', '  File ""/project/.meltano/loaders/target-bigquery/venv/lib/python3.8/site-packages/target_bigquery/__init__.py"", line 92, in main
    for state in state_iterator:
', '  File ""/project/.meltano/loaders/target-bigquery/venv/lib/python3.8/site-packages/target_bigquery/process.py"", line 55, in process
    for s in handler.on_stream_end():
', '  File ""/project/.meltano/loaders/target-bigquery/venv/lib/python3.8/site-packages/target_bigquery/processhandler.py"", line 261, in on_stream_end
    self._do_temp_table_based_load(rows)
', '  File ""/project/.meltano/loaders/target-bigquery/venv/lib/python3.8/site-packages/target_bigquery/processhandler.py"", line 168, in _do_temp_table_based_load
    raise e
', '  File ""/project/.meltano/loaders/target-bigquery/venv/lib/python3.8/site-packages/target_bigquery/processhandler.py"", line 136, in _do_temp_table_based_load
    job = self._load_to_bq(
', '  File ""/project/.meltano/loaders/target-bigquery/venv/lib/python3.8/site-packages/target_bigquery/processhandler.py"", line 215, in _load_to_bq
    load_job = client.load_table_from_file(
', '  File ""/project/.meltano/loaders/target-bigquery/venv/lib/python3.8/site-packages/google/cloud/bigquery/client.py"", line 1760, in load_table_from_file
    response = self._do_resumable_upload(
', '  File ""/project/.meltano/loaders/target-bigquery/venv/lib/python3.8/site-packages/google/cloud/bigquery/client.py"", line 2085, in _do_resumable_upload
    upload, transport = self._initiate_resumable_upload(
', '  File ""/project/.meltano/loaders/target-bigquery/venv/lib/python3.8/site-packages/google/cloud/bigquery/client.py"", line 2127, in _initiate_resumable_upload
    upload.initiate(
', '  File ""/project/.meltano/loaders/target-bigquery/venv/lib/python3.8/site-packages/google/resumable_media/requests/upload.py"", line 345, in initiate
    response = _helpers.http_request(
', '  File ""/project/.meltano/loaders/target-bigquery/venv/lib/python3.8/site-packages/google/resumable_media/requests/_helpers.py"", line 136, in http_request
    return _helpers.wait_and_retry(func, RequestsMixin._get_status_code, retry_strategy)
', '  File ""/project/.meltano/loaders/target-bigquery/venv/lib/python3.8/site-packages/google/resumable_media/_helpers.py"", line 150, in wait_and_retry
    response = func()
', '  File ""/project/.meltano/loaders/target-bigquery/venv/lib/python3.8/site-packages/google/auth/transport/requests.py"", line 480, in request
    response = super(AuthorizedSession, self).request(
', '  File ""/project/.meltano/loaders/target-bigquery/venv/lib/python3.8/site-packages/requests/sessions.py"", line 542, in request
    resp = self.send(prep, **send_kwargs)
', '  File ""/project/.meltano/loaders/target-bigquery/venv/lib/python3.8/site-packages/requests/sessions.py"", line 655, in send
    r = adapter.send(request, **kwargs)
', '  File ""/project/.meltano/loaders/target-bigquery/venv/lib/python3.8/site-packages/requests/adapters.py"", line 529, in send
    raise ReadTimeout(e, request=request)
', ""requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='bigquery.googleapis.com', port=443): Read timed out. (read timeout=60)
""]
meltano                | ELT could not be completed: Loader failed
ELT could not be completed: Loader failed
```

It seems the proximal cause is a timeout in BigQuery, but this appears to have caused Meltano to corrupt the state DB.

The first logs from the next ELT run are:

```
meltano                | Running extract & load...
meltano                | Found state from 2021-11-15 23:15:20.194728.
meltano                | No state was found, complete import.
tap-mysql--incremental | time=2021-11-15 23:45:21 name=tap_mysql level=INFO message=Server Parameters: version: 5.7.34-google, wait_timeout: 28000, innodb_lock_wait_timeout: 50, max_allowed_packet: 33554432, interactive_timeout: 28800
tap-mysql--incremental | time=2021-11-15 23:45:21 name=tap_mysql level=INFO message=Server SSL Parameters(blank means SSL is not active): [ssl_version: ], [ssl_cipher: ]
tap-mysql--incremental | time=2021-11-15 23:45:21 name=tap_mysql level=WARNING message=Columns {'id'} are primary keys but were not selected. Adding them.
tap-mysql--incremental | time=2021-11-15 23:45:22 name=tap_mysql level=INFO message=Beginning sync for InnoDB table qwil.admin_comments_comment
tap-mysql--incremental | time=2021-11-15 23:45:22 name=tap_mysql level=INFO message=Stream qwil-admin_comments_comment is using full table replication
tap-mysql--incremental | time=2021-11-15 23:45:22 name=tap_mysql level=INFO message=Detected auto-incrementing primary key(s) - will replicate incrementally
tap-mysql--incremental | time=2021-11-15 23:45:22 name=tap_mysql level=INFO message=Running SELECT `content_type_id`,`object_id`,`time`,`comment`,`id`,`user_id` FROM `qwil`.`admin_comments_comment` WHERE `id` <= 2789 ORDER BY `id` ASC
tap-mysql--incremental | /project/.meltano/extractors/tap-mysql--incremental/venv/lib/python3.8/site-packages/pymysql/connections.py:1077: UserWarning: Previous unbuffered result was left incomplete
tap-mysql--incremental |   warnings.warn(""Previous unbuffered result was left incomplete"")
```

The ""previous unbuffered result was left incomplete"" log sounds pretty suspicious, though it is coming from the `tap-mysql` and not meltano so perhaps it's a red herring. However you can clearly see that Meltano is a bit confused with the ""found state"" => ""no state found"" logs.

### Possible fixes
Is it possible that Meltano isn't flushing its writes to the state DB, such that on a hard crash it is corrupting the DB?


### Further regression test",4
97038222,2021-11-09 19:20:44.213,Dumping state as part of an elt call should pull directly from DB,"In https://meltano.slack.com/archives/C01PKLU5D1R/p1636061054200300 while troubleshooting a state issue related to https://gitlab.com/meltano/meltano/-/issues/3045 folks ran into issues with the `--dump=state` command failing. This was a bit of a red herring or more a side effect - with the command simply complaining that the state file couldn't be read. When in reality the state capability wasn't being advertised by the plugin and so no state file was even being produced. 

While investigating that we realized the `meltano elt --dump=state` wasn't actually hitting the system db directly. It indirectly makes a bare `_invoke()` call using [plugin_invoker.dump](https://gitlab.com/meltano/meltano/-/blob/master/src/meltano/core/plugin_invoker.py#L288), which behind the scenes triggers the [look_up_state_hook](https://gitlab.com/meltano/meltano/-/blob/master/src/meltano/core/plugin/singer/tap.py#L165) - which in turn tries to read the state from the db, and if successful then [writes out the state to the state file](https://gitlab.com/meltano/meltano/-/blob/master/src/meltano/core/plugin/singer/tap.py#L246). After `_invoke()` completes, dump just tries to the read state file.

1. If we can deprecate/phase out the use of a state file, we could just update have the dump pull the state straight from the system db and go ahead and simplify the code in the tap ditching the state file generation code. 
2. If we need to continue generating the state file - dump could still do that - but we'd definitely want to refactor dump and look_up_state_hook to share some code. 

With either path, a single consolidated look_up_state method for both the tap and dump to use is probably the move.",4
96974985,2021-11-09 01:07:15.626,"Meltano should print a warning if any of these are missing from capabilities: (`state`, `discover`, (`catalog` XOR `properties`))","There are enough instances of expensive debug operations at this point (most recently [today](https://meltano.slack.com/archives/CKHP6G5V4/p1636419311194200?thread_ts=1636399782.175500&cid=CKHP6G5V4)), that I think we should be printing an explicit warning message whenever any of these capabilities are not found in `discovery.yml`:

1. If `state` capability is not declared, then during `meltano elt`:
    > Warning: Capability 'state' is not declared for `{tap-name}`. Incremental sync bookmarks will be disabled as a result.
2. If `discover` capability is not declared, then during `meltano elt` and `meltano invoke`:
    > Warning: Capability 'discover' is not declared for `{tap-name}`. Proceeding without catalog discovery.
3. If neither `catalog` nor `properties` capability is declared, then during `meltano invoke` and `meltano etl`:
    > Warning: Capabilities `catalog` and `properties` are both undeclared for `{tap-name}`. Catalog and selection features are disabled.
4. During `add --custom`, set default capabilities to `discover,state,catalog` rather than `[]`.
5. Warn if `capabilities` is fully empty, during `meltano invoke` and perhaps also during `meltano install`.
6. `--dump` commands should fail immediately with a clear message about missing declared capabilities.
    1. `--dump=catalog` should fail hard for missing `discover`, should warn for missing `catalog`.
    1. `--dump=state` should fail hard for missing `state`.


I think these should probably print even when `--log-level=debug` is not set.",4
96808344,2021-11-05 15:57:36.935,One-Step Publish for Meltano and SDK,"We have a one-click prerelease publish in the SDK. That's a simpler problem because we don't have release notes and the version number is just an auto-generated `dev` bump using the CI build number.
 
Opening this issue to figure out how we get to similar simplicity for normal deployments.

## Goals

1. Continue frequent releases.
2. Reduce cost to dev productivity.
3. Make dual-channel releases (`edge`/`main`) more viable, as proposed in https://gitlab.com/meltano/meta/-/issues/135

## Statu Quo

Here are the current manual deployments steps (excluding marketing/slack/tweets, etc.):

1. Open a release branch. (Triggers an auto-commit with the version bump.)
2. Open a tag from the release branch. (Triggers the PyPi publish.)
3. Groom the changelog. (Can be performed before, after, or in parallel with the above - since the changelog itself is not shipped.)

## Ideal State

### Changelog automation

I don't know as much about this space but I believe there are some available options.

### One-Step version bump, validation, and publish

Rather than requiring the tag to be created explicitly in a separate step, we can modify the CI pipeline to _also_ run the publish step in the special CI job that runs when a new branch is created like `release/*`.

The special CI pipeline for `release/*` tags can be removed in favor of the branch pipeline, to eliminate the need for a second step, and avoid ""chaining"" of auto-commits and CI jobs. We could still create the tag as part of the CI pipeline, rather than as a manual step.

### Proposed flow

Releases:

1. As release manager, I create a correctly-named release branch which triggers the release pipeline.
2. I click ""Merge when pipeline succeeds"" immediately - meaning, a successful CI run (including publish steps) will close out the version bump.

Change Log Grooming:

1. If needed, I create and merge a _separate_ changelog grooming MR.",8
96807828,2021-11-05 15:46:54.033,Block interface tests and mvp implementation,"Issue to discuss and track our first block implementation. 


**Draft/placeholder - actively editing ~Florian**

----

#### Mergeable/deliverables:

- [x] ExtractLoadBlocks
- [x] PluginCommandBlock
- [x] IOBlock - which is parent of:
  - [ ] PublisherBlock
  - [ ] ConsumerBlock
  - [ ] TranslatorBlock
- [x] sample unit tests demonstrating construciton of and use of the ExtractLoadBlocks",12
96622206,2021-11-02 21:22:40.816,Support a Python-based Plugin Architecture,"Currently all Python-specific plugin handlers live in our main `Meltano` repo and is installed in a single go.

To make a leaner install, we could (for instance) refactor ""Singer EL"", ""dbt Transform"", and ""Airflow Orchestrator"" into separate repositories.

For each type of plugin we want to support, we'd have to create an interface layer and decide which hooks/notifications/invocations are needed by the respective type of plugin.

Note: The refactoring effort for the Singer EL would likely be very large. It might make more sense to use orchestrators (Dagster or Airflow) first.

## How is this different?

Today, if we want to support Dagster in the same way we support Airflow, we have three levers we can control:

1. CLI-based 'command' definitions. This is basically just aliasing existing CLI commands from the tool. Not capable of deep integrations.
2. Files-based plugins. This is the least robust: we download some pre-configured files into the repo.
3. Python code-based approach. This is the only ""deep"" integration option we have today. This gives a lot of power to wrap the tool's capabilities, but it also requires expanding the code base of `Meltano` core repo.

This proposal would give an alternate path to the third option, enabling us to write an external plugin definition in python, and import it into the project without having it defined tightly as part of that repo.",40
96616553,2021-11-02 18:56:27.586,Prettier exception prints,"On the structlog site, there's some example screenshots of exceptions that are inherently very easy to read due to formatting and structure. 

Should we look at adopting something like [rich](https://rich.readthedocs.io/) or [better-exceptions](https://github.com/Qix-/better-exceptions) for Meltano and/or the SDK?

The docs for structlog seem to imply we only need to install the libraries, but I expect it may be slightly more work, and there could be reasons I'm not thinking of why we would not want to use them.

https://www.structlog.org/en/stable/development.html:

![image](/uploads/3e18f622a003275ef6358c18dab5977c/image.png)",2
96352662,2021-10-28 20:23:09.893,elt jobs appear to complete successfully but exit with return code 1 occasionally,"<!---
Please read this!

Before opening a new issue, make sure to search for keywords in the issues
filtered by the ""regression"" or ""bug"" label and verify the issue you're about to submit isn't a duplicate.

If you are submitting an issue with a tap, please include:

- account details
- target details
- entities selected  with meltano select (if you have selected any entities), as the bug may be related to a specific entity
- the full elt command you are running
- full output of the meltano elt command. Logs can get pretty long, so you can add the full log as a snippet in the Meltano project and add a link in the issue.
--->

### What is the current *bug* behavior?
As reported by a user in slack (https://meltano.slack.com/archives/C01TCRBBJD7/p1635428092076900). They're intermittently running into a situation where elt commands appear based on the output logs that they've completed successfully, but we actually have n exit status/return code of 1. 

They also reported that the job is marked as failed in the meltano db with the error message:

```
{""error"": ""No heartbeat recorded for 5 minutes. The process was likely killed unceremoniously.""
```


### What is the expected *correct* behavior?
_What should be happening?_



### Steps to reproduce

Unknown/TBD at this point.



### Relevant logs and/or screenshots

```
[2021-10-28 08:18:56,028] {bash_operator.py:157} INFO - meltano                      | elt       | Incremental state has been updated at 2021-10-28 08:18:56.027750.
[2021-10-28 08:18:58,706] {bash_operator.py:157} INFO - meltano                      | elt       | Extract & load complete!
[2021-10-28 08:18:58,707] {bash_operator.py:157} INFO - meltano                      | elt       | Transformation skipped.
[2021-10-28 08:18:58,727] {bash_operator.py:157} INFO - (<State.FAIL: (3, ('RUNNING',))>, <State.SUCCESS: (2, ())>)
[2021-10-28 08:19:00,227] {bash_operator.py:161} INFO - Command exited with return code 1
[2021-10-28 08:19:00,385] {taskinstance.py:1150} ERROR - Bash command failed
Traceback (most recent call last):
  File ""/project/.meltano/orchestrators/airflow/venv/lib/python3.6/site-packages/airflow/models/taskinstance.py"", line 984, in _run_raw_task
    result = task_copy.execute(context=context)
  File ""/project/.meltano/orchestrators/airflow/venv/lib/python3.6/site-packages/airflow/operators/bash_operator.py"", line 165, in execute
    raise AirflowException(""Bash command failed"")
airflow.exceptions.AirflowException: Bash command failed
[2021-10-28 08:19:00,387] {taskinstance.py:1194} INFO - Marking task as FAILED. dag_id=meltano_secondary_full_table, task_id=extract_load, execution_date=20211027T070000, start_date=20211028T074156, end_date=20211028T081900
[2021-10-28 08:19:01,488] {local_task_job.py:102} INFO - Task exited with return code 1
```


### Possible fixes




### Further regression test
_Ensure we automatically catch similar issues in the future_

- [ ] Write additional adequate test cases and submit test results
- [ ] Test results should be reviewed by a person from the team",4
96352217,2021-10-28 20:12:20.489,Add lock artifacts during `meltano add` for stability and portability of plugin definitions,"This has come up twice this week, especially in context of pipelines and integrations with external tools.

## Original proposal

Original proposal recommends an actual `.lock.yml` file.

<details><summary>Click to expand</summary>

1. `meltano lock` would generate or update a lock file - also in yaml format.
2. This would resolve all dependencies, including the `discovery.yml` ref and combination of all `include-path` files for multi-meltano files.

In future iterations this could be expanded to:

3. Include `catalog.json` caches.
4. Resolve all config representations of `environments` so that each environment has a fully resolved schema (_except_ secrets and the content of any externally-managed environment variables.)~~ - Not needed in initial release.
5. Optionally, include rich information of the DAG structure for known/declared pipelines or schedules.

Based on this lock file artifact, external tools such as Terraform, Databand, etc. could build environments and/or obtain rich project metadata needed for integration.

This may also provide improved stability projects for projects running in the wild, as discussed in this comment: https://gitlab.com/groups/meltano/-/epics/128#note_734475476

</details>

### Alternate implementation option (2022-02-10)

Here's an alternative that just focuses on freezing the plugin definitions, ideally copying those verbatim from their source of truth on hub.meltano.com

#### Stable Installations via `meltano add`

We alter how `meltano add` installs new plugins:

1. Instead of installing directly from `discovery.yml` and relying on the `discovery.yml` definition for resolution, we go to https://hub.meltano.com and download the file associated with the plugin.
2. The downloaded definition file will be installed into `${MELTANO_PROJECT_DIR}/plugins/` as `${MELTANO_PROJECT_DIR}/plugins/extractors/tap-mytap--variantname.yml`.
3. An optional `plugin_definitions_path: <str>` config option in `meltano.yml` will allow users to change the default path. This would simply default to `${MELTANO_PROJECT_DIR}/plugins` if not set by the user.
    - This is separate from [include_paths](https://docs.meltano.com/concepts/project#multiple-yaml-files) in `meltano.yml`, since (1) we need to know exactly where to download new definitions and (2) the plugin definition files adhere to a new plugin spec format and not to the `meltano.yml` project format.
4. After downloading the definition, the plugin entry will be added to `meltano.yml` exactly as is performed today.
5. Instead of resolving setting definitions via `discovery.yml`, `meltano.yml` will only refer to the downloaded definitions.
    - This will require a one-time update to our parser to also interpret and import data from the native/standard plugin definitions from hub.meltano.com.

#### Definition updates via `meltano add --upgrade`

We add something like `meltano add extractor tap-mytap --upgrade` which will replace the current definition with the new definition.

#### Migration to locked definitions via `meltano install --lock` or `meltano install --freeze`

For projects already in the wild, we can add a `--lock` or `--freeze` option to `meltano install` which generates a ""locked"" plugin definition file for any which are missing. As above, this would create one file per plugin used in the project. If a plugin definition already exists, we skip adding that file.

#### ""Single-file option""

I think we've declined this option so I'm collapsing it. Please sound off in comments if you have a strong preference otherwise.

<details><summary>Click to expand</summary>

~~As an alternative to downloading and adding files individually, we could follow basically the same behavior as described, except we import/merge definitions in a single file called something at the root of the Meltano project dir like `plugins-registry.yml`, `registered-plugins.yml`, or `plugins-definitions.yml`. Since it would be a single file, we could have it at a fixed path relative to the root, and perhaps in the future allow the user to specify their own path to that file.

(AJ Speaking) I'm personally less inclined towards the single-lock file approach after thinking on this more. The one-file-per-plugin approach creates cleaner file diffs and is more scalable for a large number of plugins.~~

</details>

#### Deprecating `discovery.yml`

After implementing this proposal, `discovery.yml` as it exists today would be fully deprecated. Projects which have not yet had their definitions locked would still fall back to the bundled `discovery.yml` file. For the purposes of not breaking users on older versions, we would simply leave the online version of this file (https://discovery.meltano.com/discovery.yml) frozen at a point in time without needing to continue maintaining updates in `discovery.yml`.

#### Spec versioning

As @aphethean calls out in a thread on this issue https://gitlab.com/meltano/meltano/-/issues/3031#note_842196683, we also need to handle spec versioning so that the file spec can evolve without breaking users.

For instance, @aphethean provides a sample used by Kubernetes (`apiVersion: networking.k8s.io/v1beta1`) and we can see something similar in `dbt`'s use of `config-version: 2`.

This makes sure we meet this requirement as expressed in the comment:

> 1. ‘meltano add extractor tap-csv’
> 1. Aeons later, tap-csv plugin definition will always work with my meltano project.",12
96211878,2021-10-27 02:41:01.317,CLI command `meltano upgrade` does not work when installed with `pipx`,"There appears to be a bug in `meltano upgrade` when installed via `pipx install`.

1. Install with pipx: `pipx install meltano`.
2. Wait some time.
3. Print version to confirm version is stale. (`1.82.0` below)
4. Run `meltano upgrade`. Logs appear to successfully upgrade to the latest.
5. Print version (`meltano --version`) to confirm version is still stale. (still `1.82.0` in log below, should be `1.85.0`)

## Expected behavior

We should make `meltano upgrade` work with pipx-installed instances of `meltano`.

If not feasible, or as a stop gap, we could print a warning:

`Error. Detected pipx-managed environment. Please retry upgrade by running 'pipx upgrade meltano'.`

## Workaround

The simple workaround is just to use `pipx upgrade meltano` instead of `meltano upgrade`.

## Log

<details><summary>Click to expand</summary>

```console
aj@ajs-macbook-pro meltano % meltano --version
meltano, version 1.82.0
aj@ajs-macbook-pro meltano % meltano upgrade  
Upgrading `meltano` package...
Collecting meltano
  Downloading meltano-1.85.0.tar.gz (3.7 MB)
     |████████████████████████████████| 3.7 MB 2.0 MB/s 
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
    Preparing wheel metadata ... done
Requirement already satisfied: networkx<3.0,>=2.2 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano) (2.6.2)
Requirement already satisfied: atomicwrites<2.0.0,>=1.2.1 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano) (1.4.0)
Requirement already satisfied: sqlalchemy<2.0.0,>=1.3.19 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano) (1.4.22)
Requirement already satisfied: click<8.0,>=7.0 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano) (7.1.2)
Requirement already satisfied: async_generator<2.0,>=1.10 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano) (1.10)
Requirement already satisfied: pyhocon<0.4.0,>=0.3.51 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano) (0.3.58)
Collecting jsonschema<4.0.0,>=3.0.0
  Using cached jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)
Requirement already satisfied: gunicorn<20.0.0,>=19.9.0 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano) (19.10.0)
Requirement already satisfied: flask-executor<0.10.0,>=0.9.2 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano) (0.9.4)
Requirement already satisfied: flask<2,>=1 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano) (1.1.4)
Requirement already satisfied: ipython<8.0.0,>=7.5.0 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano) (7.26.0)
Requirement already satisfied: psycopg2-binary<3.0.0,>=2.8.5 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano) (2.9.1)
Requirement already satisfied: python-dotenv<0.15.0,>=0.14.0 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano) (0.14.0)
Requirement already satisfied: simplejson<4.0.0,>=3.16.0 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano) (3.17.3)
Requirement already satisfied: pyhumps==1.2.2 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano) (1.2.2)
Requirement already satisfied: smtpapi<0.5.0,>=0.4.1 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano) (0.4.7)
Requirement already satisfied: pyyaml<6.0.0,>=5.3.1 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano) (5.4.1)
Collecting werkzeug<2,>=1
  Using cached Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)
Requirement already satisfied: aiohttp<4.0.0,>=3.4.4 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano) (3.7.4.post0)
Requirement already satisfied: flatten-dict<0.2.0,>=0.1.0 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano) (0.1.0)
Requirement already satisfied: sqlparse<0.4.0,>=0.3.0 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano) (0.3.1)
Requirement already satisfied: click-default-group<2.0.0,>=1.2.1 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano) (1.2.2)
Requirement already satisfied: requests<3.0.0,>=2.23.0 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano) (2.26.0)
Requirement already satisfied: flask-cors<4.0.0,>=3.0.7 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano) (3.0.10)
Requirement already satisfied: alembic<2.0.0,>=1.5.0 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano) (1.6.5)
Requirement already satisfied: bcrypt<4.0.0,>=3.2.0 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano) (3.2.0)
Requirement already satisfied: authlib<0.11,>=0.10 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano) (0.10)
Requirement already satisfied: flask-restful<0.4.0,>=0.3.7 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano) (0.3.9)
Requirement already satisfied: psutil<6.0.0,>=5.6.3 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano) (5.8.0)
Requirement already satisfied: snowflake-sqlalchemy<2.0.0,>=1.2.3 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano) (1.3.1)
Requirement already satisfied: markdown<4.0.0,>=3.0.1 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano) (3.3.4)
Requirement already satisfied: fasteners<0.16.0,>=0.15.0 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano) (0.15)
Requirement already satisfied: meltano-flask-security<0.2.0,>=0.1.0 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano) (0.1.0)
Requirement already satisfied: email-validator<2.0.0,>=1.1.2 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano) (1.1.3)
Requirement already satisfied: pypika<0.26.0,>=0.25.1 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano) (0.25.2)
Requirement already satisfied: watchdog<0.10.0,>=0.9.0 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano) (0.9.0)
Requirement already satisfied: python-gitlab<2.0.0,>=1.8.0 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano) (1.15.0)
Requirement already satisfied: flask-sqlalchemy<3.0.0,>=2.4.4 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano) (2.5.1)
Requirement already satisfied: attrs>=17.3.0 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.4.4->meltano) (21.2.0)
Requirement already satisfied: chardet<5.0,>=2.0 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.4.4->meltano) (4.0.0)
Requirement already satisfied: async-timeout<4.0,>=3.0 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.4.4->meltano) (3.0.1)
Requirement already satisfied: yarl<2.0,>=1.0 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.4.4->meltano) (1.6.3)
Requirement already satisfied: typing-extensions>=3.6.5 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.4.4->meltano) (3.10.0.0)
Requirement already satisfied: multidict<7.0,>=4.5 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.4.4->meltano) (5.1.0)
Requirement already satisfied: python-editor>=0.3 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from alembic<2.0.0,>=1.5.0->meltano) (1.0.4)
Requirement already satisfied: Mako in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from alembic<2.0.0,>=1.5.0->meltano) (1.1.4)
Requirement already satisfied: python-dateutil in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from alembic<2.0.0,>=1.5.0->meltano) (2.8.2)
Requirement already satisfied: cryptography in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from authlib<0.11,>=0.10->meltano) (3.4.7)
Requirement already satisfied: six>=1.4.1 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from bcrypt<4.0.0,>=3.2.0->meltano) (1.16.0)
Requirement already satisfied: cffi>=1.1 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from bcrypt<4.0.0,>=3.2.0->meltano) (1.14.6)
Requirement already satisfied: pycparser in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from cffi>=1.1->bcrypt<4.0.0,>=3.2.0->meltano) (2.20)
Requirement already satisfied: dnspython>=1.15.0 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from email-validator<2.0.0,>=1.1.2->meltano) (2.1.0)
Requirement already satisfied: idna>=2.0.0 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from email-validator<2.0.0,>=1.1.2->meltano) (3.2)
Requirement already satisfied: monotonic>=0.1 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from fasteners<0.16.0,>=0.15.0->meltano) (1.6)
Requirement already satisfied: itsdangerous<2.0,>=0.24 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from flask<2,>=1->meltano) (1.1.0)
Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from flask<2,>=1->meltano) (2.11.3)
Requirement already satisfied: pytz in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from flask-restful<0.4.0,>=0.3.7->meltano) (2021.1)
Requirement already satisfied: aniso8601>=0.82 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from flask-restful<0.4.0,>=0.3.7->meltano) (9.0.1)
Requirement already satisfied: pathlib2>=2.3 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from flatten-dict<0.2.0,>=0.1.0->meltano) (2.3.6)
Requirement already satisfied: setuptools>=18.5 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from ipython<8.0.0,>=7.5.0->meltano) (56.0.0)
Requirement already satisfied: appnope in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from ipython<8.0.0,>=7.5.0->meltano) (0.1.2)
Requirement already satisfied: pexpect>4.3 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from ipython<8.0.0,>=7.5.0->meltano) (4.8.0)
Requirement already satisfied: backcall in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from ipython<8.0.0,>=7.5.0->meltano) (0.2.0)
Requirement already satisfied: matplotlib-inline in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from ipython<8.0.0,>=7.5.0->meltano) (0.1.2)
Requirement already satisfied: pygments in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from ipython<8.0.0,>=7.5.0->meltano) (2.9.0)
Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from ipython<8.0.0,>=7.5.0->meltano) (3.0.19)
Requirement already satisfied: decorator in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from ipython<8.0.0,>=7.5.0->meltano) (5.0.9)
Requirement already satisfied: pickleshare in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from ipython<8.0.0,>=7.5.0->meltano) (0.7.5)
Requirement already satisfied: traitlets>=4.2 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from ipython<8.0.0,>=7.5.0->meltano) (5.0.5)
Requirement already satisfied: jedi>=0.16 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from ipython<8.0.0,>=7.5.0->meltano) (0.18.0)
Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from jedi>=0.16->ipython<8.0.0,>=7.5.0->meltano) (0.8.2)
Requirement already satisfied: MarkupSafe>=0.23 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from Jinja2<3.0,>=2.10.1->flask<2,>=1->meltano) (2.0.1)
Collecting pyrsistent>=0.14.0
  Downloading pyrsistent-0.18.0-cp39-cp39-macosx_10_9_x86_64.whl (68 kB)
     |████████████████████████████████| 68 kB 2.3 MB/s 
Requirement already satisfied: Flask-Mail>=0.7.3 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano-flask-security<0.2.0,>=0.1.0->meltano) (0.9.1)
Requirement already satisfied: Flask-Principal>=0.3.3 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano-flask-security<0.2.0,>=0.1.0->meltano) (0.4.0)
Requirement already satisfied: Flask-BabelEx>=0.9.3 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano-flask-security<0.2.0,>=0.1.0->meltano) (0.9.4)
Requirement already satisfied: passlib>=1.7 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano-flask-security<0.2.0,>=0.1.0->meltano) (1.7.4)
Requirement already satisfied: Flask-Login>=0.3.0 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano-flask-security<0.2.0,>=0.1.0->meltano) (0.5.0)
Requirement already satisfied: Flask-WTF>=0.13.1 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from meltano-flask-security<0.2.0,>=0.1.0->meltano) (0.15.1)
Requirement already satisfied: speaklater>=1.2 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from Flask-BabelEx>=0.9.3->meltano-flask-security<0.2.0,>=0.1.0->meltano) (1.3)
Requirement already satisfied: Babel>=1.0 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from Flask-BabelEx>=0.9.3->meltano-flask-security<0.2.0,>=0.1.0->meltano) (2.9.1)
Requirement already satisfied: blinker in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from Flask-Mail>=0.7.3->meltano-flask-security<0.2.0,>=0.1.0->meltano) (1.4)
Requirement already satisfied: WTForms in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from Flask-WTF>=0.13.1->meltano-flask-security<0.2.0,>=0.1.0->meltano) (2.3.3)
Requirement already satisfied: ptyprocess>=0.5 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from pexpect>4.3->ipython<8.0.0,>=7.5.0->meltano) (0.7.0)
Requirement already satisfied: wcwidth in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8.0.0,>=7.5.0->meltano) (0.2.5)
Requirement already satisfied: pyparsing>=2.0.3 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from pyhocon<0.4.0,>=0.3.51->meltano) (2.4.7)
Requirement already satisfied: aenum==2.1.2 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from pypika<0.26.0,>=0.25.1->meltano) (2.1.2)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from requests<3.0.0,>=2.23.0->meltano) (1.26.6)
Requirement already satisfied: certifi>=2017.4.17 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from requests<3.0.0,>=2.23.0->meltano) (2021.5.30)
Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from requests<3.0.0,>=2.23.0->meltano) (2.0.4)
Requirement already satisfied: snowflake-connector-python<3.0.0 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from snowflake-sqlalchemy<2.0.0,>=1.2.3->meltano) (2.5.1)
Requirement already satisfied: oscrypto<2.0.0 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from snowflake-connector-python<3.0.0->snowflake-sqlalchemy<2.0.0,>=1.2.3->meltano) (1.2.1)
Requirement already satisfied: azure-storage-blob<13.0.0,>=12.0.0 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from snowflake-connector-python<3.0.0->snowflake-sqlalchemy<2.0.0,>=1.2.3->meltano) (12.8.1)
Requirement already satisfied: pycryptodomex!=3.5.0,<4.0.0,>=3.2 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from snowflake-connector-python<3.0.0->snowflake-sqlalchemy<2.0.0,>=1.2.3->meltano) (3.10.1)
Requirement already satisfied: asn1crypto<2.0.0,>0.24.0 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from snowflake-connector-python<3.0.0->snowflake-sqlalchemy<2.0.0,>=1.2.3->meltano) (1.4.0)
Requirement already satisfied: azure-common<2.0.0 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from snowflake-connector-python<3.0.0->snowflake-sqlalchemy<2.0.0,>=1.2.3->meltano) (1.1.27)
Requirement already satisfied: pyjwt<3.0.0 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from snowflake-connector-python<3.0.0->snowflake-sqlalchemy<2.0.0,>=1.2.3->meltano) (2.1.0)
Requirement already satisfied: boto3<2.0.0,>=1.4.4 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from snowflake-connector-python<3.0.0->snowflake-sqlalchemy<2.0.0,>=1.2.3->meltano) (1.18.21)
Requirement already satisfied: pyOpenSSL<21.0.0,>=16.2.0 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from snowflake-connector-python<3.0.0->snowflake-sqlalchemy<2.0.0,>=1.2.3->meltano) (20.0.1)
Requirement already satisfied: msrest>=0.6.18 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from azure-storage-blob<13.0.0,>=12.0.0->snowflake-connector-python<3.0.0->snowflake-sqlalchemy<2.0.0,>=1.2.3->meltano) (0.6.21)
Requirement already satisfied: azure-core<2.0.0,>=1.10.0 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from azure-storage-blob<13.0.0,>=12.0.0->snowflake-connector-python<3.0.0->snowflake-sqlalchemy<2.0.0,>=1.2.3->meltano) (1.17.0)
Requirement already satisfied: botocore<1.22.0,>=1.21.21 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from boto3<2.0.0,>=1.4.4->snowflake-connector-python<3.0.0->snowflake-sqlalchemy<2.0.0,>=1.2.3->meltano) (1.21.21)
Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from boto3<2.0.0,>=1.4.4->snowflake-connector-python<3.0.0->snowflake-sqlalchemy<2.0.0,>=1.2.3->meltano) (0.10.0)
Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from boto3<2.0.0,>=1.4.4->snowflake-connector-python<3.0.0->snowflake-sqlalchemy<2.0.0,>=1.2.3->meltano) (0.5.0)
Requirement already satisfied: requests-oauthlib>=0.5.0 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from msrest>=0.6.18->azure-storage-blob<13.0.0,>=12.0.0->snowflake-connector-python<3.0.0->snowflake-sqlalchemy<2.0.0,>=1.2.3->meltano) (1.3.0)
Requirement already satisfied: isodate>=0.6.0 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from msrest>=0.6.18->azure-storage-blob<13.0.0,>=12.0.0->snowflake-connector-python<3.0.0->snowflake-sqlalchemy<2.0.0,>=1.2.3->meltano) (0.6.0)
Requirement already satisfied: oauthlib>=3.0.0 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.18->azure-storage-blob<13.0.0,>=12.0.0->snowflake-connector-python<3.0.0->snowflake-sqlalchemy<2.0.0,>=1.2.3->meltano) (3.1.1)
Requirement already satisfied: greenlet!=0.4.17 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from sqlalchemy<2.0.0,>=1.3.19->meltano) (1.1.1)
Requirement already satisfied: ipython-genutils in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from traitlets>=4.2->ipython<8.0.0,>=7.5.0->meltano) (0.2.0)
Requirement already satisfied: argh>=0.24.1 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from watchdog<0.10.0,>=0.9.0->meltano) (0.26.2)
Requirement already satisfied: pathtools>=0.1.1 in /Users/aj/.pyenv/versions/3.9.5/lib/python3.9/site-packages (from watchdog<0.10.0,>=0.9.0->meltano) (0.1.2)
Building wheels for collected packages: meltano
  Building wheel for meltano (PEP 517) ... done
  Created wheel for meltano: filename=meltano-1.85.0-py3-none-any.whl size=3804221 sha256=90f383b7f4f21fbcc1ea83a344e008526e1bebf452501bf856dc1c90b4d6087c
  Stored in directory: /Users/aj/Library/Caches/pip/wheels/93/4d/cb/60a6fec735d00bc166eb0897d2a749a4a4ee6c626d323492e7
Successfully built meltano
Installing collected packages: werkzeug, pyrsistent, jsonschema, meltano
  Attempting uninstall: werkzeug
    Found existing installation: Werkzeug 0.16.1
    Uninstalling Werkzeug-0.16.1:
      Successfully uninstalled Werkzeug-0.16.1
  Attempting uninstall: jsonschema
    Found existing installation: jsonschema 2.6.0
    Uninstalling jsonschema-2.6.0:
      Successfully uninstalled jsonschema-2.6.0
Successfully installed jsonschema-3.2.0 meltano-1.85.0 pyrsistent-0.18.0 werkzeug-1.0.1
The `meltano` package has been upgraded.

Reloading UI...
UI is not running

Updating files managed by plugins...
Updating file bundle 'dbt'...
Updated file bundle 'dbt'
Updating 'dbt' files in project...
Nothing to update

Applying migrations to system database...
[2021-10-26 19:30:41,733] [23420|MainThread|alembic.runtime.migration] [INFO] Context impl SQLiteImpl.
[2021-10-26 19:30:41,733] [23420|MainThread|alembic.runtime.migration] [INFO] Will assume non-transactional DDL.
System database up-to-date.

Recompiling models...

Meltano and your Meltano project have been upgraded!
aj@ajs-macbook-pro meltano % meltano --version
meltano, version 1.82.0
```

</details>

And:

```console
aj@ajs-macbook-pro meltano % which meltano       
/Users/aj/.local/bin/meltano
aj@ajs-macbook-pro meltano % cat /Users/aj/.local/bin/meltano
#!/Users/aj/.local/pipx/venvs/meltano/bin/python
# -*- coding: utf-8 -*-
import re
import sys
from meltano.cli import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())
```

## Addl. info (pipx debug)

This runs as expected and printed paths may be a clue:

```console
aj@ajs-macbook-pro meltano % pipx upgrade --verbose meltano
pipx >(setup:717): pipx version is 0.16.3
pipx >(setup:718): Default python interpreter is '/Users/aj/.pyenv/versions/3.8.10/bin/python3'
pipx >(run_pipx_command:168): Virtual Environment location is /Users/aj/.local/pipx/venvs/meltano
pipx >(needs_upgrade:69): Time since last upgrade of shared libs, in seconds: 25. Upgrade will be run by pipx if greater than 2592000.
pipx >(upgrade:91): Upgrading shared libraries in /Users/aj/.local/pipx/shared
upgrading shared libraries...
pipx >(run_subprocess:135): running /Users/aj/.local/pipx/shared/bin/python -m pip --disable-pip-version-check install --upgrade pip setuptools wheel
pipx >(_parsed_package_to_package_or_url:128): cleaned package spec: meltano
upgrading meltano...
pipx >(run_subprocess:135): running /Users/aj/.local/pipx/venvs/meltano/bin/python -m pip install --upgrade meltano
pipx >(run_subprocess:135): running <fetch_info_in_venv commands>
pipx >(get_venv_metadata_for_package:303): get_venv_metadata_for_package: 10964ms
pipx >(_parsed_package_to_package_or_url:128): cleaned package spec: meltano
pipx >(_symlink_package_apps:124): Same path /Users/aj/.local/bin/meltano and /Users/aj/.local/pipx/venvs/meltano/bin/meltano
upgraded package meltano from 1.82.0 to 1.85.0 (location: /Users/aj/.local/pipx/venvs/meltano)
aj@ajs-macbook-pro meltano % meltano --version
meltano, version 1.85.0
```",1
96111328,2021-10-25 20:32:35.418,Add CLI support for environments,"In !2355 we did not add support for managing environments with the command line. We should have them! Based on discussion in https://gitlab.com/meltano/meltano/-/merge_requests/2383#note_713553913

At a minimum:

<details><summary>Old version</summary>

* `meltano add <environment>`
* `meltano remove <environment>`
* `meltano config <environment> (list/set/unset/reset)`
</details>

```console
$ meltano environment --help
Usage: meltano environment [OPTIONS] COMMAND [ARGS]...

Options:
  --database-uri TEXT  System database URI
  --help               Show this message and exit.

Commands:
  add    Add a new environment
  list   Enumerate available environments
  remove Remove an environment definition
```",8
96107535,2021-10-25 19:06:40.230,Add Snowplow Tracking to Meltano,"https://gitlab.com/meltano/meltano/-/blob/master/src/meltano/core/tracking/ga_tracker.py seems to be the primary place where GA Events are defined. All instances of `GoogleAnalyticsTracker` would have to be updated as well.

The aim of this issue would be to have 100% parity for Snowplow events with what we had for GA.

2022-02-24 Update

After syncing with Snowcat Cloud and the team we're aiming to do the following:

* Get the end-to-end data flowing to both GA and Snowplow
* This means using the structured event format to achieve parity with what we have in GA
  * This is a known sub-optimal approach as we aim to overhaul all events with proper schemas and structure
* New events should only be added following improved schema workflow and control

## Implementation Outline

<details><summary>(larger scope)</summary>

- [ ] create a new `MeltanoTracking` class to track usage, with support for many `BaseTracker` instances
- [ ] create a new `BaseTracker` class to host generic tracking methods
- [ ] create new `SnowplowTracker` class, inheriting from `BaseTracker`, to forward to Snowplow
- [ ] adapt `GoogleAnalyticsTracker` to inherit from `BaseTracker`
- [ ] replace all uses of `GoogleAnalyticsTracker` with `MeltanoTracking`

</details>",8
96107301,2021-10-25 19:01:44.612,Stand-up full Snowplow Infrastructure,"Based on https://gitlab.com/meltano/meltano/-/issues/2972, we have decided to use and implement Snowplow for the open source event tracking. 

Given that the team has experience with AWS and Snowplow, deploying the infrastructure on our own AWS infra is ideal sicne it gives us full control of the data.

This issue tracks the work to get everything stood up on the AWS side. At the end of it, we should have a URL we can send events to which will be enriched and landed in S3. [This project](https://gitlab.com/meltano/saved-from-gitlab/gitlab-com-infrastructure/-/tree/master/environments/aws-snowplow) can serve as a detailed guide on how to get it working.

https://gitlab.com/gitlab-com/gitlab-com-infrastructure/-/tree/master/environments/aws-snowplow is also available!",12
96090843,2021-10-25 14:24:43.999,Add role and batch_wait_limit_seconds setting to default target-snowflake,"The Wise variant is missing the optional role setting https://github.com/transferwise/pipelinewise-target-snowflake#configuration-settings

batch_wait_limit_seconds
    Integer(Default: None)
    Maximum time to wait for batch to reach batch_size_rows.

@pnadolny13 we should just do a full review to make sure everything is captured.",1
96054288,2021-10-25 06:08:14.807,Decide on input variables for `terraform-meltano-aws` and similar IAC modules,"As a follow-on from the conversation in #2987 (internal), I thought it would be helpful to start discussing what configuration levels would be built into our Meltano IAC module for Terraform. To be sure that we have a specific-enough proposal without getting overwhelmed, I think we can describe this in AWS constructs _first_ and later translate these into the relevant constructs for GCP, Azure, etc.

## Functional data project configuration

These inputs drive core functionality of the Meltano project:

- `meltano_project_file` (str) - The path to a `meltano.yml` file to use in configuration of the environment. This would be [parsed by terraform](https://www.terraform.io/docs/language/functions/yamldecode.html) and may affect which resources Terraform creates, and how many. (Altering `meltano.yml` may require rerunning `terraform apply`.)
- `meltano_project_dir` (str) - The path to the Meltano project folder containing job definitions, dbt transforms, etc.
- `meltano_environment_name` (str) - The name of the environment to deploy, corresponding to a key in the `environments` entry in `meltano.yml`. This drives environment variable names and other configs as needed for the chosen environment. 
- `project_shortname` (str) - When AWS resources are created, they will be created with this text in the resource name.
- `aws_region` (required, str) - The region ID (e.g. `us-east-2`)

To keep the config DRY, most/all other configuration would ideally be migrated into _either_ the `environments` feature in `meltano.yml` _or_ the base plugin or utility config. Then `meltano.yml` (and possibly also `discovery.yml`) would be read as an input to the Terraform module's configuration.

## Optional, AWS infrastructure reuse and interop:

These inputs are all optional, and they allow the Terraform module to reuse existing AWS resources if desired:

- `use_existing_vpc_id` (optional, str) - The existing VPC ID to use. If null, a new VPC will be created.
- `use_existing_private_subnets` (optional, list(str)) - The existing private subnets to use. If null, a new VPC will be created.
- `use_existing_public_subnets` (optional, list(str)) - The existing private subnets to use. If null, a new VPC will be created.
- `use_existing_s3_logging_path` (optional, str) - The S3 path to use for logging purposes, when S3 log location is needed. If not provided, a new S3 bucket will be created.
- `use_existing_s3_artifacts_path` (optional, str) - The S3 path to use for storing temporary and long-term internal project artifacts, such as lambda function definitions, zip artifacts, and other internal project file storage. If not provided, a new S3 bucket will be created.

There may be more here but generally the config would follow the same pattern. In the first iterations, these may be all automatic - meaning we may not at first allow reusing existing resources. However, as the module matures, it should accept an increasing amount of interop and reuse with other environments, either for reasons of cost savings (RDS, K8 cluster, etc.) or corporate compliance policies (subnets, logging bucket, etc.). Most of these resources are free or have negligible cost, so probably would not need to prioritize reuse except for compliance reasons and for interop with existing application infrastructure.",8
95910043,2021-10-21 17:14:48.628,Always print `--log-level=debug` guidance after failure,"In slack we are often providing the guidance that people can rerun their pipelines with `--log-level=debug` - generally needed when a tap or target cannot properly configure itself or fails auth.

What do folks think about _always_ printing this after failure?

Something like:

> .... has failed. For more detailed log messages check '...' log directory or rerun with the --log-level=debug CLI flag.

Related slack thread: https://meltano.slack.com/archives/C013EKWA2Q1/p1634836288002100?thread_ts=1634832556.001800&cid=C013EKWA2Q1",4
95821081,2021-10-20 19:53:31.034,Uninstall documentation,"### Problem to solve
Need to make it clearer that the way to force a reinstall of a tap is to delete the directory in the `.meltano` directory.

Currently there is documentation on how to [install](https://meltano.com/docs/plugin-management.html#custom-plugins) a custom plugin, but especially when using custom taps, it isn't immediately clear how to remove them. 


### Target audience
Developers building custom taps and using meltano as part of the development process.


### Further details
This currently is relatively undocumented, only in [Slack](https://meltano.slack.com/archives/CFG3C3C66/p1594084482487700?thread_ts=1593999699.428400&cid=CFG3C3C66) is it made clear that removing files in `.meltano/extractors` will trigger a complete reinstall, ie once deleted, `meltano install <type> <name> --reinstall` will reinstall it.



### Proposal
Provide a proper reinstall option, ie `meltano install <type> <name> --reinstall`, with the same impact of a `rm -rf .meltano/extractors/tap-<name>

Currently in the [docs](https://meltano.com/docs/plugin-management.html#installing-your-project-s-plugins) it indicates that reinstall is possible, but this does not do a complete reinstall.

> To (re)install a specific plugin in your project, use meltano install <type> <name>, e.g. meltano install extractor tap-gitlab.


### What does success look like, and how can we measure that?
Able to remove a tap without deleting files in `.meltano`


### Links / references

* [Install docs](https://meltano.com/docs/plugin-management.html#installing-your-project-s-plugins)
* [Slack conversation](https://meltano.slack.com/archives/CFG3C3C66/p1594084482487700?thread_ts=1593999699.428400&cid=CFG3C3C66)

_Please note that this was taken from GitLab, to be changed accordingly_",1
95641264,2021-10-18 21:12:26.578,Python 3.6 EOL,"The official EOL date for Python 3.6 is 2021-12-23: https://endoflife.date/python

Users can check their Python version by running `python --version` at the command line.

Beginning in Jan 2022, new releases of Meltano and the SDK will no longer offer support for Python 3.6.",2
95439962,2021-10-14 15:57:42.907,meltano | Transformation failed (2): Env var required but not provided: 'MELTANO_LOAD_SCHEMA',"According to @DouweM, this can occur if a custom loader that doesn’t define a schema setting, so the env var doesn’t get populated and dbt doesn’t know what schema to read from.

I think Meltano is probably functioning as designed this feels like a bug to me. In a world where we have 10s of taps loading to the same environment (#2869), dbt should probably be mapping to schema or tap names statically (in yaml config) rather than dynamically (in env vars).

We should also strive to improve the user experience for users creating their own transformation sets, which will increasingly become the norm as we are moving away from the more fragile `transform` packages that expect specific database dialect (generally postgres) and a specific variant of the given tap.

Labeling as a bug because of the user impact, although in fairness, I do think this is functioning as originally designed.",8
95246721,2021-10-11 22:13:58.040,Problems with environment variables taking precedence over declared ones,"If a plugin configuration option is set to a specific environment variable, but that config option as a default environment variable it can be configured with, and _both_ are defined, Meltano uses the default one rather than the explicitly declared one.

Example:

```bash
meltano init demo
meltano add extractor tap-gitlab
meltano config tap-gitlab set private_token '$MY_REAL_GITLAB_API_TOKEN'
export GITLAB_API_TOKEN=wrong-token
export MY_REAL_GITLAB_API_TOKEN=right-token
meltano config tap-gitlab
```

```
{
  ""api_url"": ""https://gitlab.com"",
  ""private_token"": ""wrong-token"",
  ""groups"": """",
  ""projects"": """",
  ""ultimate_license"": false,
  ""fetch_merge_request_commits"": false,
  ""fetch_pipelines_extended"": false
}
```

To me, this is unexpected behavior. I would expect `GITLAB_API_TOKEN` to be used if I hadn't specified anything for `api_url`, but since I explicitly told Meltano to use `MY_REAL_GITLAB_API_TOKEN` I'd expect that to take precedence.

## Proposed Solution (AJ)

I think we should deprecate `env: CUSTOM_ENV_NAME` and `env_aliases: [...]` in favor of the more explicit `<plugin_name>_<setting_name>`. I've created #3120 to track this.",4
95061309,2021-10-07 18:37:09.583,Hide `Explore` and `Dashboards` in UI by default,"To help new users, I think we should remove `Explore` and `Dashboards` from the new user experience here:

![image](/uploads/38ae54b0e73c218d21120cdceb4a6474/image.png)

Rather than have this on by default, I would like users to have to provide an environment variable to enable them, with the caveat that this is legacy functionality and the corresponding activities are not being actively supported.

This is a half-step between removing the functionality entirely and would focus on reducing frustration for new users who naturally and understandably gravitate to the UI experience first.

Presumably, users could optionally type in the URLs directly if needed. Goal here is to reduce discovery during new-user experience, not so much prevent someone who already is using this functionality from leveraging it.",1
94566331,2021-09-29 18:44:24.961,Revert per build network bridge work around,"As a workaround for https://gitlab.com/meltano/meltano/-/issues/2951 where we were seeing network timeouts on package installs [we deployed a workaround](https://gitlab.com/meltano/meltano/-/commit/4f2ce5b060a45aff37a580710f3b48c1c7215f63) recommended upstream by Gitlab. However, Gitlab has since worked with their provider to resolve the underlying issue.

The workaround we deployed is a subtle change in behavior in how networking in our CI containers is built/configured so we should revert this change until we want to (intentionally) make use of this feature.",1
94553384,2021-09-29 14:48:49.656,API endpoint for testing an extractor instantiates an ABC,"<!---
Please read this!

Before opening a new issue, make sure to search for keywords in the issues
filtered by the ""regression"" or ""bug"" label and verify the issue you're about to submit isn't a duplicate.

If you are submitting an issue with a tap, please include:

- account details
- target details
- entities selected  with meltano select (if you have selected any entities), as the bug may be related to a specific entity
- the full elt command you are running
- full output of the meltano elt command. Logs can get pretty long, so you can add the full log as a snippet in the Meltano project and add a link in the issue.
--->

### What is the current *bug* behavior?
_What is happening now?_

From a slack converstation (https://meltano.slack.com/archives/CMN8HELB0/p1632924967171100), it seems like the test endpoint is instantiating the abstract base class `PluginTestService` instead of `ExtractorTestService`.

### What is the expected *correct* behavior?
_What should be happening?_

The endpoint should test an extractor correctly.

### Steps to reproduce
_How one can reproduce the issue?_

Try hitting the `POST /<plugin_ref:plugin_ref>/configuration/test` endpoint.

### Relevant logs and/or screenshots
_Please use code blocks (\`\`\`) to format console output_

```
[2021-09-29 16:13:56,137] [74486|MainThread|meltano.api.app] [ERROR] Exception on /api/v1/orchestrations/extractors/tap-hubspot/configuration/test [POST]
[...]
TypeError: Can't instantiate abstract class PluginTestService with abstract method validate
[2021-09-29 16:13:56,140] [74486|MainThread|meltano.api] [INFO] Error: 500 Internal Server Error: The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.
```

### Possible fixes
_If you can, link to the line of code that might be responsible for the problem or suggest a fix_

Replace `PluginTestService` with `ExtractorTestService` in the function body.

### Further regression test
_Ensure we automatically catch similar issues in the future_

- [ ] Write additional adequate test cases and submit test results
- [ ] Test results should be reviewed by a person from the team",1
94045037,2021-09-21 14:09:09.498,Download Log Button in UI doesnt download most recent one,"Hi everyone,

I noticed the download button on the meltano UI seem not to work as described. Instead of downloading the most recent log after a Run of a  pipeline it somehow gets an older one. I couldn't narrow it down to why exactly that log, but with ongoing runs it keeps jumping back to that specific log all the time.

Best
Nic",4
93745232,2021-09-15 20:55:12.744,Document GitLab's container registry as an option for pulling images,Based on a comment from https://gitlab.com/meltano/meltano/-/issues/2934#note_678349501,4
93616199,2021-09-14 07:23:10.606,Make Docker images available in AWS public ECR gallery,"Using Docker has become pretty annoying if you use a code pipeline. You invariably will get ""toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading:"".

Since we build stuff with Meltano, we get this quite a bit. Is it possible for Meltano to upload their images to the AWS ECR public registry please as well? See here: https://gallery.ecr.aws/",4
93649221,2021-09-13 20:12:51.635,Create a GitHub Action for Meltano ELT,"## Summary
[//]: # (Concisely summarize the feature you are proposing.)

GitHub Actions are a CI/CD offering with capabilities similar to those of GitLab CI, except that they can be published to the [a marketplace](https://docs.github.com/en/actions/creating-actions/publishing-actions-in-github-marketplace) where they get more visibility and users can add them to their projects with one click.

This action would be useful for developers that are first trying out Meltano, or don't need a full Kubernetes/cloud deployment with an orchestrator, metrics UIs, etc.

## Proposed benefits
[//]: # (Concisely summarize the benefits this feature would bring to yourself and other users.)

Give users that want to try out Meltano for ELT a way that is quick and easy.

## Proposal details
[//]: # (In as much detail as you are able, describe the feature you'd like to build or would like to see built.)

- A [composite action](https://docs.github.com/en/actions/creating-actions/creating-a-composite-action) is probably easier but we can consider a [docker action](https://docs.github.com/en/actions/creating-actions/creating-a-docker-container-action) too.
  Either way, the action would install the latest or some configurable Meltano version for the user and any other dependencies, and also run `meltano install`.
- Inputs to the job could be: `tap`, `target`, `job_id`, `transform` and `log_level` to compose a command like

  ```shell
  meltano --log-level={log_level} elt {tap} {target} --job_id={job_id} --transform={transform}
  ```

## Best reasons not to build
[//]: # (Will this negatively affect any existing functionality? Do you anticipate any breaking changes versus what may already be working today? Make the counter-argument to your proposal here.)

This only solves ELT and very basic orchestration needs. No fancy UI for jobs and retries are manual.",1
93464825,2021-09-10 14:09:32.391,Create Local Deployment File Bundle,"Having produced a proof of concept local deployment (#2910) we would now like to publish this as a [File Bundle](https://meltano.com/docs/plugins.html#file-bundles) for installation via `meltano add`. This includes:

- Publishing the Helm Charts for Meltano and Airflow
  - Helm expects charts to be hosted under plain http(s) urls. I intend to use GitLab Pages to fulfil this requirement.
- Publishing the Terraform Modules for Kind, Postgres and Docker
  - Terraform supports installing modules [directly from a git repository](https://www.terraform.io/docs/language/modules/sources.html#generic-git-repository)
- Creating a File Bundle that includes an `infrastructure/local` dir, `main.tf` file and other artefacts needed to launch the local deployment.",12
93276548,2021-09-07 16:43:33.852,"(Meta) Add top-level ""job"" with ""tasks"" in Meltano.yml, refactored from just Schedules to Jobs+Schedules","#### Update 2022-05-26

* Job support added in https://gitlab.com/meltano/meltano/-/merge_requests/2629 - no related issue as MR was scoped down.
* Support for schedules tracked in https://gitlab.com/meltano/meltano/-/issues/3479

----


Currently a ""Schedule"" in `meltano.yml` is called a ""Pipeline"" in the UI. (#1673) 

The proposed change is to add `jobs` construct, with one or more steps - similar to the proposal of a multi-step `meltano run` but stored in the yaml file and able to be scheduled.

This is essentially the translation of `meltano run` and `meltano invoke` statements into a DAG definition. Importantly, though, we need to be mindful of how much complexity we want to introduce.

Instead of what we have now that is restricted to just EL pairs:

- A schedule has a tap and target and transform spec (only/skip/etc.).

We'd have:

- A named `job` has one or more `tasks`
- A `schedule` points to a named `job` _or_ the `job` contains a `schedule` description
- Optionally, we can choose to support parallel groups of tasks, such as invoking multiple taps together.
  - Parallel groups might run serially if invoked locally via the CLI, but they'd generate a parallel structure when passed to an external orchestrator such as Airflow.

The assumption is that these would all run using `meltano run`.

For example:

```yaml
jobs:
- name: daily-eltp
  tasks:
  # EL pipelines can run in parallel:
  - - tap-shopify target-snowflake
    - tap-github target-snowflake
  # These dbt build steps run in sequence:
  - dbt:run --full-refresh
  - dbt:test
  # Last mile steps can run in parallel:
  - - superset:build             # BI build
    - tap-snowflake target-yaml  # reverse ETL can run in parallel with BI build
- name: hourly-refresh
  tasks:
  - tap-slack target-snowflake
  - dbt:run

# schedules are separate and can reference jobs
schedules:
# ...
```

## Defining steps with rich metadata (out of scope for iteration 1)

We should plan for future support of rich task definitions (could have `name`, `type`, 'needs', etc.) instead of simple strings. 

The shorthand of simple string representation is easy to understand, read, and maintain. However, this cannot handle advanced dag dependency representations. Supporting both smartly will likely be needed in future for advanced use cases:

```yaml
jobs:
- name: daily-eltp
  tasks:
  # A complex task object could be inlined with string-based tasks
  - type: invoke
    command: dbt:run
    config: ... # perhaps overrides here or perhaps some other future syntax options
  - dbt:test
  - - superset:build             # BI build
    - tap-snowflake target-yaml  # reverse ETL can run in parallel with BI build
```

For each item in the tasks array, if the type is `string`, pass along basically as a `meltano run` arg. If the type is object (dict in python), then parse as a complex task.

## Jobs' relationship to schedules

Schedules likely should be environment specific, but perhaps this should be held until a future iteration.

```yaml
schedules:
- name: daily-eltp-schedule
  job: daily-eltp
  interval: daily
  environments:
  - prod
- name: hourly-refresh-schedule
  job: hourly-refresh
  interval: hourly
  environments:
  - prod
  - dev
#...
```


## Referencing a job from `meltano run`

We would expand `meltano run` to accept job names as well as plugin names.

So assuming 2 jobs are defined, `dbt-daily-refresh` and `daily-el-sync`, these invocations could be valid:

_Possibly out of scope for first iteration_

### Example A: EL steps followed by a named job.

Runs one EL task and then all the tasks in the 'dbt-daily-refresh' job.

```bash
meltano run tap-slack target-snowflake dbt-daily-refresh
```

### Example B: A job followed by a list of commands.

Runs all the EL tasks in the `daily-el-sync` job and then run dbt.

```bash
meltano run daily-el-sync dbt:run dbt:test
```

### Example C: Two sequential jobs.

Runs all the tasks in the `daily-el-sync` job and then all the tasks in the 'dbt-daily-refresh' job.

```bash
meltano run daily-el-sync dbt:run dbt:test
```

## Regarding name collisions

In order for jobs to be referenced in `meltano run`, there cannot be collisions between plugin names and job names.

## CLI Interface

#### jobs

`meltano job <job_name>` add a job

`meltano job <job_name> add schedule <interval>` Schedule the job

`meltano job <job_name> add schedule '[<interval>, ...]' ` Add more schedules

`meltano job <job_name> tasks <run commands>` # should this be set?

`meltano job <job_name> tasks '[<run command>, ...]' `

#### schedules

`meltano schedule <schedule_name> set <job_name> <interval> <environment>` 

`meltano schedule <schedule_name> set <job_name>`

`meltano schedule <schedule_name> set <interval>`

`meltano schedule <schedule_name> set <environment>`

`meltano schedule list [--format=json]`  Does this work with jobs that are scheduled via the job definition?

`meltano schedule run <schedule_name>` Similar to current behavior this would pass any CLI args to `meltano run`

### meltano run syntax

""Run"" would accept a job name in place of a command or plugin name.

```
meltano run <job_name>
```

And job names and command/plugin names can be sequenced:

```
meltano run <job_name> dbt:run dbt:build
```

In this scenario, everything runs sequentially but the user doesn't have to type as much.

## Relation to `meltano invoke`

Jobs are not comatible with this syntax: `meltano invoke <job_name>`.

Instead users should use `meltano run <job_name>` or `meltano schedule run <schedule_name` syntax.",8
93119602,2021-09-04 14:41:35.581,meltano install fails in Ubuntu 20.04 LXD image unless you use venv,"### What is the current *bug* behavior?

`meltano install` works on my local env, running with a conda built python env.. but when i went to LXD based containers and wanted to use plain ol' python.. it crashed



### What is the expected *correct* behavior?

meltano install should work without needing venv or any other virtual environment.. I am in an LXD container and I understand the risks.. the container is empty except for meltano and my custom tap


### Steps to reproduce

I do a juju charm of meltano in order to setup the env.. without LXD I am not sure you could do it but.. essentially clone a template meltano project into an LXD container... and try to install it

```
version: 1
send_anonymous_usage_stats: false
project_id: SOMEID
plugins:
  extractors:
  - name: tap-csv
    variant: meltano
    pip_url: git+https://gitlab.com/meltano/tap-csv.git
    config:
      files:
      - entity: tickers
        file: extract/example_data.csv
        keys:
        - ticker
  - name: tap-name
    namespace: tap_namespace
    executable: /PATH_TO_CUSTOM_TAP.sh
  loaders:
  - name: target-postgres
    variant: datamill-co
    pip_url: singer-target-postgres
    config:
      postgres_host: SOMEIP
      postgres_username: SOMEUSERNAME
      postgres_database: SOMEDB
      postgres_schema: public
      disable_collection: true
  orchestrators:
  - name: airflow
    pip_url: apache-airflow==1.10.14 --constraint https://raw.githubusercontent.com/apache/airflow/constraints-1.10.14/constraints-3.6.txt
  files:
  - name: airflow
    pip_url: git+https://gitlab.com/meltano/files-airflow.git
schedules:
- name: csv-to-postgres
  extractor: tap-csv
  loader: target-postgres
  transform: skip
  interval: '@daily'
  start_date: 2021-08-07 13:52:51.715954
- name: csv-to-postgres
  extractor: tap-csv
  loader: target-postgres
  transform: skip
  interval: '@hourly'
  start_date: 2021-08-07 13:52:51.715954
- name: csv-to-postgres
  extractor: tap-csv
  loader: target-postgres
  transform: skip
  interval: '@once'
  start_date: 2021-08-07 13:52:51.715954

```


### Relevant logs and/or screenshots
_Please use code blocks (\`\`\`) to format console output_

```
meltano --log-level=debug install
[2021-09-04 13:38:02,314] [1172|MainThread|root] [DEBUG] Creating engine <meltano.core.project.Project object at 0x7f20bb37c760>@sqlite:////home/ubuntu/meltano_proj_repo/.meltano/meltano.db
[2021-09-04 13:38:02,359] [1172|MainThread|urllib3.connectionpool] [DEBUG] Starting new HTTPS connection (1): www.meltano.com:443
[2021-09-04 13:38:02,930] [1172|MainThread|urllib3.connectionpool] [DEBUG] https://www.meltano.com:443 ""GET /discovery.yml HTTP/1.1"" 200 115895
Installing 5 plugins...
[2021-09-04 13:38:03,586] [1172|MainThread|asyncio] [DEBUG] Using selector: EpollSelector
Installing file bundle 'airflow'...
[2021-09-04 13:38:03,587] [1172|MainThread|meltano.core.venv_service] [DEBUG] Removed old virtual environment for 'files/airflow'
[2021-09-04 13:38:03,587] [1172|MainThread|meltano.core.venv_service] [DEBUG] Failed to remove directory tree /home/ubuntu/meltano_proj_repo/.meltano/run/airflow
Traceback (most recent call last):
  File ""/usr/lib/python3.8/shutil.py"", line 707, in rmtree
    orig_st = os.lstat(path)
FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/meltano_proj_repo/.meltano/run/airflow'
[2021-09-04 13:38:03,587] [1172|MainThread|meltano.core.venv_service] [DEBUG] Creating virtual environment for 'files/airflow'
Installing orchestrator 'airflow'...
[2021-09-04 13:38:03,592] [1172|MainThread|meltano.core.venv_service] [DEBUG] Removed old virtual environment for 'orchestrators/airflow'
[2021-09-04 13:38:03,593] [1172|MainThread|meltano.core.venv_service] [DEBUG] Failed to remove directory tree /home/ubuntu/meltano_proj_repo/.meltano/run/airflow
Traceback (most recent call last):
  File ""/usr/lib/python3.8/shutil.py"", line 707, in rmtree
    orig_st = os.lstat(path)
FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/meltano_proj_repo/.meltano/run/airflow'
[2021-09-04 13:38:03,593] [1172|MainThread|meltano.core.venv_service] [DEBUG] Creating virtual environment for 'orchestrators/airflow'
Installing loader 'target-postgres'...
[2021-09-04 13:38:03,597] [1172|MainThread|meltano.core.venv_service] [DEBUG] Removed old virtual environment for 'loaders/target-postgres'
[2021-09-04 13:38:03,597] [1172|MainThread|meltano.core.venv_service] [DEBUG] Failed to remove directory tree /home/ubuntu/meltano_proj_repo/.meltano/run/target-postgres
Traceback (most recent call last):
  File ""/usr/lib/python3.8/shutil.py"", line 707, in rmtree
    orig_st = os.lstat(path)
FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/meltano_proj_repo/.meltano/run/target-postgres'
[2021-09-04 13:38:03,597] [1172|MainThread|meltano.core.venv_service] [DEBUG] Creating virtual environment for 'loaders/target-postgres'
Installing extractor 'tap-ibkr'...
Installing extractor 'tap-csv'...
[2021-09-04 13:38:03,601] [1172|MainThread|meltano.core.venv_service] [DEBUG] Removed old virtual environment for 'extractors/tap-csv'
[2021-09-04 13:38:03,601] [1172|MainThread|meltano.core.venv_service] [DEBUG] Failed to remove directory tree /home/ubuntu/meltano_proj_repo/.meltano/run/tap-csv
Traceback (most recent call last):
  File ""/usr/lib/python3.8/shutil.py"", line 707, in rmtree
    orig_st = os.lstat(path)
FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/meltano_proj_repo/.meltano/run/tap-csv'
[2021-09-04 13:38:03,601] [1172|MainThread|meltano.core.venv_service] [DEBUG] Creating virtual environment for 'extractors/tap-csv'
File bundle 'airflow' could not be installed: could not create the virtualenv for 'files/airflow'

Extractor 'tap-csv' could not be installed: could not create the virtualenv for 'extractors/tap-csv'

Orchestrator 'airflow' could not be installed: could not create the virtualenv for 'orchestrators/airflow'

Loader 'target-postgres' could not be installed: could not create the virtualenv for 'loaders/target-postgres'

Installed 0/5 plugins
Skipped installing 1/5 plugins
[2021-09-04 13:38:03,679] [1172|MainThread|meltano.cli.utils] [DEBUG] Failed to install plugin(s)
Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/dist-packages/meltano/cli/__init__.py"", line 44, in main
    cli(obj={""project"": None})
  File ""/usr/lib/python3/dist-packages/click/core.py"", line 764, in __call__
    return self.main(*args, **kwargs)
  File ""/usr/lib/python3/dist-packages/click/core.py"", line 717, in main
    rv = self.invoke(ctx)
  File ""/usr/lib/python3/dist-packages/click/core.py"", line 1137, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/usr/lib/python3/dist-packages/click/core.py"", line 956, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/usr/lib/python3/dist-packages/click/core.py"", line 555, in invoke
    return callback(*args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/meltano/cli/params.py"", line 23, in decorate
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/meltano/cli/params.py"", line 56, in decorate
    func(project, *args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/meltano/cli/install.py"", line 59, in install
    raise CliError(""Failed to install plugin(s)"")
meltano.cli.utils.CliError: Failed to install plugin(s)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/dist-packages/meltano/cli/__init__.py"", line 52, in main
    raise CliError(str(err)) from err
meltano.cli.utils.CliError: Failed to install plugin(s)
Failed to install plugin(s)
```

### Possible fixes
_If you can, link to the line of code that might be responsible for the problem or suggest a fix_

once I started using venv within the container.. it all went smoothly and I have meltano up and running inside the Ubuntu LXD container now.. but I shouldn't have to use venv IMO.. 

### Further regression test
_Ensure we automatically catch similar issues in the future_

- [ ] Write additional adequate test cases and submit test results
- [ ] Test results should be reviewed by a person from the team",4
92938667,2021-09-01 14:11:00.684,Support Deployment to AWS Lambda (Serverless),"AWS Lambda provides a fully-managed (serverless) environment that includes both scheduling and compute (including docker). By supporting lambda as a deployment option and orchestrator for Meltano we can provide an easy onramp into the Meltano ecosystem for customers already on AWS but otherwise unwilling to host and manage a self-deployed orchestrator like Airflow.

Integration would include:

- A lambda-compatible docker container for Meltano
- A way to manage state utilising only serverless services (DynamoDB, Aurora Serverless, S3 etc.)
- A way to gracefully handle the 15min max execution time of Lambda.

There are several possible approaches to the final requirement:

- Include a new `--max-execution-time` flag that terminates the tap gracefully (emitting state) even when more upstream records are available. This allows meltano to sync data using successive Lambda invocations without being 'reset' to a previous state due to timeouts, but carries the risk that streams fall behind and never catch up with the upstream source.
- Parallelise pipelines using 'one stream per function' to minimise the risk of several streams in a single pipeline approaching the limit.",40
92846507,2021-08-31 08:07:24.001,Support Deployment Plugins,"For Meltano to truly be a 'one-stop-shop' for your data platform, it needs to be able to deploy the individual tools and services that make up that platform. To achieve this Meltano should support deployment plugins, being existing tools for deploying and managing running infrastructure (such as Helm, Terraform, Ansible, Salt, Cloudformation). For those just getting started, Meltano should aim to provide opinionated base config for common scenarios (e.g. containers on Kubernetes), and for power users of these deployment tools the config and internals should still be accessible and easily customisable via existing Meltano config routes.

As a rough guide, the CLI could look something like:

```bash
# First a user would add a deployment. This would install all the necessary tooling to actually deploy to this platform during `meltano install`. Meltano will use all of the context of installed plugins, orchestrators and config to create appropriate deployment artefacts.
meltano add deployment --platform kubernetes --environment prod

# Users can optionally configure their deployment
meltano config deployment --environment prod set namespace ""team-data""

# Finally a new deployment can be executed 🚀
meltano deploy --environment prod

# Individual tools can also be invoked and used directly
meltano invoke helm list --all-namespaces --all
```",40
92696255,2021-08-27 12:26:32.387,Refresh catalog on every invoke (fresh_catalog: true)),"I want to have Meltano build a new catalog every run when running `meltano invoke tap-oracle`

A key at the SingerPlugin level probably makes sense, maybe call it `fresh_catalog` default to false? 

More specific than #2850 as #2627 didn't solve what I'm after. What I really want is a way to manage and watch my catalog change over time (#2677 / #2805 ), but this issue will be an incremental improvement over where I""m at today. 

Today I delete the catalog and cache key from `.meltano/run/tap-name/*`",4
92230218,2021-08-18 22:02:56.170,SIGTERM handling not compatible with Windows,"As described in #2743, the SIGTERM handling is not currently supported on Windows.

Here is a possible explanation with workaround: https://stackoverflow.com/questions/26061814/signal-handling-in-windows",4
92148274,2021-08-17 21:50:20.183,Refactor to convert plugins to leverage async patterns in hooks,"As a follow up to https://gitlab.com/meltano/meltano/-/issues/2868 (refactor SingerTarget to own bookmark writer) and a next step to https://gitlab.com/meltano/meltano/-/issues/2725 (Refactor `meltano elt`, `SingerRunner` and `DbtRunner` to be composable). We should refactor the plugins various before/after hooks and supporting methods to be async.",8
91977173,2021-08-14 17:18:19.568,meltano invoke target-csv - 'NoneType' object has no attribute 'full_refresh',"Steps to replicate problem
1. Install latest version of meltano
2. `meltano add target-csv`
3. `meltano invoke target-csv`


```:17
(.venv) visch@visch-ubuntu:~/git/meltano/testdir$ meltano --log-level=debug invoke target-csv
[2021-08-14 13:16:28,669] [1885|MainThread|root] [DEBUG] Creating engine <meltano.core.project.Project object at 0x7f7e7383ee50>@sqlite:////home/visch/git/meltano/testdir/.meltano/meltano.db
[2021-08-14 13:16:28,701] [1885|MainThread|urllib3.connectionpool] [DEBUG] Starting new HTTPS connection (1): www.meltano.com:443
[2021-08-14 13:16:28,922] [1885|MainThread|urllib3.connectionpool] [DEBUG] https://www.meltano.com:443 ""GET /discovery.yml?project_id=e713709f-f225-4fff-8f19-03f3a5b9e910 HTTP/1.1"" 200 115895
[2021-08-14 13:16:29,356] [1885|MainThread|meltano.core.utils] [DEBUG] Variable '$MELTANO_LOAD_SCHEMA' is missing from the environment.
[2021-08-14 13:16:29,358] [1885|MainThread|meltano.core.utils] [DEBUG] Variable '$MELTANO_LOAD_SCHEMA' is missing from the environment.
[2021-08-14 13:16:29,358] [1885|MainThread|root] [DEBUG] Created configuration at /home/visch/git/meltano/testdir/.meltano/run/target-csv/target.871a464e-14e9-4f81-8884-facea30b2ffd.config.json
[2021-08-14 13:16:29,359] [1885|MainThread|root] [DEBUG] Deleted configuration at /home/visch/git/meltano/testdir/.meltano/run/target-csv/target.871a464e-14e9-4f81-8884-facea30b2ffd.config.json
[2021-08-14 13:16:29,359] [1885|MainThread|meltano.cli.utils] [DEBUG] 'NoneType' object has no attribute 'full_refresh'
Traceback (most recent call last):
  File ""/home/visch/git/meltano/.venv/lib/python3.8/site-packages/meltano/cli/__init__.py"", line 44, in main
    cli(obj={""project"": None})
  File ""/home/visch/git/meltano/.venv/lib/python3.8/site-packages/click/core.py"", line 829, in __call__
    return self.main(*args, **kwargs)
  File ""/home/visch/git/meltano/.venv/lib/python3.8/site-packages/click/core.py"", line 782, in main
    rv = self.invoke(ctx)
  File ""/home/visch/git/meltano/.venv/lib/python3.8/site-packages/click/core.py"", line 1259, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/home/visch/git/meltano/.venv/lib/python3.8/site-packages/click/core.py"", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/home/visch/git/meltano/.venv/lib/python3.8/site-packages/click/core.py"", line 610, in invoke
    return callback(*args, **kwargs)
  File ""/home/visch/git/meltano/.venv/lib/python3.8/site-packages/meltano/cli/params.py"", line 23, in decorate
    return func(*args, **kwargs)
  File ""/home/visch/git/meltano/.venv/lib/python3.8/site-packages/meltano/cli/params.py"", line 56, in decorate
    func(project, *args, **kwargs)
  File ""/home/visch/git/meltano/.venv/lib/python3.8/site-packages/meltano/cli/invoke.py"", line 65, in invoke
    handle = invoker.invoke(*plugin_args, command=command_name)
  File ""/home/visch/git/meltano/.venv/lib/python3.8/site-packages/meltano/core/plugin_invoker.py"", line 272, in invoke
    with self._invoke(*args, **kwargs) as (popen_args, popen_options, popen_env):
  File ""/usr/lib/python3.8/contextlib.py"", line 113, in __enter__
    return next(self.gen)
  File ""/home/visch/git/meltano/.venv/lib/python3.8/site-packages/meltano/core/plugin_invoker.py"", line 257, in _invoke
    with self.plugin.trigger_hooks(""invoke"", self, args):
  File ""/usr/lib/python3.8/contextlib.py"", line 113, in __enter__
    return next(self.gen)
  File ""/home/visch/git/meltano/.venv/lib/python3.8/site-packages/meltano/core/behavior/hookable.py"", line 70, in trigger_hooks
    self.__class__.trigger(self, f""before_{hook_name}"", *args, **kwargs)
  File ""/home/visch/git/meltano/.venv/lib/python3.8/site-packages/meltano/core/behavior/hookable.py"", line 97, in trigger
    raise err
  File ""/home/visch/git/meltano/.venv/lib/python3.8/site-packages/meltano/core/behavior/hookable.py"", line 89, in trigger
    hook_func(target, *args, **kwargs)
  File ""/home/visch/git/meltano/.venv/lib/python3.8/site-packages/meltano/core/plugin/singer/target.py"", line 107, in setup_bookmark_writer_hook
    self.setup_bookmark_writer(plugin_invoker)
  File ""/home/visch/git/meltano/.venv/lib/python3.8/site-packages/meltano/core/plugin/singer/target.py"", line 120, in setup_bookmark_writer
    incomplete_state = elt_context.full_refresh and elt_context.select_filter
AttributeError: 'NoneType' object has no attribute 'full_refresh'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/home/visch/git/meltano/.venv/lib/python3.8/site-packages/meltano/cli/__init__.py"", line 52, in main
    raise CliError(str(err)) from err
meltano.cli.utils.CliError: 'NoneType' object has no attribute 'full_refresh'
'NoneType' object has no attribute 'full_refresh'
(.venv) visch@visch-ubuntu:~/git/meltano/testdir$ meltano --version
meltano, version 1.79.0```",2
91736373,2021-08-10 09:35:49.104,A file bundle is added twice via `--include-related` if its configuration does not contain the `update` field,"### What is the current *bug* behavior?

When adding a plugin with `--include-related`, Meltano adds a related file bundle plugin twice if it does not include the `update` field in its schema. The file bundles installs correctly if `update` is given with any key-value pair.


### What is the expected *correct* behavior?

The file bundle should be added only once with `--include-related`, regardless of whether or not the `update` field is provided.

### Steps to reproduce

1. Define a custom file bundle plugin (e.g. `files-google-analytics`) sharing a namespace of some other plugin (e.g. `tap-google-analytics` with namespace `tap_google_analytics`) **WITHOUT** any `update` field

```yaml
files:
  name: files-google-analytics
  namespace: tap_google_analytics
  repo: https://github.com/me/files-google-analytics
  pip_url: https://github.com/me/files-google-analytics
```

1. Add the file bundle via `--include-related` (e.g. `meltano add extractor tap-google-analytics --include related`)
1. Observe file bundle (`files-google-analytics`) add twice, with two occurences of `Adding related file bundle 'files-google-analytics' to your Meltano project...` in the output log.

### Relevant logs and/or screenshots

*Installs correctly*

```yaml
files:
  name: files-google-analytics
  namespace: tap_google_analytics
  update:
    path/to/some/file: true
  repo: https://github.com/me/files-google-analytics
  pip_url: https://github.com/me/files-google-analytics
```

```yaml
files:
  name: files-google-analytics
  namespace: tap_google_analytics
  update:
    path/to/some/file: false
  repo: https://github.com/me/files-google-analytics
  pip_url: https://github.com/me/files-google-analytics
```

*Installs incorrectly*

```yaml
files:
  name: files-google-analytics
  namespace: tap_google_analytics
  repo: https://github.com/me/files-google-analytics
  pip_url: https://github.com/me/files-google-analytics
```

```yaml
files:
  name: files-google-analytics
  namespace: tap_google_analytics
  update: {}
  repo: https://github.com/me/files-google-analytics
  pip_url: https://github.com/me/files-google-analytics
```

### Possible fixes

If `update` is not provided, Meltano [defaults it to an empty object (`{}`)](https://meltano.com/docs/plugins.html#update-extra), which I assume is the root cause of this bug. Specifying `update: {}` manually yields the same results.

### Further regression test

- [ ] Write additional adequate test cases and submit test results
- [ ] Test results should be reviewed by a person from the team",4
91175958,2021-07-30 22:57:22.692,Additional Metadata for PyPI,"## Summary

Add more project metadata fields for PyPI.

## Proposed benefits
[//]: # (Concisely summarize the benefits this feature would bring to yourself and other users.)

- More resource links in our PyPI page: https://pypi.org/project/meltano/
- Better SourceRank score: https://libraries.io/pypi/meltano/sourcerank

## Proposal details
[//]: # (In as much detail as you are able, describe the feature you'd like to build or would like to see built.)

[According to the Poetry docs](https://python-poetry.org/docs/pyproject/), we seem to be missing a few optional metadata fields:

- `maintainers`
- `homepage` (https://meltano.com)
- `documentation` (https://meltano.com/docs)
- `keywords`

We can also [add custom URLs](https://python-poetry.org/docs/pyproject/#urls):

```toml
[tool.poetry.urls]
""Issue Tracker"" = ""https://gitlab.com/meltano/meltano/-/issues""
""Twitter"" = ""https://twitter.com/meltanodata/""
""Changelog"" = ""https://gitlab.com/meltano/meltano/-/blob/master/CHANGELOG.md""
""Slack"" = ""https://join.slack.com/t/meltano/shared_invite/zt-obgpdeba-7yrqKhwyMBfdHDXsZY8G7Q""
""GitHub Mirror"" = ""https://github.com/meltano/meltano""
""Youtube"" = ""https://www.youtube.com/meltano""
```

## Best reasons not to build
[//]: # (Will this negatively affect any existing functionality? Do you anticipate any breaking changes versus what may already be working today? Make the counter-argument to your proposal here.)

None that I can think of.

A similar issue for the SDK is https://gitlab.com/meltano/sdk/-/issues/175.",1
90776313,2021-07-24 17:25:42.516,"Introduce top-level ""Environments"" within `meltano.yml` config","## Background

As Meltano becomes the end-to-end tool for more parts of dataops pipelines, there's a need to create a top-level construct and understanding of the project's SQL data repository (Snowflake, Spark, Athena, etc.) and that repository's corresponding ""environments"" such as ""prod"", ""beta"", ""cicd"", and ""dev"". Some of these environments may be dynamically keyed and fully ""disposable"", such as ""cicd-<build_num>"", and ""dev-<person_id>"".

## Today's Solutions

Today there are two or three places where a user needs to specify their data platform information - for instance ""target-snowflake"" config, ""dbt profiles"", and the analyze config, and possibly a ""reverse-etl"" connector such as ""tap-snowflake"". There's not yet a meltano-native construct for reusing this config.

## Proposed Solution

### Environment config as a top-level construct

We propose adding a top-level `environments:` config element. This could be a combination of ""smart"" and ""passthrough"" settings, where ""smart"" settings are known and expected by meltano, and ""passthrough"" settings are anything that the target (or dbt profile) may require.

### Environment Inheritance

To ease onboarding to support DRY principals - we should allow environments to inherit from each other. For instance, prod may be identical to dev except in the credentials and db name. Server name, and other connection-level parameters may be identical between these environments and we should allow the config to be reused - in the same way that the `inherits-from` construct supports this for plugins today.

### Strawman Sample

This is only intended for conversation - the exact spec will evolve based on input:

`meltano.yml`:

```yaml
project_id: ...
plugins: ...
schedules: ...
plugins:
  extractors:
  # this section unchanged - extractors referenced by name below in `sources`
  - name: tap-github
    namespace: tap_github
    executable: tap-github
    capabilities:
    - state
    - catalog
    - discover
    settings:
    - name: searches
      kind: array
    - name: auth_token
      kind: password
    - name: start_date
      value: '2010-01-01T00:00:00Z'
  # ...

environments:

- name: prod
  job_id_prefix: prod_             # will be appended with ${TAP_NAMESPACE} to make default job IDs

  # The `destination` is the singular landing location (""target"") for this environment
  # Other environments inherit this destination and can customize either with `vars` or by overriding completely
  default_loader: target-snowflake

  enable_transforms: true  # or false if the target does not support transforms (e.g. target-jsonl or target-s3-csv)

  # Here we list all the sources that `prod` should contain
  # These reference names declared under `plugins->extractors`
  # Other environments may inherit this list or override it
  default_extractors:
  - tap-github
  - tap-zendesk
  - tap-slack
  - tap-github

  config:
    plugins:
      loaders:
      - name: target-snowflake
        config:
          db_name: ""${DB_NAME}""                # References an environment-scoped env var
          load_schema: ""${TAP_NAMESPACE}_raw""  # Dynamic from the name of the currently-executing tap
          account: ""${SNOWFLAKE_ACCOUNT}""
          user: ""${SNOWFLAKE_USER}""
          role: prod_loader_role
          # ...

  env:
    # Env vars can be declared here when multiple plugins need to reference the same values
    DB_NAME: ""Waffleshop_DB""
    SNOWFLAKE_ACCOUNT: waffleshop.snowflakecomputing.net
    SNOWFLAKE_USER: prod_loader

- name: beta
  job_id_prefix: beta_                       # will be appended with ${TAP_NAMESPACE} to make default job IDs
  inherits_from: prod

  config:
    plugins:
      loaders:
      - name: target-snowflake
        config:
          role: beta_loader_role

  env:
    DB_NAME: ""Waffleshop_Beta""
    SNOWFLAKE_USER: beta_loader

# The following environments are ""disposable"", meaning they are intentionally keyed for temporarily use

- name: user_dev
  job_id_prefix: dev_${USER}_                # will be appended with ${TAP_NAMESPACE} to make default job IDs
  inherits_from: beta

  config:
    plugins:
      loaders:
      - name: target-snowflake
        config:
          role: beta_loader_role
          load_schema: ""${USER}dev_${TAP_NAMESPACE}_raw""        # Dynamic based on the USER environment variable

  env:
    DB_NAME: ""Development""
    SNOWFLAKE_USER: ""${USER}""

- name: cicd
  job_id_prefix: cicd_b${GITHUB_BUILD_NUM}_  # will be appended with ${TAP_NAMESPACE} to make default job IDs
  inherits_from: beta
  sources:
  config:
    plugins:
      extractors:
      # Overrides to make CICD run faster using a subset of data
      - name: tap-github
        config:
          searches:
          - {name: ""target-athena"", query: ""target-athena+fork:only""}
          
      loaders:
      - name: target-snowflake
        config:
          role: cicd_loader_role
          load_schema: ""raw_b${GITHUB_BUILD_NUM}_${TAP_NAMESPACE}"" # Dynamic based on build number from Github CI pipeline
```

In the simplest interations, all of the environment's config would be available to invoked plugins via a name prefix such as `$MELTANO_ENV_{VAR_NAME}`, such as `$MELTANO_ENV_SNOWFLAKE_USER`, `$MELTANO_ENV_DB_NAME`. Importantly, since we expect to be toggling the environment back and forth at runtime, the generated environment variables would not have the name of the environment contained within the variable name. However, we probably could still expose the environment name itself in something like `$MELTANO_ENV_NAME`, in case a plugin has native configuration which can branch based on that name.

### Possibility for native understanding of needs of different plugin types

The above is a simple ""passthrough"" approach where these keywords might not be natively understood by Meltano. However, we may choose to create subprofiles (now or in a future iteration) where meltano could send slightly different config to a transformer versus what it would send to a loader.

## Best reason not to build

None I can think of. It is one level of additional config to setup, so we may want to ensure it is optional. Especially for EL-only pipelines where there are no further steps, we would not want a user to _necessarily_ need to configure an environment if they are happy with simple tap->target configuration as they perform today.",12
90291165,2021-07-14 20:04:12.901,"""Yank"" older meltano versions which are missing Python constraints","We have a problem right now that many users are installing meltano on Python 3.9 - which is not supported - but the install is moving forward anyway on older versions (specifically `1.67`) which were published without an explicit max supported Python version. The net result is (1) the new user gets a stale version of Meltano, and (2) they don't get any messaging about 3.9 being unsupported.

## Long-term mitigation needed

Although we are soon going to launch 3.9 support (#2545) this issue will resurface again in October when [Python 3.10 is expected to release](https://www.google.com/search?q=python+3.10+release+date).

## ""Yanking"" old versions

According to the PyPi docs:

![image](/uploads/89cadb432804675f08528e8b7952d389/image.png)

- https://pypi.org/help/#yanked
- https://www.python.org/dev/peps/pep-0592/

## Risk of Resurgence Post-Fix

If we do the one-time yank of old versions, this issue should not resurface because all published versions have a max supported version.",1
90037653,2021-07-09 17:10:29.542,Start tracking open source licenses,"We should start tracking the licenses of the open source packages Meltano depends on, to make sure Meltano can sitll be licensed Apache 2.

We can use https://docs.gitlab.com/ee/user/compliance/license_compliance/. It uses https://github.com/pivotal/LicenseFinder, which requires `requirements.txt`, which we don't have since we're using Poetry, but we should be able to generate that on the fly using the `SETUP_CMD` env var described in https://docs.gitlab.com/ee/user/compliance/license_compliance/#installing-custom-dependencies.",12
89864528,2021-07-06 22:42:10.315,Implement interface for meltano `validator` plugins,"Related to #2454, we will need a spec for how validators should interface with meltano CLI.

As part of the spec definition, we're looking at how the following validators would work:

- Great Expectations (full)
- `dbt test` (native dbt feature)
- Stream maps
- SQLFluff - (potentially as this could be considered a code validator)

## Latest Proposal (2021-12-07):

Similar to the below from 2021-10-27, except that there is no ""top level `tests:`"" config. Instead, ""plugins"" `have` ""commands"" and ""commands"" `can be of type` ""test"". A command is of type `test` if its name starts with the word `test`.

## Older Proposal (2021-10-27):

* Discussion of the proposed spec from this comment thread: https://gitlab.com/meltano/meltano/-/issues/2838#note_682557413

1. Already today: **Any plugin type can have a `test` command, added inline with other commands using existing command syntax.**
1. Already today: Each test command is invocable directly and can perform its actions based on just environment vars and other context such as command line args.
1. New: **Any plugin with a `test` command can also have a `tests:` list entry in the top-level plugin config.** Each item in the list of `tests` can have the following properties:
    1. `name` - optional, if test is to be referenced by name
    2. `executable` - optional, overrides the executable of the default `test` command at the plugin level
    3. `args` - optional, overrides the args of the `test` command at the plugin level
    4. `env` (or `vars`) - a list of env variable name/value combinations to be passed to the executor
1. New: Tests could be invoked with any of the following syntaxes:
    1. `meltano run <plugin>:test` - runs any and all tests defined in the plugin declaration.
        - If the `test` command exists but no `tests:` declaration, only the one test command will be executed.
        - If the `tests:` entry also exists, all enumerated tests will be run in order.
        - If the `test` command does not exist, the command fails.
    2. `meltano test --all` (a new cli command) runs all defined tests in all plugins.
    3. `meltano test <plugin>` runs all defined tests for the named plugin.
    3. `meltano test <plugin1> <plugin2>` runs all defined tests for the named plugins.
    4. `meltano test <plugin>:<test-name>` runs a named test for the named plugin.
    4. `meltano test <plugin1>:<test-name1> <plugin2>:<test-name2>` runs multiple named tests for the named plugins.

Future Considerations:

Sequencing would need to be provided explicitly in the `v1` release, but `depends_on` behavior could be added in a v2, perhaps after `environments` (#2869) is released.


<details><summary>Previous proposals, click to expand</summary>
## Option A: Dedicated CLI command `meltano validate` (confirmed first iteration)

This would run validations as a dedicated top-level command. How the integration is performed would be governed by (a) custom code in meltano library and/or (b) configuration specs in the `validators:` entry in `discovery.yml`.

## Option B: Integration with `meltano elt` (won't do, deprecating in favor of `meltano run`)

Could be enabled/disabled similar to how transforms are treated, with a CLI flag.

## Option C: Integration with (WIP) `meltano run` (will do, but in future iteration)

Could be listed in a pipeline definition such as:

`meltano run tap-salesforce target-snowflake dbt:test ge:test`

Given an invocation such as the above, Meltano may be able to infer that the two validators can run in parallel, and may automatically parallelize them.
</details>",12
89795001,2021-07-05 20:37:21.864,Document where domains/DNS is managed,"We should document on https://meltano.com/handbook/engineering/ where our various domain names are registered (AWS, NameCheap, Gandi) and where DNS is managed (typically SiteGround).

Related to https://gitlab.com/gitlab-com/gl-infra/infrastructure/-/issues/12688

@aaronsteers I seem to recall you already created an issue for not, but if not, this can be it.",2
89630866,2021-07-01 18:33:23.964,Coroutine async issue with parallel installs version 1.77.0,"We are installing meltano inside a Docker container with a fairly simple `pip install upgrade pip` + `pip install meltano` with the conditions:

Docker Image: Ubuntu 20.04
Python version: 3.8.5
Meltano version: 1.77.0 (but applies to 1.73.0 - 1.77.0)

and the command `meltano install` causes the following problem:

```
meltano install
Installing 3 plugins...
Installing loader 'target-mysql'...
Installing loader 'target-postgres'...
Installing extractor 'tap-s3-csv'...
'coroutine' object has no attribute 'read'
```

We are installing 1 Singer tap and 2 targets, not a particularly complicated setup. To solve this problem, we are currently using Meltano version 1.72.0. Note that I am totally new to Meltano, so I can't really propose comprehensive solutions. Please let me know if you need any more information.

UPDATE: That was the original situation but once I did `apt-get python3-venv`, all of these issues got solved and meltano version 1.77.0 was installed as it should be. So, maybe we just need better documentation on how to install Meltano in a custom docker container.",4
89548766,2021-06-30 14:42:27.059,meltano add extractor tap-google-analytics --include-related   causes a plugin install error.,"### What is the current *bug* behavior?
When running `meltano add extractor tap-google-analytics --include-related` you get an error `File transform/dbt_project.yml could not be found. Run meltano add files dbt to set up a dbt project.` and at the end of the install process you get messages saying `Installed 5/6 plugins` & `Failed to install plugin(s)`

The dbt files do still get added under in the transform dir.

This is using python 3.8.5 & Meltano version 1.77.0


### What is the expected *correct* behavior?
No error should be displaying.



### Steps to reproduce
1. `meltano init new_project`
2. Inside the new project run `meltano add extractor tap-google-analytics --include-related`
3. You get the error `File transform/dbt_project.yml' could not be found. Run `meltano add files dbt` to set up a dbt project.` and the summary `Installed 5/6 plugins` & `Failed to install plugin(s)`
4. Look inside the transform dir, and you'll see the dbt files that apparently failed to install.


### Relevant logs and/or screenshots

```
(venv) dw:~/Documents/test-google$ meltano add extractor tap-google-analytics --include-related
Added extractor 'tap-google-analytics' to your Meltano project
Variant:	meltano (default)
Repository:	https://gitlab.com/meltano/tap-google-analytics
Documentation:	https://hub.meltano.com/extractors/google-analytics.html

Added related transform 'tap-google-analytics' to your Meltano project
Variant:	meltano (default)
Repository:	https://gitlab.com/meltano/dbt-tap-google-analytics

Added related transformer 'dbt' to your Meltano project
Repository:	https://github.com/fishtown-analytics/dbt
Documentation:	https://meltano.com/docs/transforms.html

Added related file bundle 'dbt' to your Meltano project
Repository:	https://gitlab.com/meltano/files-dbt

Added related model 'model-google-analytics' to your Meltano project
Variant:	meltano (default)
Repository:	https://gitlab.com/meltano/model-google-analytics

Added related dashboard 'dashboard-google-analytics' to your Meltano project
Variant:	meltano (default)
Repository:	https://gitlab.com/meltano/dashboard-google-analytics

Installing dashboard 'dashboard-google-analytics'...
Installing model 'model-google-analytics'...
Installing file bundle 'dbt'...
Installing transformer 'dbt'...
Installing transform 'tap-google-analytics'...
Adding dbt package 'https://gitlab.com/meltano/dbt-tap-google-analytics.git@config-version-2' to 'transform/packages.yml'...
Adding dbt model to 'transform/dbt_project.yml'...
File transform/dbt_project.yml' could not be found. Run `meltano add files dbt` to set up a dbt project.

Installing extractor 'tap-google-analytics'...
Installed model 'model-google-analytics'

Installed file bundle 'dbt'

Adding 'dbt' files to project...
Created transform/dbt_project.yml
Created transform/.gitignore
Created transform/profile/profiles.yml
Created transform/models/.gitkeep
Installed dashboard 'dashboard-google-analytics'

Importing reports...
Imported report 'Audience Overview per month'
Imported report 'Relative change per month'
Imported report 'Active Users'
Imported report 'Audience Overview per day'
Importing dashboards...
Imported dashboard 'Google Analytics'
Installed extractor 'tap-google-analytics'

Installed transformer 'dbt'

Installed 5/6 plugins
Failed to install plugin(s)
```

![meltanodbtfiles](/uploads/12699cb51162d369ff34557c945308bc/meltanodbtfiles.png)

### Possible fixes
_If you can, link to the line of code that might be responsible for the problem or suggest a fix_



### Further regression test
_Ensure we automatically catch similar issues in the future_

- [ ] Write additional adequate test cases and submit test results
- [ ] Test results should be reviewed by a person from the team",1
89428092,2021-06-28 19:15:43.878,Add tap-twilio as discoverable,,1
89415214,2021-06-28 14:42:30.733,Update Slack Users to 1200+,,1
89198678,2021-06-23 21:46:24.752,Add tap-ask-nicely,,1
88909627,2021-06-18 02:53:36.149,Copy updated dbt and transform settings from discovery.yml due to dbt 0.19.1 upgrade,"The following discussions from !2170 should be addressed:

- [ ] @tayloramurphy started a [discussion](https://gitlab.com/meltano/meltano/-/merge_requests/2170#note_602038447):  (+3 comments)

    > @aaronsteers will you also make an MR to update [discovery.yml](https://gitlab.com/meltano/hub/-/blob/main/_data/meltano/discovery.yml), [dbt.yml](https://gitlab.com/meltano/hub/-/blob/main/_data/meltano/transformers/dbt.yml), and the [transform yamls](https://gitlab.com/meltano/hub/-/tree/main/_data/meltano/transforms)?",2
88885342,2021-06-17 14:01:03.276,"Generate a ""run_results"" artifact after any Meltano run","In discussion with Stephen Bailey from Immuta he brought up the idea of having some kind of run artifact similar to what dbt does with [their artifacts](https://docs.getdbt.com/reference/artifacts/dbt-artifacts#when-are-artifacts-produced). 

At a minimum it would be good to have some kind of summary of an `elt` run to be able to ship anywhere, but I could also see us having the equivalent of a manifest generated as well. This would tie in well with https://gitlab.com/meltano/meltano/-/issues/2611",8
88813557,2021-06-16 13:20:23.312,Update location of simplified Spec,"### What is the current *bug* behavior?
Links from google and the Meltano blog point to:

https://meltano.com/docs/singer-spec.html

which returns a 404 Not Found error. Specific blog post:

https://meltano.com/blog/2021/04/07/a-simplified-take-on-the-singer-spec/

### What is the expected *correct* behavior?

New URL is:

https://hub.meltano.com/singer/spec",1
88599822,2021-06-12 00:21:06.844,Publish dbt packages to https://hub.getdbt.com/,"Publishing to dbtHub could give ~~2~~ 3 significant benefits:

1. Allow installation via semantic versioning, which could give a better versioning option vs the branch-pinning solution for `config-version-2` updates needed for #2694. (Semver range restrictions are only available for hub-hosted packages.)
2. Introduce dbt users to Meltano by surfacing our work there on https://hub.getdbt.com/
3. Attract users (and community contributors) for those packages.

@tayloramurphy, @DouweM - Thoughts?

Checklist:

- [ ] Documentation:
    - [ ] Readmes should answer: ""What is Meltano?"" and ""What is Singer?"".
    - [ ] Readmes should mention all streams need to be selected.
    - [ ] Readmes should have a short walkthrough (and/or link to one) using meltano to sink from tap to target and run the transform.
    - [ ] We should note which database(s) each package has been tested on. If possible, we should try to add Snowflake as well as Postgres.
- [ ] CI Testing
    - [ ] We should require an end-to-end CI test for each before publishing to Meltano hub. (This could be the first Meltano experience for many new users, and I expect there could be a large demand/interest.)
- [ ] Migration:
    - [ ] GitHub project migration or mirroring (barring GitLab support on Meltano side https://github.com/fishtown-analytics/hub.getdbt.com/issues/571)",8
88544583,2021-06-10 22:26:11.224,Safely skip `docker build` step (`meltano_dev`),"The `meltano_dev` step takes approximately 14-15 minutes, which is between 33 and 50 percent of average CI build times. We can reduce this time with one or more of the following methods: 

1. Only rebuilding the docker image when dependencies change:
    1. Create a hash of `pyproject.toml` and `poetry.lock` and use a combination/hash of those hashes as the docker tag.
    2. Before running the docker build, check if the docker hash already exists as a tag. If so, reuse the existing image in subsequent steps (skips the build time).
    3. Create a manual build step which can be used to force docker image build a build even when nothing has changed.
2. Running 'fail-early' tests redundantly without a fully-rebuilt docker image.
    1. Start with a stock python image and/or the image from the last stable master build.
    2. Run `poetry install` - which should finish immediately if there's nothing to do.
    3. Run at minimum `pytest [3.8]` and `python_lint [3.8]` using the older image. The results should mostly be reduntent with the similar tests, except they should run (and fail) about 12 minutes faster.
    4. As with the above, if we choose to use the last master image rather than a stock python 3.8 image, we will want a manual build step option to replace whatever prior/cached image with the latest on the current branch.

Any other options/strategies I'm missing?",8
88406866,2021-06-08 16:10:06.672,Docs for ephemeral production pipelines running (incrementally) with no Postgres backend,"In creating the pipeline for the Hub, I did not find docs for how to productionalize a pipeline while not requiring a postgres database. I was thinking of adding a short guide in `Tutorials` or in `Productionalization`.

Here are the related resources I found:

- How to use the `state` extra: https://meltano.com/docs/plugins.html#state-extra
- `--dump` option with example: https://meltano.com/docs/command-line-interface.html#examples

I think the process we want for the Gitlab pipeline is:

1. Load `state.json` from external storage (in our case, either S3 or a Gitlab CI artifact store.)
2. Ensure the `state` extra is configured.
3. ~~**To confirm:** during execution, `state.json` should hopefully be continually updated by meltano. (I think _we do not_ need the `--dump` option, in this case.)~~ During load, state is incremented updated in the system db (only).
   - State must be exported by running a second invocation with the `--dump=state` CLI option.
4. Catch failures and upload the new state regardless, so that incremental processing will continue on the next run even if the operation is interrupted.
5. Still raise a failure (return non-zero) if the pipeline fails, so the job status reflects the execution result.",4
88341741,2021-06-07 17:46:11.086,Add Lightdash as a utility plugin,"This is a new BI tool: open source, (apparently) CI-friendly, and integrated with dbt.

https://www.lightdash.com/


License: MIT

Sample code (inline with a dbt `schema.yml` file):

![image](/uploads/5693ca56e20bfdac84bd0949f7401400/image.png)
From https://docs.lightdash.com/guides/how-to-create-metrics",4
88288543,2021-06-07 01:25:28.832,catalog file is invalid: 'NoneType' object has no attribute 'startswith',"Slack thread:https://meltano.slack.com/archives/C01TCRBBJD7/p1623028702188300

```
[2021-06-06 21:13:58,784] [20702|MainThread|meltano.cli.utils] [DEBUG] Applying catalog rules failed: catalog file is invalid: 'NoneType' object has no attribute 'startswith'
Traceback (most recent call last):
  File ""/home/visch/git/oracle2mssql/.venv/lib/python3.8/site-packages/meltano/core/plugin/singer/tap.py"", line 351, in apply_catalog_rules
    cache_key = self.catalog_cache_key(plugin_invoker)
  File ""/home/visch/git/oracle2mssql/.venv/lib/python3.8/site-packages/meltano/core/plugin/singer/tap.py"", line 372, in catalog_cache_key
    if plugin_invoker.plugin.pip_url.startswith(""-e""):
AttributeError: 'NoneType' object has no attribute 'startswith'
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File ""/home/visch/git/oracle2mssql/.venv/lib/python3.8/site-packages/meltano/cli/__init__.py"", line 44, in main
    cli(obj={""project"": None})
  File ""/home/visch/git/oracle2mssql/.venv/lib/python3.8/site-packages/click/core.py"", line 829, in __call__
    return self.main(*args, **kwargs)
  File ""/home/visch/git/oracle2mssql/.venv/lib/python3.8/site-packages/click/core.py"", line 782, in main
    rv = self.invoke(ctx)
  File ""/home/visch/git/oracle2mssql/.venv/lib/python3.8/site-packages/click/core.py"", line 1259, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/home/visch/git/oracle2mssql/.venv/lib/python3.8/site-packages/click/core.py"", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/home/visch/git/oracle2mssql/.venv/lib/python3.8/site-packages/click/core.py"", line 610, in invoke
    return callback(*args, **kwargs)
  File ""/home/visch/git/oracle2mssql/.venv/lib/python3.8/site-packages/meltano/cli/params.py"", line 23, in decorate
    return func(*args, **kwargs)
  File ""/home/visch/git/oracle2mssql/.venv/lib/python3.8/site-packages/meltano/cli/params.py"", line 56, in decorate
    func(project, *args, **kwargs)
  File ""/home/visch/git/oracle2mssql/.venv/lib/python3.8/site-packages/meltano/cli/invoke.py"", line 65, in invoke
    handle = invoker.invoke(*plugin_args, command=command_name)
  File ""/home/visch/git/oracle2mssql/.venv/lib/python3.8/site-packages/meltano/core/plugin_invoker.py"", line 244, in invoke
    with self._invoke(*args, **kwargs) as (popen_args, popen_options, popen_env):
  File ""/usr/lib/python3.8/contextlib.py"", line 113, in __enter__
    return next(self.gen)
  File ""/home/visch/git/oracle2mssql/.venv/lib/python3.8/site-packages/meltano/core/plugin_invoker.py"", line 229, in _invoke
    with self.plugin.trigger_hooks(""invoke"", self, args):
  File ""/usr/lib/python3.8/contextlib.py"", line 113, in __enter__
    return next(self.gen)
  File ""/home/visch/git/oracle2mssql/.venv/lib/python3.8/site-packages/meltano/core/behavior/hookable.py"", line 70, in trigger_hooks
    self.__class__.trigger(self, f""before_{hook_name}"", *args, **kwargs)
  File ""/home/visch/git/oracle2mssql/.venv/lib/python3.8/site-packages/meltano/core/behavior/hookable.py"", line 97, in trigger
    raise err
  File ""/home/visch/git/oracle2mssql/.venv/lib/python3.8/site-packages/meltano/core/behavior/hookable.py"", line 89, in trigger
    hook_func(target, *args, **kwargs)
  File ""/home/visch/git/oracle2mssql/.venv/lib/python3.8/site-packages/meltano/core/plugin/singer/tap.py"", line 303, in apply_catalog_rules_hook
    self.apply_catalog_rules(plugin_invoker, exec_args)
  File ""/home/visch/git/oracle2mssql/.venv/lib/python3.8/site-packages/meltano/core/plugin/singer/tap.py"", line 365, in apply_catalog_rules
    raise PluginExecutionError(
meltano.core.plugin.error.PluginExecutionError: Applying catalog rules failed: catalog file is invalid: 'NoneType' object has no attribute 'startswith'
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File ""/home/visch/git/oracle2mssql/.venv/lib/python3.8/site-packages/meltano/cli/__init__.py"", line 52, in main
    raise CliError(str(err)) from err
meltano.cli.utils.CliError: Applying catalog rules failed: catalog file is invalid: 'NoneType' object has no attribute 'startswith'
Applying catalog rules failed: catalog file is invalid: 'NoneType' object has no attribute 'startswith'
```

Meltano.yml
```
  - name: tap-name
       namespace: tap_oracle
       #pip_url: git+https://github.com/transferwise/pipelinewise-tap-oracle
       executable: /home/visch/git/tap-oracle/venv/bin/tap-oracle
```


If you change this to 

```
  - name: tap-name
       namespace: tap_oracle
       pip_url: fake
       executable: /home/visch/git/tap-oracle/venv/bin/tap-oracle
```

everything works",2
88229562,2021-06-04 16:17:12.203,health check return 401 when MELTANO_AUTHENTICATION is set to true,"<!---
Please read this!

Before opening a new issue, make sure to search for keywords in the issues
filtered by the ""regression"" or ""bug"" label and verify the issue you're about to submit isn't a duplicate.

If you are submitting an issue with a tap, please include:

- account details
- target details
- entities selected  with meltano select (if you have selected any entities), as the bug may be related to a specific entity
- the full elt command you are running
- full output of the meltano elt command. Logs can get pretty long, so you can add the full log as a snippet in the Meltano project and add a link in the issue.
--->

### What is the current *bug* behavior?
_What is happening now?_
health check (/api/v1/health) return 401 when MELTANO_AUTHENTICATION is set to true.

### What is the expected *correct* behavior?
_What should be happening?_
it should return a 200 without being authentificated.


### Steps to reproduce
_How one can reproduce the issue?_
set MELTANO_AUTHENTICATION to true, run meltano ui and then check the response at http://host:port/api/v1/health
### Relevant logs and/or screenshots
_Please use code blocks (\`\`\`) to format console output_
i will add it later.


### Possible fixes
_If you can, link to the line of code that might be responsible for the problem or suggest a fix_



### Further regression test
_Ensure we automatically catch similar issues in the future_

- [ ] Write additional adequate test cases and submit test results
- [ ] Test results should be reviewed by a person from the team",8
88122570,2021-06-02 19:36:06.165,"Minor tech debt from pre-commit, flakehell, and flake8","As noted in https://gitlab.com/meltano/meltano/-/merge_requests/2150#note_591431777, there's some minor tech debt I want to document for future review.

1. [ ] `pre-commit` hooks are not version locked with poetry
2. [ ] `flakehell` uses a private interface of `flake8`, which broke when `flake8` released `3.9.1` (also for `3.9.2` and likely future releases as well).
3. [ ] Workaround added in https://gitlab.com/meltano/meltano/-/merge_requests/2150 adds a `extended_default_ignore=[]` declaration into `pyproject.toml`, which fixes the issue.

In future, it would be good to see if we can sync the pre-commit hooks with poetry (if possible) and/or other mitigations.",20
87892634,2021-05-28 21:21:07.292,Emit structured (JSON) logs,"All of my meltano logs end up in datadog. If the logs were formatted as json, it would automatically break up each log message into its various components. This is possible to do with a custom regex as well, but not quite as convenient.",8
87892108,2021-05-28 20:57:25.017,Update stale `meltano add --custom` doc example,"I missed this during the release and will need to follow up. The docs are correct in how they instruct the user to proceed, but the example invocation of the `add --custom` command is stale by at least a couple releases.

Search `meltano add --custom extractor tap-covid-19` to find the sample - and update the page with the latest cli prompts and input values.",2
87771495,2021-05-26 19:22:54.447,Change default behavior in `meltano config ... set` when `kind` is missing,"We should considering altering how Meltano behaves when `kind` is missing from specific settings. The default behavior when `kind` is not know would be to treat the value as insensitive. We may instead want to prompt the user to specify the `kind` and/or the sensitivity level of an otherwise undeclared setting value.

For instance, prompting a user to specify it if configuring with `meltano config <plugin> set <setting> <value>. Especially for custom taps (including many taps discovered from the hub), the `kind` and therefor the sensitivity level is not known, and we should probably not automatically default to that value _not_ being a secret. 

This could also serve as a catch for typos or accidentally-incorrect setting names: rather than going through the wizard's steps and storing under the provided setting name, the user may realize they've input an incorrect setting name and abort, then retry the process with the correct setting name.

Here's an example invocation. The result of the below (using an unknown/incorrect setting name) is that the setting gets stored in yaml as clear text:

```
% poetry run meltano config target-jsonl set myset thisisnotasecret
Loader 'target-jsonl' setting 'myset' was set in `meltano.yml`: 'thisisnotasecret'
```


## Proposal

- We should at minimum warn the user that `kind` is missing.
- If an interactive prompt is available, we can ask the user interactively what the proper `kind` value is _before_ serializing the new setting value.
  - The setting's `kind` entry would be serialized to yaml as well.
- We can add a `--quiet` flag optionally, to mute the new behavior and treat the setting as `kind: string` if not set. (Same as current status quo.)

## Proposal (Updated 2022-04-16)

- Combining with #3227+:
  - If `--interactive` is used, we can _also_ prompt for setting config (including `kind`) at the same time as prompting for the new value.
  - If `--interactive` is not set, we can fail with a prompt to use `--interactive` _or_ we can automatically switch to `--interactive` mode if `kind` is missing or the setting is not yet defined.",4
87268193,2021-05-18 09:10:49.995,`meltano invoke airflow` fails with `StrictVersion` error when Airflow is misconfigured,"When LocalExecutor set w/ default SQLite meltano gives StrictVersion object has no attribute 'version' error. 
The actual message should be something like that: ""error: cannot use SQLite with the LocalExecutor.""",4
87229805,2021-05-17 20:00:29.571,Document how we're deploying Meltano for the hub,"With https://gitlab.com/meltano/hub/-/issues/15 we're truly dogfooding Meltano (https://gitlab.com/meltano/meta/-/issues/4). This is a great opportunity to create some relevant content around this effort.

- [ ] Blog Post on how we're using Meltano to build the Hub
- [ ] Documentation update in several areas:
  - [ ] Cross-link in https://meltano.com/docs/production.html
  - [ ] We should have a section on the Hub on how we're building the hub ""Architecture"" 
  - [ ] Create a ""Production Deployments in the Wild"" section or something like that within the Meltano documentation. We can link ours and the GitLab data team's.",4
87118247,2021-05-15 05:39:44.192,Melano cache error noise after a tap-postgres elt,"<!---
Please read this!

Before opening a new issue, make sure to search for keywords in the issues
filtered by the ""regression"" or ""bug"" label and verify the issue you're about to submit isn't a duplicate.

If you are submitting an issue with a tap, please include:

- account details
- target details
- entities selected  with meltano select (if you have selected any entities), as the bug may be related to a specific entity
- the full elt command you are running
- full output of the meltano elt command. Logs can get pretty long, so you can add the full log as a snippet in the Meltano project and add a link in the issue.
--->

### What is the current *bug* behavior?
_What is happening now?_



### What is the expected *correct* behavior?
_What should be happening?_



### Steps to reproduce

Run a postgres to redshift ELT with key-based replication.



### Relevant logs and/or screenshots
![image](/uploads/f3ca4b8681a2bee49e4f0b36dabe3d5e/image.png)


### Possible fixes
_If you can, link to the line of code that might be responsible for the problem or suggest a fix_



### Further regression test
_Ensure we automatically catch similar issues in the future_

- [ ] Write additional adequate test cases and submit test results
- [ ] Test results should be reviewed by a person from the team",4
87104909,2021-05-14 18:02:00.150,"New `meltano state` command to rename, alter, print, and copy job states","### Problem to solve

Sometimes it's nice to be able to change the `job_id` for a particular pipeline, perhaps for example a second version of a plugin is added (you started with `tap-google-analytics` but now have `tap-google-analytics-site1` and `tap-google-analytics-site2`) and you need to disambiguate, or you're still figuring out naming conventions as we are.

If we just change it in `meltano.yml`, Meltano will see new runs as a new job and not find the existing state.

### Further details

As a workaround, we can go into the Meltano database and mess with the `job` table, but this is annoying and error prone.

### Proposal

I see a few directions here. We could add a command like `meltano job rename <old_job_id> <new_job_id>`. `meltano job` could also later include `list`, which might be helpful for building a `tap-meltano` in the future if you want to use Meltano to track pipelines.  Alternate vocabulary could be `runs` instead of `jobs` as it seems like the codebase and docs are preferring that term.

Another way to do it would be to make a `meltano state` command to manage state. It could have `meltano state get <job_id>`, `meltano state set <job_id> <state>`, `meltano state reset <job_id>` (see #2568+). Then renaming could be something like `meltano state get <old_job_id> | meltano state set <new_job_id>`. Pipe syntax would be handy if you wanted to use a tool like `jq` to mutate states in complex ways.

Both of these options use a noun command as a verb command doesn't seem to make sense here. Currently Meltano seems to use a mix of verbs (`discover`, `init`, `install`, `upgrade`) and nouns (`user`, `schema`, `config`).

### Proposal Update (2022-01-31)

_(Appended by AJ.)_

To manage state migration, we'd add `get`, `set`, `clear`, and `copy` options a new `meltano state` command.

1. `list`
   - `meltano state list` (output list of available Job IDs to STDOUT)
1. `set`
   - `meltano state set <JOB-ID> <state-json-text>` (input from text string or STDIN)
   - `meltano state set <JOB-ID> --input-file=<json-state-file>` (input from file)
   - `cat state.json | meltano state set <JOB-ID>` (input from text string or STDIN)
1. `show` (or `get`)
   - `meltano state show <JOB-ID>` (output to STDOUT)
   - `meltano state show <JOB-ID> > <json-state-file>` (output to file)
1. `clear` (or `reset`)
   - `meltano state clear <JOB-ID>` (or `meltano state reset`)
1. `copy`
   - `meltano state copy <JOB-ID-1> <JOB-ID-2>`
1. `move`
   - `meltano state move <JOB-ID-1> <JOB-ID-2>` (or `meltano state rename`)
1. `merge`
   - `meltano state merge <FROM-JOB-ID> <TO-JOB-ID>` (update job 2 state, replacing and merging status from job 1)

### First iteration

As a first iteration, I think we'd just need items `#1-3`, which are `list`, `set`, and `show` - and optionally the fourth subcommand `clear`. With those three or four subcommands, users could accomplish the other functions in a multi-step processes. For instance, instead of `copy`, the user could run something like `meltano state show <OLD-ID> | meltano state set` and accomplish the same effect. Similarly, a `rename` operation could have the same workaround as `copy` except also running a manual `clear` after the copy.",12
86813182,2021-05-10 22:00:55.190,Entity selection does not work when stream names have periods in them,"### What is the current *bug* behavior?
I’ve installed a custom plugin for redshift as an extractor (`pip install tap-redshift`). I had to do some debugging of their code but I got it working all except for the entity selection. When I do a `meltano select tap-redshift --list --all`, I get a list of all the tables in the `schema` set in my config. However, when I try to add model selection to the `meltano.yml` file using the expected syntax (i.e., `database.schema.table.*`), I can’t get any of the entities in that list highlighted in green (‘selected’) no matter what I do. It almost feels like the select cli command doesn’t play nice with custom plugins.


### What is the expected *correct* behavior?
If I enter
```
select:
- database.schema.table.*
```
I would expect all columns in the listed table to be selected in green ('selected') after running a `meltano select --list --all` commmand.

### Steps to reproduce
Here's my `meltano.yml`
```
version: 1
plugins:
  extractors:
  - name: tap-redshift
    namespace: tap_redshift
    pip_url: tap-redshift
    executable: tap-redshift
    capabilities:
    - config
    - discover
    - catalog
    - properties
    config:
      host: ############
      port: 5439
      dbname: matillion
      schema: referencedata
      start_date: 1900-01-01T01:01:01Z
      user: ########
      password: #########
    select:
    - 'matillion.referencedata.t_ref_score_range.*'
    metadata:
      '*':
        replication-method: FULL_TABLE

```


### Relevant logs and/or screenshots
```
% meltano select tap-redshift --list --all                                   
Legend:
        selected
        excluded
        automatic

Enabled patterns:
        matillion.referencedata.t_ref_score_range.*

Selected attributes:
        [excluded ] matillion.referencedata.t_ref_code_set.code_set_code
        [excluded ] matillion.referencedata.t_ref_code_set.code_set_id
        [excluded ] matillion.referencedata.t_ref_code_set.code_set_name
        [excluded ] matillion.referencedata.t_ref_code_set.date_modified
        [excluded ] matillion.referencedata.t_ref_code_set.modified_by
        [excluded ] matillion.referencedata.t_ref_code_set.owner_name
        [excluded ] matillion.referencedata.t_ref_code_set.steward_name
        [excluded ] matillion.referencedata.t_ref_score_range.date_modified
        [excluded ] matillion.referencedata.t_ref_score_range.lower_bound
        [excluded ] matillion.referencedata.t_ref_score_range.metric_name
        [excluded ] matillion.referencedata.t_ref_score_range.score_range_id
        [excluded ] matillion.referencedata.t_ref_score_range.score_value
        [excluded ] matillion.referencedata.t_ref_score_range.upper_bound
```

On a separate note, the workaround I found was to use quotes and ignore the first couple of dots in the entity name. Specifically:
```
    select:
    - '*t_ref_score_range.*'
```
gets me exactly what I want.


### Possible fixes
_If you can, link to the line of code that might be responsible for the problem or suggest a fix_
Not sure",4
86699604,2021-05-08 04:35:49.969,Incorrect docs link after `meltano add transformer dbt`,"After installing dbt, the docs link provided is `https://meltano.com/docs/transforms.md`. The correct link is: `https://meltano.com/docs/transforms.html`


```
ajsteers@ajs-macbook-pro meltano % meltano add transformer dbt
Transformer 'dbt' already exists in your Meltano project
To add it to your project another time so that each can be configured differently,
add a new plugin inheriting from the existing one with its own unique name:
        meltano add transformer dbt--new --inherit-from dbt

Installing transformer 'dbt'...
Installed transformer 'dbt'


To learn more about transformer 'dbt', visit https://meltano.com/docs/transforms.md
```",1
86543574,2021-05-05 18:18:49.059,Meltano ELT fails when running on native windows,"(needs the Windows Support tag)

To replicate on windows

```
PS C:\code\meltano-windowstest2> meltano --version
meltano, version 1.73.0
PS C:\code\meltano-windowstest2> $Env:PYTHONASYNCIODEBUG=1
PS C:\code\meltano-windowstest2> meltano --log-level=debug elt tap-csv target-csv
```

Output from Meltano command (Note that the data in this instance actually does properly flow to my output file, but the program is stuck and I have to signal the process (CTRL+C) to stop the program, which then leads to the exceptions being dumped. Also note that I tried to dump the output to a log file instead of my command prompt output but that didn't include the exception information from asyncio (`>log 2>&1`) after the CTRL+C was sent so this is just a straight copy from my powershell window instead. 

The exceptions not being printed is probably caused by `loop.set_exception_handler` not being utilized. That could probably be a separate issue, and would help some folks with debugging asyncio. I only stumbled on this because I mess up code all the time and was upset I couldn't see RunTime Exceptions.

**Output from a ""successful"" run**
```
PS C:\code\meltano-windowstest2> meltano --version
meltano, version 1.73.0
PS C:\code\meltano-windowstest2> $Env:PYTHONASYNCIODEBUG=1
PS C:\code\meltano-windowstest2> meltano --log-level=debug elt tap-csv target-csv
[2021-05-05 14:12:21,076] [12024|MainThread|root] [DEBUG] Creating engine <meltano.core.project.Project object at 0x000001A94C457970>@sqlite:///C:\code\meltano-windowstest2/.meltano/meltano.db
[2021-05-05 14:12:21,110] [12024|MainThread|asyncio] [DEBUG] Using proactor: IocpProactor
meltano | DEBUG Starting new HTTPS connection (1): www.meltano.com:443
meltano | DEBUG https://www.meltano.com:443 ""GET /discovery.yml?project_id=1281de93-235e-4185-85ce-2b3555a991ae HTTP/1.1"" 200 103053
meltano | DEBUG Variable '$MELTANO_LOAD_SCHEMA' is missing from the environment.
meltano          | INFO Running extract & load...
meltano          | DEBUG Created configuration at C:\code\meltano-windowstest2\.meltano\run\elt\2021-05-05T181221--tap-csv--target-csv\1aec1d92-7fb9-4e86-885f-f71ddf7f6db1\tap.config.json
meltano          | DEBUG Could not find tap.properties.json in C:\code\meltano-windowstest2\.meltano\extractors\tap-csv\tap.properties.json, skipping.
meltano          | DEBUG Could not find tap.properties.cache_key in C:\code\meltano-windowstest2\.meltano\extractors\tap-csv\tap.properties.cache_key, skipping.
meltano          | DEBUG Could not find state.json in C:\code\meltano-windowstest2\.meltano\extractors\tap-csv\state.json, skipping.
meltano          | DEBUG Variable '$MELTANO_LOAD_SCHEMA' is missing from the environment.
meltano          | DEBUG Variable '$MELTANO_LOAD_SCHEMA' is missing from the environment.
meltano          | DEBUG Created configuration at C:\code\meltano-windowstest2\.meltano\run\elt\2021-05-05T181221--tap-csv--target-csv\1aec1d92-7fb9-4e86-885f-f71ddf7f6db1\target.config.json
meltano          | WARNING No state was found, complete import.
meltano          | DEBUG Invoking: ['C:\\code\\meltano-windowstest2\\.meltano\\extractors\\tap-csv\\venv\\Scripts\\tap-csv', '--config', 'C:\\code\\meltano-windowstest2\\.meltano\\run\\elt\\2021-05-05T181221--tap-csv--target-csv\\1aec1d92-7fb9-4e86-885f-f71ddf7f6db1\\tap.config.json', '--discover']
meltano          | DEBUG Env: REDACTED
meltano          | WARNING A catalog file was found, but it will be ignored as the extractor does not advertise the `catalog` or `properties` capability
meltano          | DEBUG Invoking: ['C:\\code\\meltano-windowstest2\\.meltano\\extractors\\tap-csv\\venv\\Scripts\\tap-csv', '--config', 'C:\\code\\meltano-windowstest2\\.meltano\\run\\elt\\2021-05-05T181221--tap-csv--target-csv\\1aec1d92-7fb9-4e86-885f-f71ddf7f6db1\\tap.config.json']
meltano          | DEBUG Env: REDACTED
meltano          | DEBUG execute program 'C:\\code\\meltano-windowstest2\\.meltano\\extractors\\tap-csv\\venv\\Scripts\\tap-csv' stdout=<pipe> stderr=<pipe>
meltano          | DEBUG process 'C:\\code\\meltano-windowstest2\\.meltano\\extractors\\tap-csv\\venv\\Scripts\\tap-csv' created: pid 7140
meltano          | WARNING Executing <Task pending name='Task-1' coro=<_run_job() running at c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\elt.py:214> wait_for=<Future pending cb=[<TaskWakeupMethWrapper object at 0x000001A9533305E0>()] created at c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py:422> cb=[_run_until_complete_cb() at c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py:184] created at c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\utils\__init__.py:37> took 0.969 seconds
meltano          | DEBUG <_ProactorReadPipeTransport fd=4>: Fatal read error on pipe transport
Traceback (most recent call last):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\proactor_events.py"", line 299, in _loop_reading
    self._read_fut = self._loop._proactor.recv(self._sock, 32768)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\windows_events.py"", line 445, in recv
    self._register_with_iocp(conn)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\windows_events.py"", line 718, in _register_with_iocp
    _overlapped.CreateIoCompletionPort(obj.fileno(), self._iocp, 0, 0)
OSError: [WinError 87] The parameter is incorrect
meltano          | ERROR Exception in callback _ProactorReadPipeTransport._loop_reading()
handle: <Handle _ProactorReadPipeTransport._loop_reading() created at c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\proactor_events.py:187>
source_traceback: Object created at (most recent call last):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\windows_events.py"", line 316, in run_forever
    super().run_forever()
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py"", line 570, in run_forever
    self._run_once()
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py"", line 1851, in _run_once
    handle._run()
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\events.py"", line 81, in _run
    self._context.run(self._callback, *self._args)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py"", line 201, in _read_from_fd
    read_transport, _ = await loop.connect_read_pipe(
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py"", line 1525, in connect_read_pipe
    transport = self._make_read_pipe_transport(pipe, protocol, waiter)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\proactor_events.py"", line 664, in _make_read_pipe_transport
    return _ProactorReadPipeTransport(self, sock, protocol, waiter, extra)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\proactor_events.py"", line 187, in __init__
    self._loop.call_soon(self._loop_reading)
Traceback (most recent call last):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\proactor_events.py"", line 299, in _loop_reading
    self._read_fut = self._loop._proactor.recv(self._sock, 32768)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\windows_events.py"", line 445, in recv
    self._register_with_iocp(conn)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\windows_events.py"", line 718, in _register_with_iocp
    _overlapped.CreateIoCompletionPort(obj.fileno(), self._iocp, 0, 0)
OSError: [WinError 87] The parameter is incorrect

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\events.py"", line 81, in _run
    self._context.run(self._callback, *self._args)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\proactor_events.py"", line 309, in _loop_reading
    self._fatal_error(exc, 'Fatal read error on pipe transport')
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\proactor_events.py"", line 131, in _fatal_error
    self._force_close(exc)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\proactor_events.py"", line 134, in _force_close
    if self._empty_waiter is not None and not self._empty_waiter.done():
AttributeError: '_ProactorReadPipeTransport' object has no attribute '_empty_waiter'
meltano          | DEBUG <_ProactorReadPipeTransport fd=6>: Fatal read error on pipe transport
Traceback (most recent call last):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\proactor_events.py"", line 299, in _loop_reading
    self._read_fut = self._loop._proactor.recv(self._sock, 32768)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\windows_events.py"", line 445, in recv
    self._register_with_iocp(conn)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\windows_events.py"", line 718, in _register_with_iocp
    _overlapped.CreateIoCompletionPort(obj.fileno(), self._iocp, 0, 0)
OSError: [WinError 87] The parameter is incorrect
meltano          | ERROR Exception in callback _ProactorReadPipeTransport._loop_reading()
handle: <Handle _ProactorReadPipeTransport._loop_reading() created at c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\proactor_events.py:187>
source_traceback: Object created at (most recent call last):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\windows_events.py"", line 316, in run_forever
    super().run_forever()
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py"", line 570, in run_forever
    self._run_once()
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py"", line 1851, in _run_once
    handle._run()
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\events.py"", line 81, in _run
    self._context.run(self._callback, *self._args)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py"", line 201, in _read_from_fd
    read_transport, _ = await loop.connect_read_pipe(
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py"", line 1525, in connect_read_pipe
    transport = self._make_read_pipe_transport(pipe, protocol, waiter)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\proactor_events.py"", line 664, in _make_read_pipe_transport
    return _ProactorReadPipeTransport(self, sock, protocol, waiter, extra)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\proactor_events.py"", line 187, in __init__
    self._loop.call_soon(self._loop_reading)
Traceback (most recent call last):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\proactor_events.py"", line 299, in _loop_reading
    self._read_fut = self._loop._proactor.recv(self._sock, 32768)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\windows_events.py"", line 445, in recv
    self._register_with_iocp(conn)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\windows_events.py"", line 718, in _register_with_iocp
    _overlapped.CreateIoCompletionPort(obj.fileno(), self._iocp, 0, 0)
OSError: [WinError 87] The parameter is incorrect

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\events.py"", line 81, in _run
    self._context.run(self._callback, *self._args)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\proactor_events.py"", line 309, in _loop_reading
    self._fatal_error(exc, 'Fatal read error on pipe transport')
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\proactor_events.py"", line 131, in _fatal_error
    self._force_close(exc)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\proactor_events.py"", line 134, in _force_close
    if self._empty_waiter is not None and not self._empty_waiter.done():
AttributeError: '_ProactorReadPipeTransport' object has no attribute '_empty_waiter'
meltano          | DEBUG Read pipe 4 connected: (<_ProactorReadPipeTransport fd=4>, <asyncio.streams.StreamReaderProtocol object at 0x000001A9530B7B20>)
meltano          | DEBUG Read pipe 6 connected: (<_ProactorReadPipeTransport fd=6>, <asyncio.streams.StreamReaderProtocol object at 0x000001A9530B7CD0>)
meltano          | DEBUG Read pipe 836 connected: (<_ProactorReadPipeTransport fd=836 read=<_OverlappedFuture pending overlapped=<pending, 0x1a953203dc0> cb=[_ProactorReadPipeTransport._loop_reading()] created at c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\windows_events.py:465>>, <ReadSubprocessPipeProto fd=1 pipe=<_ProactorReadPipeTransport fd=836 read=<_OverlappedFuture pending overlapped=<pending, 0x1a953203dc0> cb=[_ProactorReadPipeTransport._loop_reading()] created at c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\windows_events.py:465>>>)
meltano          | DEBUG Read pipe 876 connected: (<_ProactorReadPipeTransport fd=876 read=<_OverlappedFuture pending overlapped=<pending, 0x1a9531180d0> cb=[_ProactorReadPipeTransport._loop_reading()] created at c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\windows_events.py:465>>, <ReadSubprocessPipeProto fd=2 pipe=<_ProactorReadPipeTransport fd=876 read=<_OverlappedFuture pending overlapped=<pending, 0x1a9531180d0> cb=[_ProactorReadPipeTransport._loop_reading()] created at c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\windows_events.py:465>>>)
meltano          | INFO execute program 'C:\\code\\meltano-windowstest2\\.meltano\\extractors\\tap-csv\\venv\\Scripts\\tap-csv': <_WindowsSubprocessTransport pid=7140 running stdout=<_ProactorReadPipeTransport fd=836 read=<_OverlappedFuture pending overlapped=<pending, 0x1a953203dc0> cb=[_ProactorReadPipeTransport._loop_reading()] created at c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\windows_events.py:465>> stderr=<_ProactorReadPipeTransport fd=876 read=<_OverlappedFuture pending overlapped=<pending, 0x1a9531180d0> cb=[_ProactorReadPipeTransport._loop_reading()] created at c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\windows_events.py:465>>>
meltano          | DEBUG Invoking: ['C:\\code\\meltano-windowstest2\\.meltano\\loaders\\target-csv\\venv\\Scripts\\target-csv', '--config', 'C:\\code\\meltano-windowstest2\\.meltano\\run\\elt\\2021-05-05T181221--tap-csv--target-csv\\1aec1d92-7fb9-4e86-885f-f71ddf7f6db1\\target.config.json']
meltano          | DEBUG Env: REDACTED
meltano          | DEBUG execute program 'C:\\code\\meltano-windowstest2\\.meltano\\loaders\\target-csv\\venv\\Scripts\\target-csv' stdin=<pipe> stdout=<pipe> stderr=<pipe>
meltano          | DEBUG process 'C:\\code\\meltano-windowstest2\\.meltano\\loaders\\target-csv\\venv\\Scripts\\target-csv' created: pid 18608
meltano          | DEBUG Write pipe 904 connected: (<_ProactorWritePipeTransport fd=904 read=<_OverlappedFuture pending overlapped=<pending, 0x1a953375430> cb=[_ProactorWritePipeTransport._pipe_closed()] created at c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\windows_events.py:465>>, <WriteSubprocessPipeProto fd=0 pipe=<_ProactorWritePipeTransport fd=904 read=<_OverlappedFuture pending overlapped=<pending, 0x1a953375430> cb=[_ProactorWritePipeTransport._pipe_closed()] created at c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\windows_events.py:465>>>)
meltano          | DEBUG Read pipe 968 connected: (<_ProactorReadPipeTransport fd=968 read=<_OverlappedFuture pending overlapped=<pending, 0x1a953375550> cb=[_ProactorReadPipeTransport._loop_reading()] created at c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\windows_events.py:465>>, <ReadSubprocessPipeProto fd=1 pipe=<_ProactorReadPipeTransport fd=968 read=<_OverlappedFuture pending overlapped=<pending, 0x1a953375550> cb=[_ProactorReadPipeTransport._loop_reading()] created at c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\windows_events.py:465>>>)
meltano          | DEBUG Read pipe 920 connected: (<_ProactorReadPipeTransport fd=920 read=<_OverlappedFuture pending overlapped=<pending, 0x1a953375670> cb=[_ProactorReadPipeTransport._loop_reading()] created at c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\windows_events.py:465>>, <ReadSubprocessPipeProto fd=2 pipe=<_ProactorReadPipeTransport fd=920 read=<_OverlappedFuture pending overlapped=<pending, 0x1a953375670> cb=[_ProactorReadPipeTransport._loop_reading()] created at c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\windows_events.py:465>>>)
meltano          | INFO execute program 'C:\\code\\meltano-windowstest2\\.meltano\\loaders\\target-csv\\venv\\Scripts\\target-csv': <_WindowsSubprocessTransport pid=18608 running stdin=<_ProactorWritePipeTransport fd=904 read=<_OverlappedFuture pending overlapped=<pending, 0x1a953375430> cb=[_ProactorWritePipeTransport._pipe_closed()] created at c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\windows_events.py:465>> stdout=<_ProactorReadPipeTransport fd=968 read=<_OverlappedFuture pending overlapped=<pending, 0x1a953375550> cb=[_ProactorReadPipeTransport._loop_reading()] created at c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\windows_events.py:465>> stderr=<_ProactorReadPipeTransport fd=920 read=<_OverlappedFuture pending overlapped=<pending, 0x1a953375670> cb=[_ProactorReadPipeTransport._loop_reading()] created at c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\windows_events.py:465>>>
tap-csv          | INFO Starting sync
tap-csv          | INFO Syncing entity 'things' from file: 'input.csv'
tap-csv (out)    | {""type"": ""SCHEMA"", ""stream"": ""things"", ""schema"": {""type"": ""object"", ""properties"": {""id"": {""type"": ""string""}, "" name"": {""type"": ""string""}}}, ""key_properties"": [""id""]}
tap-csv (out)    | {""type"": ""RECORD"", ""stream"": ""things"", ""record"": {""id"": ""1"", "" name"": "" Derek""}}
tap-csv (out)    | {""type"": ""RECORD"", ""stream"": ""things"", ""record"": {""id"": ""2"", "" name"": "" Cener""}}
tap-csv (out)    | {""type"": ""STATE"", ""value"": {}}
tap-csv          | INFO Sync completed
meltano          | DEBUG <_ProactorReadPipeTransport fd=876> received EOF
meltano          | DEBUG <_ProactorReadPipeTransport fd=836> received EOF
meltano          | INFO <_WindowsSubprocessTransport pid=7140 running stdout=<_ProactorReadPipeTransport closing fd=836> stderr=<_ProactorReadPipeTransport closing fd=876>> exited with return code 0
target-csv       | INFO Sending version information to singer.io. To disable sending anonymous usage data, set the config parameter ""disable_collection"" to true
target-csv (out) | {}
meltano          | INFO Incremental state has been updated at 2021-05-05 18:12:22.353325.
meltano          | DEBUG Incremental state: {}
meltano          | DEBUG <_ProactorReadPipeTransport fd=968> received EOF
meltano          | DEBUG <_ProactorReadPipeTransport fd=920> received EOF
meltano          | INFO <_WindowsSubprocessTransport pid=18608 running stdin=<_ProactorWritePipeTransport closed> stdout=<_ProactorReadPipeTransport closing fd=968> stderr=<_ProactorReadPipeTransport closing fd=920>> exited with return code 0
meltano          | DEBUG Deleted configuration at C:\code\meltano-windowstest2\.meltano\run\elt\2021-05-05T181221--tap-csv--target-csv\1aec1d92-7fb9-4e86-885f-f71ddf7f6db1\target.config.json
meltano          | DEBUG Deleted configuration at C:\code\meltano-windowstest2\.meltano\run\elt\2021-05-05T181221--tap-csv--target-csv\1aec1d92-7fb9-4e86-885f-f71ddf7f6db1\tap.config.json
meltano          | INFO Extract & load complete!
meltano          | INFO Transformation skipped.

Aborted!
Exception ignored in: <coroutine object _run_job at 0x000001A9530120C0>
RuntimeError: coroutine ignored GeneratorExit
meltano          | ERROR Task was destroyed but it is pending!
source_traceback: Object created at (most recent call last):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""C:\Users\derek\AppData\Local\Programs\Python\Python38\Scripts\meltano.exe\__main__.py"", line 7, in <module>
    sys.exit(main())
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\__init__.py"", line 43, in main
    cli(obj={""project"": None})
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\core.py"", line 829, in __call__
    return self.main(*args, **kwargs)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\core.py"", line 782, in main
    rv = self.invoke(ctx)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\core.py"", line 1259, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\core.py"", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\core.py"", line 610, in invoke
    return callback(*args, **kwargs)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\params.py"", line 23, in decorate
    return func(*args, **kwargs)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\params.py"", line 57, in decorate
    func(project, *args, **kwargs)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\elt.py"", line 137, in elt
    run_async(_run_job(project, job, session, context_builder, force=force))
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\utils\__init__.py"", line 37, in run_async
    future = asyncio.ensure_future(coro)
task: <Task pending name='Task-1' coro=<_run_job() running at c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\elt.py:214> wait_for=<Task pending name='Task-4' coro=<Out._read_from_fd() running at c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py:205> wait_for=<Future pending cb=[<TaskWakeupMethWrapper object at 0x000001A9530B7CA0>()] created at c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py:422> cb=[<TaskWakeupMethWrapper object at 0x000001A953330040>()] created at c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py:148> created at c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\utils\__init__.py:37>
--- Logging error ---
Traceback (most recent call last):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\logging\__init__.py"", line 1084, in emit
    stream.write(msg + self.terminator)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py"", line 65, in write
    self.__out.writeline(line)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py"", line 174, in writeline
    click.echo(self.prefix + line, nl=False, file=self)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\utils.py"", line 272, in echo
    file.write(message)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\_compat.py"", line 710, in _safe_write
    return _write(s)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\colorama\ansitowin32.py"", line 41, in write
    self.__convertor.write(text)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\colorama\ansitowin32.py"", line 164, in write
    self.wrapped.write(text)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py"", line 179, in write
    self.file.write(remove_ansi_escape_sequences(data))
ValueError: I/O operation on closed file.
Call stack:
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py"", line 1733, in call_exception_handler
    self.default_exception_handler(context)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py"", line 1707, in default_exception_handler
    logger.error('\n'.join(log_lines), exc_info=exc_info)
Message: 'Task was destroyed but it is pending!\nsource_traceback: Object created at (most recent call last):\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\runpy.py"", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\runpy.py"", line 87, in _run_code\n    exec(code, run_globals)\n  File ""C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python38\\Scripts\\meltano.exe\\__main__.py"", line 7, in <module>\n    sys.exit(main())\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\cli\\__init__.py"", line 43, in main\n    cli(obj={""project"": None})\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\click\\core.py"", line 829, in __call__\n    return self.main(*args, **kwargs)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\click\\core.py"", line 782, in main\n    rv = self.invoke(ctx)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\click\\core.py"", line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\click\\core.py"", line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\click\\core.py"", line 610, in invoke\n    return callback(*args, **kwargs)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\cli\\params.py"", line 23, in decorate\n    return func(*args, **kwargs)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\cli\\params.py"", line 57, in decorate\n    func(project, *args, **kwargs)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\cli\\elt.py"", line 137, in elt\n    run_async(_run_job(project, job, session, context_builder, force=force))\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\core\\utils\\__init__.py"", line 37, in run_async\n    future = asyncio.ensure_future(coro)\ntask: <Task pending name=\'Task-1\' coro=<_run_job() running at c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\cli\\elt.py:214> wait_for=<Task pending name=\'Task-4\' coro=<Out._read_from_fd() running at c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\core\\logging\\output_logger.py:205> wait_for=<Future pending cb=[<TaskWakeupMethWrapper object at 0x000001A9530B7CA0>()] created at c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\asyncio\\base_events.py:422> cb=[<TaskWakeupMethWrapper object at 0x000001A953330040>()] created at c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\core\\logging\\output_logger.py:148> created at c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\core\\utils\\__init__.py:37>'
Arguments: ()
Exception ignored in: <generator object Job._handling_sigterm at 0x000001A953096AC0>
Traceback (most recent call last):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\job\job.py"", line 229, in _handling_sigterm
    signal.signal(signal.SIGTERM, original_termination_handler)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\signal.py"", line 47, in signal
    handler = _signal.signal(_enum_to_int(signalnum), _enum_to_int(handler))
TypeError: signal handler must be signal.SIG_IGN, signal.SIG_DFL, or a callable object
meltano          | ERROR Task was destroyed but it is pending!
source_traceback: Object created at (most recent call last):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""C:\Users\derek\AppData\Local\Programs\Python\Python38\Scripts\meltano.exe\__main__.py"", line 7, in <module>
    sys.exit(main())
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\__init__.py"", line 43, in main
    cli(obj={""project"": None})
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\core.py"", line 829, in __call__
    return self.main(*args, **kwargs)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\core.py"", line 782, in main
    rv = self.invoke(ctx)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\core.py"", line 1259, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\core.py"", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\core.py"", line 610, in invoke
    return callback(*args, **kwargs)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\params.py"", line 23, in decorate
    return func(*args, **kwargs)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\params.py"", line 57, in decorate
    func(project, *args, **kwargs)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\elt.py"", line 137, in elt
    run_async(_run_job(project, job, session, context_builder, force=force))
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\utils\__init__.py"", line 39, in run_async
    loop.run_until_complete(future)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py"", line 603, in run_until_complete
    self.run_forever()
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\windows_events.py"", line 316, in run_forever
    super().run_forever()
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py"", line 570, in run_forever
    self._run_once()
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py"", line 1851, in _run_once
    handle._run()
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\events.py"", line 81, in _run
    self._context.run(self._callback, *self._args)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\elt.py"", line 214, in _run_job
    await _run_elt(project, context_builder, output_logger)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\elt.py"", line 232, in _run_elt
    async with _redirect_output(output_logger):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\async_generator\_util.py"", line 34, in __aenter__
    return await self._agen.asend(None)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\elt.py"", line 223, in _redirect_output
    async with meltano_stdout.redirect_stdout(), meltano_stderr.redirect_stderr():
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\async_generator\_util.py"", line 34, in __aenter__
    return await self._agen.asend(None)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py"", line 161, in redirect_stdout
    async with self.writer() as stdout:
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\async_generator\_util.py"", line 34, in __aenter__
    return await self._agen.asend(None)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py"", line 148, in writer
    reader = asyncio.ensure_future(self._read_from_fd(read_fd))
task: <Task pending name='Task-3' coro=<Out._read_from_fd() running at c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py:205> wait_for=<Future pending cb=[<TaskWakeupMethWrapper object at 0x000001A9531164F0>()] created at c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py:422> created at c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py:148>
--- Logging error ---
Traceback (most recent call last):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\logging\__init__.py"", line 1084, in emit
    stream.write(msg + self.terminator)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py"", line 65, in write
    self.__out.writeline(line)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py"", line 174, in writeline
    click.echo(self.prefix + line, nl=False, file=self)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\utils.py"", line 272, in echo
    file.write(message)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\_compat.py"", line 710, in _safe_write
    return _write(s)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\colorama\ansitowin32.py"", line 41, in write
    self.__convertor.write(text)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\colorama\ansitowin32.py"", line 164, in write
    self.wrapped.write(text)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py"", line 179, in write
    self.file.write(remove_ansi_escape_sequences(data))
ValueError: I/O operation on closed file.
Call stack:
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py"", line 1733, in call_exception_handler
    self.default_exception_handler(context)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py"", line 1707, in default_exception_handler
    logger.error('\n'.join(log_lines), exc_info=exc_info)
Message: 'Task was destroyed but it is pending!\nsource_traceback: Object created at (most recent call last):\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\runpy.py"", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\runpy.py"", line 87, in _run_code\n    exec(code, run_globals)\n  File ""C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python38\\Scripts\\meltano.exe\\__main__.py"", line 7, in <module>\n    sys.exit(main())\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\cli\\__init__.py"", line 43, in main\n    cli(obj={""project"": None})\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\click\\core.py"", line 829, in __call__\n    return self.main(*args, **kwargs)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\click\\core.py"", line 782, in main\n    rv = self.invoke(ctx)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\click\\core.py"", line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\click\\core.py"", line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\click\\core.py"", line 610, in invoke\n    return callback(*args, **kwargs)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\cli\\params.py"", line 23, in decorate\n    return func(*args, **kwargs)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\cli\\params.py"", line 57, in decorate\n    func(project, *args, **kwargs)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\cli\\elt.py"", line 137, in elt\n    run_async(_run_job(project, job, session, context_builder, force=force))\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\core\\utils\\__init__.py"", line 39, in run_async\n    loop.run_until_complete(future)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\asyncio\\base_events.py"", line 603, in run_until_complete\n    self.run_forever()\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\asyncio\\windows_events.py"", line 316, in run_forever\n    super().run_forever()\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\asyncio\\base_events.py"", line 570, in run_forever\n    self._run_once()\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\asyncio\\base_events.py"", line 1851, in _run_once\n    handle._run()\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\asyncio\\events.py"", line 81, in _run\n    self._context.run(self._callback, *self._args)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\cli\\elt.py"", line 214, in _run_job\n    await _run_elt(project, context_builder, output_logger)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\cli\\elt.py"", line 232, in _run_elt\n    async with _redirect_output(output_logger):\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\async_generator\\_util.py"", line 34, in __aenter__\n    return await self._agen.asend(None)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\cli\\elt.py"", line 223, in _redirect_output\n    async with meltano_stdout.redirect_stdout(), meltano_stderr.redirect_stderr():\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\async_generator\\_util.py"", line 34, in __aenter__\n    return await self._agen.asend(None)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\core\\logging\\output_logger.py"", line 161, in redirect_stdout\n    async with self.writer() as stdout:\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\async_generator\\_util.py"", line 34, in __aenter__\n    return await self._agen.asend(None)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\core\\logging\\output_logger.py"", line 148, in writer\n    reader = asyncio.ensure_future(self._read_from_fd(read_fd))\ntask: <Task pending name=\'Task-3\' coro=<Out._read_from_fd() running at c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\core\\logging\\output_logger.py:205> wait_for=<Future pending cb=[<TaskWakeupMethWrapper object at 0x000001A9531164F0>()] created at c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\asyncio\\base_events.py:422> created at c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\core\\logging\\output_logger.py:148>'
Arguments: ()
meltano          | ERROR Task was destroyed but it is pending!
source_traceback: Object created at (most recent call last):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""C:\Users\derek\AppData\Local\Programs\Python\Python38\Scripts\meltano.exe\__main__.py"", line 7, in <module>
    sys.exit(main())
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\__init__.py"", line 43, in main
    cli(obj={""project"": None})
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\core.py"", line 829, in __call__
    return self.main(*args, **kwargs)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\core.py"", line 782, in main
    rv = self.invoke(ctx)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\core.py"", line 1259, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\core.py"", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\core.py"", line 610, in invoke
    return callback(*args, **kwargs)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\params.py"", line 23, in decorate
    return func(*args, **kwargs)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\params.py"", line 57, in decorate
    func(project, *args, **kwargs)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\elt.py"", line 137, in elt
    run_async(_run_job(project, job, session, context_builder, force=force))
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\utils\__init__.py"", line 39, in run_async
    loop.run_until_complete(future)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py"", line 603, in run_until_complete
    self.run_forever()
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\windows_events.py"", line 316, in run_forever
    super().run_forever()
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py"", line 570, in run_forever
    self._run_once()
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py"", line 1851, in _run_once
    handle._run()
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\events.py"", line 81, in _run
    self._context.run(self._callback, *self._args)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\elt.py"", line 214, in _run_job
    await _run_elt(project, context_builder, output_logger)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\elt.py"", line 232, in _run_elt
    async with _redirect_output(output_logger):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\async_generator\_util.py"", line 34, in __aenter__
    return await self._agen.asend(None)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\elt.py"", line 223, in _redirect_output
    async with meltano_stdout.redirect_stdout(), meltano_stderr.redirect_stderr():
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\async_generator\_util.py"", line 34, in __aenter__
    return await self._agen.asend(None)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py"", line 167, in redirect_stderr
    async with self.writer() as stderr:
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\async_generator\_util.py"", line 34, in __aenter__
    return await self._agen.asend(None)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py"", line 148, in writer
    reader = asyncio.ensure_future(self._read_from_fd(read_fd))
task: <Task pending name='Task-4' coro=<Out._read_from_fd() running at c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py:205> wait_for=<Future pending cb=[<TaskWakeupMethWrapper object at 0x000001A9530B7CA0>()] created at c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py:422> cb=[<TaskWakeupMethWrapper object at 0x000001A953330040>()] created at c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py:148>
--- Logging error ---
Traceback (most recent call last):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\logging\__init__.py"", line 1084, in emit
    stream.write(msg + self.terminator)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py"", line 65, in write
    self.__out.writeline(line)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py"", line 174, in writeline
    click.echo(self.prefix + line, nl=False, file=self)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\utils.py"", line 272, in echo
    file.write(message)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\_compat.py"", line 710, in _safe_write
    return _write(s)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\colorama\ansitowin32.py"", line 41, in write
    self.__convertor.write(text)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\colorama\ansitowin32.py"", line 164, in write
    self.wrapped.write(text)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py"", line 179, in write
    self.file.write(remove_ansi_escape_sequences(data))
ValueError: I/O operation on closed file.
Call stack:
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py"", line 1733, in call_exception_handler
    self.default_exception_handler(context)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py"", line 1707, in default_exception_handler
    logger.error('\n'.join(log_lines), exc_info=exc_info)
Message: 'Task was destroyed but it is pending!\nsource_traceback: Object created at (most recent call last):\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\runpy.py"", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\runpy.py"", line 87, in _run_code\n    exec(code, run_globals)\n  File ""C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python38\\Scripts\\meltano.exe\\__main__.py"", line 7, in <module>\n    sys.exit(main())\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\cli\\__init__.py"", line 43, in main\n    cli(obj={""project"": None})\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\click\\core.py"", line 829, in __call__\n    return self.main(*args, **kwargs)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\click\\core.py"", line 782, in main\n    rv = self.invoke(ctx)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\click\\core.py"", line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\click\\core.py"", line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\click\\core.py"", line 610, in invoke\n    return callback(*args, **kwargs)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\cli\\params.py"", line 23, in decorate\n    return func(*args, **kwargs)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\cli\\params.py"", line 57, in decorate\n    func(project, *args, **kwargs)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\cli\\elt.py"", line 137, in elt\n    run_async(_run_job(project, job, session, context_builder, force=force))\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\core\\utils\\__init__.py"", line 39, in run_async\n    loop.run_until_complete(future)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\asyncio\\base_events.py"", line 603, in run_until_complete\n    self.run_forever()\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\asyncio\\windows_events.py"", line 316, in run_forever\n    super().run_forever()\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\asyncio\\base_events.py"", line 570, in run_forever\n    self._run_once()\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\asyncio\\base_events.py"", line 1851, in _run_once\n    handle._run()\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\asyncio\\events.py"", line 81, in _run\n    self._context.run(self._callback, *self._args)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\cli\\elt.py"", line 214, in _run_job\n    await _run_elt(project, context_builder, output_logger)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\cli\\elt.py"", line 232, in _run_elt\n    async with _redirect_output(output_logger):\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\async_generator\\_util.py"", line 34, in __aenter__\n    return await self._agen.asend(None)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\cli\\elt.py"", line 223, in _redirect_output\n    async with meltano_stdout.redirect_stdout(), meltano_stderr.redirect_stderr():\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\async_generator\\_util.py"", line 34, in __aenter__\n    return await self._agen.asend(None)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\core\\logging\\output_logger.py"", line 167, in redirect_stderr\n    async with self.writer() as stderr:\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\async_generator\\_util.py"", line 34, in __aenter__\n    return await self._agen.asend(None)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\core\\logging\\output_logger.py"", line 148, in writer\n    reader = asyncio.ensure_future(self._read_from_fd(read_fd))\ntask: <Task pending name=\'Task-4\' coro=<Out._read_from_fd() running at c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\core\\logging\\output_logger.py:205> wait_for=<Future pending cb=[<TaskWakeupMethWrapper object at 0x000001A9530B7CA0>()] created at c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\asyncio\\base_events.py:422> cb=[<TaskWakeupMethWrapper object at 0x000001A953330040>()] created at c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\core\\logging\\output_logger.py:148>'
Arguments: ()
Exception ignored in: <coroutine object _AsyncGeneratorContextManager.__aexit__ at 0x000001A953180AC0>
Traceback (most recent call last):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\async_generator\_util.py"", line 84, in __aexit__
    raise
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\async_generator\_util.py"", line 14, in __aexit__
    await self._aiter.aclose()
RuntimeError: aclose(): asynchronous generator is already running
Exception ignored in: <coroutine object _AsyncGeneratorContextManager.__aexit__ at 0x000001A953180DC0>
Traceback (most recent call last):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\async_generator\_util.py"", line 84, in __aexit__
    raise
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\async_generator\_util.py"", line 14, in __aexit__
    await self._aiter.aclose()
RuntimeError: aclose(): asynchronous generator is already running
```



**Output of the same command with a problem reading my catalog file** (ignore the catalog errors, and you'll see the asyncio issues). When there's an issue in the tap asyncio tends to blow up on you with the waiter issue being brought up. Maybe there's something to do with the handling of a tap failure?

```
PS C:\code\meltano-windowstest2> meltano --log-level=debug elt tap-oracle target-csv
[2021-05-05 14:04:38,856] [21520|MainThread|root] [DEBUG] Creating engine <meltano.core.project.Project object at 0x000001F96C087970>@sqlite:///C:\code\meltano-windowstest2/.meltano/meltano.db
[2021-05-05 14:04:38,888] [21520|MainThread|asyncio] [DEBUG] Using proactor: IocpProactor
meltano | DEBUG Starting new HTTPS connection (1): www.meltano.com:443
meltano | DEBUG https://www.meltano.com:443 ""GET /discovery.yml?project_id=1281de93-235e-4185-85ce-2b3555a991ae HTTP/1.1"" 200 103053
meltano | DEBUG Variable '$MELTANO_LOAD_SCHEMA' is missing from the environment.
meltano          | INFO Running extract & load...
meltano          | DEBUG Created configuration at C:\code\meltano-windowstest2\.meltano\run\elt\2021-05-05T180438--tap-oracle--target-csv\8b145347-43c8-402e-ac5f-1e297b0e2f0a\tap.config.json
meltano          | DEBUG Could not find tap.properties.json in C:\code\meltano-windowstest2\.meltano\extractors\tap-oracle\tap.properties.json, skipping.
meltano          | DEBUG Could not find tap.properties.cache_key in C:\code\meltano-windowstest2\.meltano\extractors\tap-oracle\tap.properties.cache_key, skipping.
meltano          | DEBUG Could not find state.json in C:\code\meltano-windowstest2\.meltano\extractors\tap-oracle\state.json, skipping.
meltano          | DEBUG Variable '$MELTANO_LOAD_SCHEMA' is missing from the environment.
meltano          | DEBUG Variable '$MELTANO_LOAD_SCHEMA' is missing from the environment.
meltano          | DEBUG Created configuration at C:\code\meltano-windowstest2\.meltano\run\elt\2021-05-05T180438--tap-oracle--target-csv\8b145347-43c8-402e-ac5f-1e297b0e2f0a\target.config.json
meltano          | WARNING No state was found, complete import.
meltano          | INFO Found catalog in C:\code\meltano-windowstest2\powerschoolcatalog.json
meltano          | DEBUG Deleted configuration at C:\code\meltano-windowstest2\.meltano\run\elt\2021-05-05T180438--tap-oracle--target-csv\8b145347-43c8-402e-ac5f-1e297b0e2f0a\target.config.json
meltano          | DEBUG Deleted configuration at C:\code\meltano-windowstest2\.meltano\run\elt\2021-05-05T180438--tap-oracle--target-csv\8b145347-43c8-402e-ac5f-1e297b0e2f0a\tap.config.json
meltano          | DEBUG ELT could not be completed: Cannot start extractor: Catalog discovery failed: invalid catalog: Expecting value: line 1 column 1 (char 0)
Traceback (most recent call last):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\plugin\singer\tap.py"", line 263, in discover_catalog
    catalog = json.load(catalog_file)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\json\__init__.py"", line 293, in load
    return loads(fp.read(),
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\json\__init__.py"", line 357, in loads
    return _default_decoder.decode(s)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\json\decoder.py"", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\json\decoder.py"", line 355, in raw_decode
    raise JSONDecodeError(""Expecting value"", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\runner\singer.py"", line 95, in invoke
    p_tap = await tap.invoke_async(
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\plugin_invoker.py"", line 231, in invoke_async
    with self._invoke(*args, **kwargs) as (popen_args, popen_options, popen_env):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\contextlib.py"", line 113, in __enter__
    return next(self.gen)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\plugin_invoker.py"", line 212, in _invoke
    with self.plugin.trigger_hooks(""invoke"", self, args):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\contextlib.py"", line 113, in __enter__
    return next(self.gen)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\behavior\hookable.py"", line 70, in trigger_hooks
    self.__class__.trigger(self, f""before_{hook_name}"", *args, **kwargs)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\behavior\hookable.py"", line 97, in trigger
    raise err
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\behavior\hookable.py"", line 89, in trigger
    hook_func(target, *args, **kwargs)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\plugin\singer\tap.py"", line 217, in discover_catalog_hook
    self.discover_catalog(plugin_invoker, exec_args)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\plugin\singer\tap.py"", line 267, in discover_catalog
    raise PluginExecutionError(
meltano.core.plugin.error.PluginExecutionError: Catalog discovery failed: invalid catalog: Expecting value: line 1 column 1 (char 0)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\elt.py"", line 237, in _run_elt
    await _run_extract_load(elt_context, output_logger)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\elt.py"", line 275, in _run_extract_load
    await singer_runner.run(
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\runner\singer.py"", line 257, in run
    await self.invoke(
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\runner\singer.py"", line 101, in invoke
    raise RunnerError(f""Cannot start extractor: {err}"") from err
meltano.core.runner.RunnerError: Cannot start extractor: Catalog discovery failed: invalid catalog: Expecting value: line 1 column 1 (char 0)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\elt.py"", line 225, in _redirect_output
    yield
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\elt.py"", line 246, in _run_elt
    raise CliError(f""ELT could not be completed: {err}"") from err
meltano.cli.utils.CliError: ELT could not be completed: Cannot start extractor: Catalog discovery failed: invalid catalog: Expecting value: line 1 column 1 (char 0)
meltano          | WARNING Executing <Task pending name='Task-1' coro=<_run_job() running at c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\elt.py:214> wait_for=<Task pending name='Task-4' coro=<Out._read_from_fd() running at c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py:192> cb=[<TaskWakeupMethWrapper object at 0x000001F972E2D9A0>()] created at c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py:148> cb=[_run_until_complete_cb() at c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py:184] created at c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\utils\__init__.py:37> took 1.000 seconds
meltano          | DEBUG <_ProactorReadPipeTransport fd=4>: Fatal read error on pipe transport
Traceback (most recent call last):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\proactor_events.py"", line 299, in _loop_reading
    self._read_fut = self._loop._proactor.recv(self._sock, 32768)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\windows_events.py"", line 445, in recv
    self._register_with_iocp(conn)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\windows_events.py"", line 718, in _register_with_iocp
    _overlapped.CreateIoCompletionPort(obj.fileno(), self._iocp, 0, 0)
OSError: [WinError 87] The parameter is incorrect
meltano          | ERROR Exception in callback _ProactorReadPipeTransport._loop_reading()
handle: <Handle _ProactorReadPipeTransport._loop_reading() created at c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\proactor_events.py:187>
source_traceback: Object created at (most recent call last):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\windows_events.py"", line 316, in run_forever
    super().run_forever()
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py"", line 570, in run_forever
    self._run_once()
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py"", line 1851, in _run_once
    handle._run()
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\events.py"", line 81, in _run
    self._context.run(self._callback, *self._args)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py"", line 201, in _read_from_fd
    read_transport, _ = await loop.connect_read_pipe(
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py"", line 1525, in connect_read_pipe
    transport = self._make_read_pipe_transport(pipe, protocol, waiter)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\proactor_events.py"", line 664, in _make_read_pipe_transport
    return _ProactorReadPipeTransport(self, sock, protocol, waiter, extra)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\proactor_events.py"", line 187, in __init__
    self._loop.call_soon(self._loop_reading)
Traceback (most recent call last):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\proactor_events.py"", line 299, in _loop_reading
    self._read_fut = self._loop._proactor.recv(self._sock, 32768)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\windows_events.py"", line 445, in recv
    self._register_with_iocp(conn)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\windows_events.py"", line 718, in _register_with_iocp
    _overlapped.CreateIoCompletionPort(obj.fileno(), self._iocp, 0, 0)
OSError: [WinError 87] The parameter is incorrect

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\events.py"", line 81, in _run
    self._context.run(self._callback, *self._args)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\proactor_events.py"", line 309, in _loop_reading
    self._fatal_error(exc, 'Fatal read error on pipe transport')
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\proactor_events.py"", line 131, in _fatal_error
    self._force_close(exc)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\proactor_events.py"", line 134, in _force_close
    if self._empty_waiter is not None and not self._empty_waiter.done():
AttributeError: '_ProactorReadPipeTransport' object has no attribute '_empty_waiter'
meltano          | DEBUG <_ProactorReadPipeTransport fd=6>: Fatal read error on pipe transport
Traceback (most recent call last):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\proactor_events.py"", line 299, in _loop_reading
    self._read_fut = self._loop._proactor.recv(self._sock, 32768)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\windows_events.py"", line 445, in recv
    self._register_with_iocp(conn)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\windows_events.py"", line 718, in _register_with_iocp
    _overlapped.CreateIoCompletionPort(obj.fileno(), self._iocp, 0, 0)
OSError: [WinError 87] The parameter is incorrect
meltano          | ERROR Exception in callback _ProactorReadPipeTransport._loop_reading()
handle: <Handle _ProactorReadPipeTransport._loop_reading() created at c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\proactor_events.py:187>
source_traceback: Object created at (most recent call last):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\windows_events.py"", line 316, in run_forever
    super().run_forever()
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py"", line 570, in run_forever
    self._run_once()
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py"", line 1851, in _run_once
    handle._run()
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\events.py"", line 81, in _run
    self._context.run(self._callback, *self._args)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py"", line 201, in _read_from_fd
    read_transport, _ = await loop.connect_read_pipe(
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py"", line 1525, in connect_read_pipe
    transport = self._make_read_pipe_transport(pipe, protocol, waiter)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\proactor_events.py"", line 664, in _make_read_pipe_transport
    return _ProactorReadPipeTransport(self, sock, protocol, waiter, extra)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\proactor_events.py"", line 187, in __init__
    self._loop.call_soon(self._loop_reading)
Traceback (most recent call last):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\proactor_events.py"", line 299, in _loop_reading
    self._read_fut = self._loop._proactor.recv(self._sock, 32768)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\windows_events.py"", line 445, in recv
    self._register_with_iocp(conn)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\windows_events.py"", line 718, in _register_with_iocp
    _overlapped.CreateIoCompletionPort(obj.fileno(), self._iocp, 0, 0)
OSError: [WinError 87] The parameter is incorrect

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\events.py"", line 81, in _run
    self._context.run(self._callback, *self._args)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\proactor_events.py"", line 309, in _loop_reading
    self._fatal_error(exc, 'Fatal read error on pipe transport')
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\proactor_events.py"", line 131, in _fatal_error
    self._force_close(exc)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\proactor_events.py"", line 134, in _force_close
    if self._empty_waiter is not None and not self._empty_waiter.done():
AttributeError: '_ProactorReadPipeTransport' object has no attribute '_empty_waiter'
meltano          | DEBUG Read pipe 4 connected: (<_ProactorReadPipeTransport fd=4>, <asyncio.streams.StreamReaderProtocol object at 0x000001F972CF7B20>)
meltano          | DEBUG Read pipe 6 connected: (<_ProactorReadPipeTransport fd=6>, <asyncio.streams.StreamReaderProtocol object at 0x000001F972CF7CD0>)

Aborted!
Exception ignored in: <coroutine object _run_job at 0x000001F972C520C0>
RuntimeError: coroutine ignored GeneratorExit
meltano          | ERROR Task was destroyed but it is pending!
source_traceback: Object created at (most recent call last):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""C:\Users\derek\AppData\Local\Programs\Python\Python38\Scripts\meltano.exe\__main__.py"", line 7, in <module>
    sys.exit(main())
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\__init__.py"", line 43, in main
    cli(obj={""project"": None})
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\core.py"", line 829, in __call__
    return self.main(*args, **kwargs)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\core.py"", line 782, in main
    rv = self.invoke(ctx)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\core.py"", line 1259, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\core.py"", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\core.py"", line 610, in invoke
    return callback(*args, **kwargs)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\params.py"", line 23, in decorate
    return func(*args, **kwargs)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\params.py"", line 57, in decorate
    func(project, *args, **kwargs)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\elt.py"", line 137, in elt
    run_async(_run_job(project, job, session, context_builder, force=force))
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\utils\__init__.py"", line 37, in run_async
    future = asyncio.ensure_future(coro)
task: <Task pending name='Task-1' coro=<_run_job() running at c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\elt.py:214> wait_for=<Task pending name='Task-4' coro=<Out._read_from_fd() running at c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py:205> wait_for=<Future pending cb=[<TaskWakeupMethWrapper object at 0x000001F972CF7CA0>()] created at c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py:422> cb=[<TaskWakeupMethWrapper object at 0x000001F972E2D9A0>()] created at c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py:148> created at c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\utils\__init__.py:37>
--- Logging error ---
Traceback (most recent call last):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\logging\__init__.py"", line 1084, in emit
    stream.write(msg + self.terminator)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py"", line 65, in write
    self.__out.writeline(line)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py"", line 174, in writeline
    click.echo(self.prefix + line, nl=False, file=self)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\utils.py"", line 272, in echo
    file.write(message)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\_compat.py"", line 710, in _safe_write
    return _write(s)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\colorama\ansitowin32.py"", line 41, in write
    self.__convertor.write(text)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\colorama\ansitowin32.py"", line 164, in write
    self.wrapped.write(text)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py"", line 179, in write
    self.file.write(remove_ansi_escape_sequences(data))
ValueError: I/O operation on closed file.
Call stack:
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py"", line 1733, in call_exception_handler
    self.default_exception_handler(context)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py"", line 1707, in default_exception_handler
    logger.error('\n'.join(log_lines), exc_info=exc_info)
Message: 'Task was destroyed but it is pending!\nsource_traceback: Object created at (most recent call last):\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\runpy.py"", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\runpy.py"", line 87, in _run_code\n    exec(code, run_globals)\n  File ""C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python38\\Scripts\\meltano.exe\\__main__.py"", line 7, in <module>\n    sys.exit(main())\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\cli\\__init__.py"", line 43, in main\n    cli(obj={""project"": None})\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\click\\core.py"", line 829, in __call__\n    return self.main(*args, **kwargs)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\click\\core.py"", line 782, in main\n    rv = self.invoke(ctx)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\click\\core.py"", line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\click\\core.py"", line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\click\\core.py"", line 610, in invoke\n    return callback(*args, **kwargs)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\cli\\params.py"", line 23, in decorate\n    return func(*args, **kwargs)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\cli\\params.py"", line 57, in decorate\n    func(project, *args, **kwargs)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\cli\\elt.py"", line 137, in elt\n    run_async(_run_job(project, job, session, context_builder, force=force))\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\core\\utils\\__init__.py"", line 37, in run_async\n    future = asyncio.ensure_future(coro)\ntask: <Task pending name=\'Task-1\' coro=<_run_job() running at c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\cli\\elt.py:214> wait_for=<Task pending name=\'Task-4\' coro=<Out._read_from_fd() running at c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\core\\logging\\output_logger.py:205> wait_for=<Future pending cb=[<TaskWakeupMethWrapper object at 0x000001F972CF7CA0>()] created at c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\asyncio\\base_events.py:422> cb=[<TaskWakeupMethWrapper object at 0x000001F972E2D9A0>()] created at c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\core\\logging\\output_logger.py:148> created at c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\core\\utils\\__init__.py:37>'
Arguments: ()
Exception ignored in: <generator object Job._handling_sigterm at 0x000001F972CD6AC0>
Traceback (most recent call last):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\job\job.py"", line 229, in _handling_sigterm
    signal.signal(signal.SIGTERM, original_termination_handler)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\signal.py"", line 47, in signal
    handler = _signal.signal(_enum_to_int(signalnum), _enum_to_int(handler))
TypeError: signal handler must be signal.SIG_IGN, signal.SIG_DFL, or a callable object
meltano          | ERROR Task was destroyed but it is pending!
source_traceback: Object created at (most recent call last):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""C:\Users\derek\AppData\Local\Programs\Python\Python38\Scripts\meltano.exe\__main__.py"", line 7, in <module>
    sys.exit(main())
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\__init__.py"", line 43, in main
    cli(obj={""project"": None})
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\core.py"", line 829, in __call__
    return self.main(*args, **kwargs)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\core.py"", line 782, in main
    rv = self.invoke(ctx)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\core.py"", line 1259, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\core.py"", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\core.py"", line 610, in invoke
    return callback(*args, **kwargs)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\params.py"", line 23, in decorate
    return func(*args, **kwargs)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\params.py"", line 57, in decorate
    func(project, *args, **kwargs)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\elt.py"", line 137, in elt
    run_async(_run_job(project, job, session, context_builder, force=force))
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\utils\__init__.py"", line 39, in run_async
    loop.run_until_complete(future)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py"", line 603, in run_until_complete
    self.run_forever()
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\windows_events.py"", line 316, in run_forever
    super().run_forever()
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py"", line 570, in run_forever
    self._run_once()
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py"", line 1851, in _run_once
    handle._run()
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\events.py"", line 81, in _run
    self._context.run(self._callback, *self._args)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\elt.py"", line 214, in _run_job
    await _run_elt(project, context_builder, output_logger)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\elt.py"", line 232, in _run_elt
    async with _redirect_output(output_logger):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\async_generator\_util.py"", line 34, in __aenter__
    return await self._agen.asend(None)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\elt.py"", line 223, in _redirect_output
    async with meltano_stdout.redirect_stdout(), meltano_stderr.redirect_stderr():
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\async_generator\_util.py"", line 34, in __aenter__
    return await self._agen.asend(None)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py"", line 161, in redirect_stdout
    async with self.writer() as stdout:
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\async_generator\_util.py"", line 34, in __aenter__
    return await self._agen.asend(None)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py"", line 148, in writer
    reader = asyncio.ensure_future(self._read_from_fd(read_fd))
task: <Task pending name='Task-3' coro=<Out._read_from_fd() running at c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py:205> wait_for=<Future pending cb=[<TaskWakeupMethWrapper object at 0x000001F972CF7AF0>()] created at c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py:422> created at c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py:148>
--- Logging error ---
Traceback (most recent call last):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\logging\__init__.py"", line 1084, in emit
    stream.write(msg + self.terminator)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py"", line 65, in write
    self.__out.writeline(line)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py"", line 174, in writeline
    click.echo(self.prefix + line, nl=False, file=self)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\utils.py"", line 272, in echo
    file.write(message)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\_compat.py"", line 710, in _safe_write
    return _write(s)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\colorama\ansitowin32.py"", line 41, in write
    self.__convertor.write(text)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\colorama\ansitowin32.py"", line 164, in write
    self.wrapped.write(text)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py"", line 179, in write
    self.file.write(remove_ansi_escape_sequences(data))
ValueError: I/O operation on closed file.
Call stack:
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py"", line 1733, in call_exception_handler
    self.default_exception_handler(context)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py"", line 1707, in default_exception_handler
    logger.error('\n'.join(log_lines), exc_info=exc_info)
Message: 'Task was destroyed but it is pending!\nsource_traceback: Object created at (most recent call last):\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\runpy.py"", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\runpy.py"", line 87, in _run_code\n    exec(code, run_globals)\n  File ""C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python38\\Scripts\\meltano.exe\\__main__.py"", line 7, in <module>\n    sys.exit(main())\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\cli\\__init__.py"", line 43, in main\n    cli(obj={""project"": None})\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\click\\core.py"", line 829, in __call__\n    return self.main(*args, **kwargs)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\click\\core.py"", line 782, in main\n    rv = self.invoke(ctx)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\click\\core.py"", line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\click\\core.py"", line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\click\\core.py"", line 610, in invoke\n    return callback(*args, **kwargs)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\cli\\params.py"", line 23, in decorate\n    return func(*args, **kwargs)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\cli\\params.py"", line 57, in decorate\n    func(project, *args, **kwargs)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\cli\\elt.py"", line 137, in elt\n    run_async(_run_job(project, job, session, context_builder, force=force))\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\core\\utils\\__init__.py"", line 39, in run_async\n    loop.run_until_complete(future)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\asyncio\\base_events.py"", line 603, in run_until_complete\n    self.run_forever()\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\asyncio\\windows_events.py"", line 316, in run_forever\n    super().run_forever()\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\asyncio\\base_events.py"", line 570, in run_forever\n    self._run_once()\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\asyncio\\base_events.py"", line 1851, in _run_once\n    handle._run()\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\asyncio\\events.py"", line 81, in _run\n    self._context.run(self._callback, *self._args)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\cli\\elt.py"", line 214, in _run_job\n    await _run_elt(project, context_builder, output_logger)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\cli\\elt.py"", line 232, in _run_elt\n    async with _redirect_output(output_logger):\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\async_generator\\_util.py"", line 34, in __aenter__\n    return await self._agen.asend(None)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\cli\\elt.py"", line 223, in _redirect_output\n    async with meltano_stdout.redirect_stdout(), meltano_stderr.redirect_stderr():\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\async_generator\\_util.py"", line 34, in __aenter__\n    return await self._agen.asend(None)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\core\\logging\\output_logger.py"", line 161, in redirect_stdout\n    async with self.writer() as stdout:\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\async_generator\\_util.py"", line 34, in __aenter__\n    return await self._agen.asend(None)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\core\\logging\\output_logger.py"", line 148, in writer\n    reader = asyncio.ensure_future(self._read_from_fd(read_fd))\ntask: <Task pending name=\'Task-3\' coro=<Out._read_from_fd() running at c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\core\\logging\\output_logger.py:205> wait_for=<Future pending cb=[<TaskWakeupMethWrapper object at 0x000001F972CF7AF0>()] created at c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\asyncio\\base_events.py:422> created at c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\core\\logging\\output_logger.py:148>'
Arguments: ()
meltano          | ERROR Task was destroyed but it is pending!
source_traceback: Object created at (most recent call last):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""C:\Users\derek\AppData\Local\Programs\Python\Python38\Scripts\meltano.exe\__main__.py"", line 7, in <module>
    sys.exit(main())
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\__init__.py"", line 43, in main
    cli(obj={""project"": None})
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\core.py"", line 829, in __call__
    return self.main(*args, **kwargs)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\core.py"", line 782, in main
    rv = self.invoke(ctx)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\core.py"", line 1259, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\core.py"", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\core.py"", line 610, in invoke
    return callback(*args, **kwargs)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\params.py"", line 23, in decorate
    return func(*args, **kwargs)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\params.py"", line 57, in decorate
    func(project, *args, **kwargs)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\elt.py"", line 137, in elt
    run_async(_run_job(project, job, session, context_builder, force=force))
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\utils\__init__.py"", line 39, in run_async
    loop.run_until_complete(future)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py"", line 603, in run_until_complete
    self.run_forever()
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\windows_events.py"", line 316, in run_forever
    super().run_forever()
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py"", line 570, in run_forever
    self._run_once()
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py"", line 1851, in _run_once
    handle._run()
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\events.py"", line 81, in _run
    self._context.run(self._callback, *self._args)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\elt.py"", line 214, in _run_job
    await _run_elt(project, context_builder, output_logger)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\elt.py"", line 232, in _run_elt
    async with _redirect_output(output_logger):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\async_generator\_util.py"", line 34, in __aenter__
    return await self._agen.asend(None)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\cli\elt.py"", line 223, in _redirect_output
    async with meltano_stdout.redirect_stdout(), meltano_stderr.redirect_stderr():
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\async_generator\_util.py"", line 34, in __aenter__
    return await self._agen.asend(None)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py"", line 167, in redirect_stderr
    async with self.writer() as stderr:
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\async_generator\_util.py"", line 34, in __aenter__
    return await self._agen.asend(None)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py"", line 148, in writer
    reader = asyncio.ensure_future(self._read_from_fd(read_fd))
task: <Task pending name='Task-4' coro=<Out._read_from_fd() running at c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py:205> wait_for=<Future pending cb=[<TaskWakeupMethWrapper object at 0x000001F972CF7CA0>()] created at c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py:422> cb=[<TaskWakeupMethWrapper object at 0x000001F972E2D9A0>()] created at c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py:148>
--- Logging error ---
Traceback (most recent call last):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\logging\__init__.py"", line 1084, in emit
    stream.write(msg + self.terminator)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py"", line 65, in write
    self.__out.writeline(line)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py"", line 174, in writeline
    click.echo(self.prefix + line, nl=False, file=self)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\utils.py"", line 272, in echo
    file.write(message)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\click\_compat.py"", line 710, in _safe_write
    return _write(s)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\colorama\ansitowin32.py"", line 41, in write
    self.__convertor.write(text)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\colorama\ansitowin32.py"", line 164, in write
    self.wrapped.write(text)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\meltano\core\logging\output_logger.py"", line 179, in write
    self.file.write(remove_ansi_escape_sequences(data))
ValueError: I/O operation on closed file.
Call stack:
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py"", line 1733, in call_exception_handler
    self.default_exception_handler(context)
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\asyncio\base_events.py"", line 1707, in default_exception_handler
    logger.error('\n'.join(log_lines), exc_info=exc_info)
Message: 'Task was destroyed but it is pending!\nsource_traceback: Object created at (most recent call last):\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\runpy.py"", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\runpy.py"", line 87, in _run_code\n    exec(code, run_globals)\n  File ""C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python38\\Scripts\\meltano.exe\\__main__.py"", line 7, in <module>\n    sys.exit(main())\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\cli\\__init__.py"", line 43, in main\n    cli(obj={""project"": None})\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\click\\core.py"", line 829, in __call__\n    return self.main(*args, **kwargs)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\click\\core.py"", line 782, in main\n    rv = self.invoke(ctx)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\click\\core.py"", line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\click\\core.py"", line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\click\\core.py"", line 610, in invoke\n    return callback(*args, **kwargs)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\cli\\params.py"", line 23, in decorate\n    return func(*args, **kwargs)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\cli\\params.py"", line 57, in decorate\n    func(project, *args, **kwargs)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\cli\\elt.py"", line 137, in elt\n    run_async(_run_job(project, job, session, context_builder, force=force))\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\core\\utils\\__init__.py"", line 39, in run_async\n    loop.run_until_complete(future)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\asyncio\\base_events.py"", line 603, in run_until_complete\n    self.run_forever()\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\asyncio\\windows_events.py"", line 316, in run_forever\n    super().run_forever()\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\asyncio\\base_events.py"", line 570, in run_forever\n    self._run_once()\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\asyncio\\base_events.py"", line 1851, in _run_once\n    handle._run()\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\asyncio\\events.py"", line 81, in _run\n    self._context.run(self._callback, *self._args)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\cli\\elt.py"", line 214, in _run_job\n    await _run_elt(project, context_builder, output_logger)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\cli\\elt.py"", line 232, in _run_elt\n    async with _redirect_output(output_logger):\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\async_generator\\_util.py"", line 34, in __aenter__\n    return await self._agen.asend(None)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\cli\\elt.py"", line 223, in _redirect_output\n    async with meltano_stdout.redirect_stdout(), meltano_stderr.redirect_stderr():\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\async_generator\\_util.py"", line 34, in __aenter__\n    return await self._agen.asend(None)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\core\\logging\\output_logger.py"", line 167, in redirect_stderr\n    async with self.writer() as stderr:\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\async_generator\\_util.py"", line 34, in __aenter__\n    return await self._agen.asend(None)\n  File ""c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\core\\logging\\output_logger.py"", line 148, in writer\n    reader = asyncio.ensure_future(self._read_from_fd(read_fd))\ntask: <Task pending name=\'Task-4\' coro=<Out._read_from_fd() running at c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\core\\logging\\output_logger.py:205> wait_for=<Future pending cb=[<TaskWakeupMethWrapper object at 0x000001F972CF7CA0>()] created at c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\asyncio\\base_events.py:422> cb=[<TaskWakeupMethWrapper object at 0x000001F972E2D9A0>()] created at c:\\users\\derek\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\meltano\\core\\logging\\output_logger.py:148>'
Arguments: ()
Exception ignored in: <coroutine object _AsyncGeneratorContextManager.__aexit__ at 0x000001F972D5CF40>
Traceback (most recent call last):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\async_generator\_util.py"", line 84, in __aexit__
    raise
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\async_generator\_util.py"", line 14, in __aexit__
    await self._aiter.aclose()
RuntimeError: aclose(): asynchronous generator is already running
Exception ignored in: <coroutine object _AsyncGeneratorContextManager.__aexit__ at 0x000001F972DCA140>
Traceback (most recent call last):
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\async_generator\_util.py"", line 84, in __aexit__
    raise
  File ""c:\users\derek\appdata\local\programs\python\python38\lib\site-packages\async_generator\_util.py"", line 14, in __aexit__
    await self._aiter.aclose()
RuntimeError: aclose(): asynchronous generator is already running
```",4
86416389,2021-05-04 00:07:00.991,Malformed schema_mapping config passed to target-snowflake,"<!---
Please read this!

Before opening a new issue, make sure to search for keywords in the issues
filtered by the ""regression"" or ""bug"" label and verify the issue you're about to submit isn't a duplicate.

If you are submitting an issue with a tap, please include:

- account details
- target details
- entities selected  with meltano select (if you have selected any entities), as the bug may be related to a specific entity
- the full elt command you are running
- full output of the meltano elt command. Logs can get pretty long, so you can add the full log as a snippet in the Meltano project and add a link in the issue.
--->

### What is the current *bug* behavior?
_What is happening now?_

I'm trying to configure `schema_mapping` for the transferwise snowflake target.

```yaml
  loaders:
  - name: target-snowflake
    variant: transferwise
    pip_url: pipelinewise-target-snowflake==1.12.0
    config:
      account: my-account
      dbname: MY_DATABASE
      user: MY_USER
      warehouse: MY_WAREHOUSE
      file_format: MY_SCHEMA.CSV
      role: MY_ROLE
      schema_mapping:
        public:
          target_schema: RAW_PUBLIC
```

This results in this error:

```
target-snowflake | Traceback (most recent call last):
target-snowflake |   File ""/Users/dean/dev/hi-meltano/.meltano/loaders/target-snowflake/venv/bin/target-snowflake"", line 8, in <module>
target-snowflake |     sys.exit(main())
target-snowflake |   File ""/Users/dean/dev/hi-meltano/.meltano/loaders/target-snowflake/venv/lib/python3.8/site-packages/target_snowflake/__init__.py"", line 438, in main
target-snowflake |     table_cache, file_format_type = get_snowflake_statics(config)
target-snowflake |   File ""/Users/dean/dev/hi-meltano/.meltano/loaders/target-snowflake/venv/lib/python3.8/site-packages/target_snowflake/__init__.py"", line 79, in get_snowflake_statics
target-snowflake |     table_schemas=stream_utils.get_schema_names_from_config(config))
target-snowflake |   File ""/Users/dean/dev/hi-meltano/.meltano/loaders/target-snowflake/venv/lib/python3.8/site-packages/target_snowflake/stream_utils.py"", line 43, in get_schema_names_from_config
target-snowflake |     schema_names.append(target.get('target_schema'))
target-snowflake | AttributeError: 'str' object has no attribute 'get'
```

The original yaml config gets converted to json and passed to the snowflake target as a file: https://github.com/transferwise/pipelinewise-target-snowflake/blob/master/target_snowflake/__init__.py#L429

If I edit the plugin and log the contents of this file, I get:

```json
{
  ""account"": ""my-account"",
  ""dbname"": ""MY_DATABASE"",
  ""user"": ""MY_USER"",
  ""password"": ""hunter2"",
  ""warehouse"": ""MY_WAREHOUSE"",
  ""file_format"": ""MY_SCHEMA.CSV"",
  ""default_target_schema"": ""TAP_POSTGRES"",
  ""batch_size_rows"": 100000,
  ""flush_all_streams"": false,
  ""parallelism"": 0,
  ""parallelism_max"": 16,
  ""schema_mapping"": {
    ""public.target_schema"": ""RAW_PUBLIC"",
    ""public"": {
      ""target_schema"": ""RAW_PUBLIC""
    }
  },
  ""disable_table_cache"": false,
  ""add_metadata_columns"": false,
  ""hard_delete"": false,
  ""data_flattening_max_level"": 0,
  ""primary_key_required"": true,
  ""validate_records"": false,
  ""no_compression"": false,
  ""role"": ""MY_ROLE""
}
```

### What is the expected *correct* behavior?
_What should be happening?_

`schema_mapping` in the json should not include `""public.target_schema"": ""RAW_PUBLIC"",`, it should simply be

```
""schema_mapping"": {
    ""public"": {
      ""target_schema"": ""RAW_PUBLIC""
    }
  },
```

### Steps to reproduce
_How one can reproduce the issue?_

python 3.8.9
meltano 1.73.0

Full `meltano.yml`:

```yaml
version: 1
send_anonymous_usage_stats: false
plugins:
  extractors:
  - name: tap-postgres
    variant: transferwise
    pip_url: pipelinewise-tap-postgres==1.7.1
    config:
      user: postgres
      dbname: my-db
      filter_schemas: public
      default_replication_method: LOG_BASED
      logical_poll_total_seconds: 5
    select_filter:
    - public-my_table
    metadata:
      public-my_table.*:
        replication-method: FULL_TABLE
    schema:
      public-my_table:
        export_file_exchange_id:
          type:
          - string
          - 'null'
          format: date-time
  loaders:
  - name: target-snowflake
    variant: transferwise
    pip_url: pipelinewise-target-snowflake==1.12.0
    config:
      account: my-account
      dbname: MY_DATABASE
      user: MY_USER
      warehouse: MY_WAREHOUSE
      file_format: MY_SCHEMA.CSV
      role: MY_ROLE
      schema_mapping:
        public:
          target_schema: RAW_PUBLIC
```

```
$ meltano install
$ meltano elt tap-postgres target-snowflake
```

### Relevant logs and/or screenshots
_Please use code blocks (\`\`\`) to format console output_



### Possible fixes
_If you can, link to the line of code that might be responsible for the problem or suggest a fix_



### Further regression test
_Ensure we automatically catch similar issues in the future_

- [ ] Write additional adequate test cases and submit test results
- [ ] Test results should be reviewed by a person from the team",8
85822082,2021-04-23 13:21:29.608,"Follow-up from ""building a custom extractor"" tutorial docs","Logging follow-ups for further improvement on the tutorial page:


The following discussions from !2107 should be addressed:

- [ ] @DouweM started a [discussion](https://gitlab.com/meltano/meltano/-/merge_requests/2107#note_558219557):  (+1 comment)

    > Can we refer to the documentation on where this script comes from?

- [ ] @DouweM started a [discussion](https://gitlab.com/meltano/meltano/-/merge_requests/2107#note_558219582): 

    > Like above, I'd rather recommend they use `meltano config <plugin> set password <value>` which will automatically store it in the most appropriate location, which is `.env` in this case.
    > 
    > We don't need to explicitly talk about environment variables then, just like we don't in https://meltano.com/docs/getting-started.html#configure-the-extractor. They can learn more about those if they dive deeper into https://meltano.com/docs/configuration.html and find https://meltano.com/docs/configuration.html#configuring-settings.

- [ ] @DouweM started a [discussion](https://gitlab.com/meltano/meltano/-/merge_requests/2107#note_558219583): 

    > I think we can remove this section entirely, or in favor of a link to https://meltano.com/docs/configuration.html#configuration-layers.
    > 
    > In general, I think this guide should not focus too much on the different ways/places config can be stored; that's something for the Configuration guide.

- [ ] @DouweM started a [discussion](https://gitlab.com/meltano/meltano/-/merge_requests/2107#note_558219587): 

    > Instead of just mentioning a command here in a vacuum, I think we should link to the canonical guide at https://meltano.com/docs/getting-started.html#select-entities-and-attributes-to-extract.
    > 
    > I'm starting to think that this guide covers a lot more than it needs to; basically all of https://meltano.com/docs/getting-started.html still applies, except for some bits under https://meltano.com/docs/getting-started.html#add-an-extractor-to-pull-data-from-a-source. I think it'd be better to only cover on what's custom-extractor-specific, and link to the other guides for everything else.

- [ ] @DouweM started a [discussion](https://gitlab.com/meltano/meltano/-/merge_requests/2107#note_558219590):  (+1 comment)

    > I think this would make more sense as a dedicated doc in the SDK repo that we could link to from here.",2
85584822,2021-04-20 17:29:18.138,Add `--help `docs for meltano CLI,"Several of the CLI commands are not documented in `meltano --help`. Adding descriptions here could help streamline the new user experience.

![image](/uploads/d91d5e25b2e7306ba9aa9e9515f3ef9b/image.png)

Also, many of the descriptions are trailing off in the screenshot above '...'.

CLI documentation for reference: https://meltano.com/docs/command-line-interface.html

Reported in slack: https://meltano.slack.com/archives/C01TCRBBJD7/p1618932986365900?thread_ts=1618925301.362900&cid=C01TCRBBJD7

## Update 2021-11-30

This is much better now but still has some missing hints.

```console
$ meltano --help
Usage: meltano [OPTIONS] COMMAND [ARGS]...

  Get help at https://www.meltano.com/docs/command-line-interface.html

Options:
  --log-level [debug|info|warning|error|critical]
  -v, --verbose
  --environment TEXT              Meltano environment name
  --version                       Show the version and exit.
  --help                          Show this message and exit.

Commands:
  add          Add a plugin to your project.
  config       Display Meltano or plugin configuration.
  discover
  elt          meltano elt EXTRACTOR_NAME LOADER_NAME extractor_name: Which...
  environment  Manage Environments.
  init         Creates a new Meltano project
  install      Installs all the dependencies of your project based on the...
  invoke       Invoke the plugin's executable with specified arguments.
  remove       Remove a plugin from your project.
  schedule     Manage Schedules.
  schema
  select       Execute the meltano select command.
  ui
  upgrade
  user
```",2
85329762,2021-04-16 17:05:33.133,Office Hours on Meltano website should link to #office-hours,"https://meltano.com/docs/community.html#office-hours

:point_up: This should also reference our slack link for #office-hours: https://meltano.slack.com/archives/C01QS0RV78D",1
85056180,2021-04-12 15:55:29.044,Fail when plugin `pip_url` changed and installation is outdated (pip_url drift),"### Problem to solve

I installed Meltano with tap-mysql and target-postgres. After creating some pipelines I realised that for a data set I needed to track deletions and this would require the transferwise variant. I switched the variant in the meltano.yml, but then spent a whilst trying to work out why it was not creating _sdc_deleted_at columns in postgres.

This is partially as I'm new, however the solution was to reinstall target-postgres with the transferwise variant. Everything then just worked, as you'd expect!


### Proposal

It would really help (newer) users if the configuration validated the variant installed when it tried to run them.


### What does success look like, and how can we measure that?

If the installed variant of a tap or a target does not match the variant in the meltano.yml then an error or warning should be shown.

### Proposed spec (added 2022-03-03)

1. During install we drop a marker file into the root of the venv with text of the `pip_url` used during installation. (Or a hash, if we think the pip_url is sensitive.)
   - E.g. `.meltano/.../dbt/meltano_pip_url_marker.txt`
2. During execution we check the marker file against the current `pip_url` value.
3. If the comparison fails, we either warn or throw a hard failure before executing the plugin.",4
82213690,2021-04-02 22:24:10.454,update default dbt version to 0.18.2,"This issue is being created to document an MR already in progress.

At this moment, meltano uses `dbt==0.16.1` for the dbt transformer.  There appear to be some installation issues with pinned version of dbt related to `agate` and `PyICU` dependencies.

dbt recently released https://github.com/fishtown-analytics/dbt/releases/tag/v0.18.2, which pins `agate<1.6.2` to avoid installation errors relating to its new dependency `PyICU`.  

Testing was successful installing `dbt==0.18.2` from meltano:

```
 transformers:
  - name: dbt
    # pip_url: dbt==0.16.1
    pip_url: dbt==0.18.2
```


Note there are significant differences in parsing dbt_project files between versions. Migration guide here: https://docs.getdbt.com/docs/guides/migration-guide/upgrading-to-0-17-0",12
81869676,2021-03-29 19:00:20.927,Include discovery output in `meltano elt` output,"### What is the current *bug* behavior?

When running `meltano --log-level=debug elt tap-spreadsheets-anywhere target-postgres --job_id=test`, with the tap trying to load an unsupported JSON file, error messages from the tap's discover code are not shown in the output.

After checking with @DouweM, it is apparently related to the fact that meltano elt runs the tap twice, once with --discover to generate the catalog, and then in sync mode along with the target. meltano elt only forwards stdout and stderr from that sync mode run, not the discovery run, meaning that any log messages that only show up during discovery mode are lost, unless discovery mode fails entirely and all output is dumped.

### What is the expected *correct* behavior?

Log messages from the discovery step should be forwarded to the output as well.

### Steps to reproduce

- setup `tap-spreadsheets-anywhere` to load a file incorrectly formatted, so that discovery will fail with an error message. The following config for the tap can be used to trigger such an error:
```yaml
plugins:
  extractors:
  - name: tap-spreadsheets-anywhere
    pip_url: git+https://github.com/ets/tap-spreadsheets-anywhere.git
    config:
      tables:
        - path: https://salsa.debian.org/iso-codes-team/iso-codes/-/raw/main/data/
        format: json
        name: iso_countries
        pattern: ""iso_3166-1.*""
        key_properties:
        - 'alpha_2'
        start_date: '2011-01-01T00:00:00Z'
```
- run `meltano --log-level=debug elt tap-spreadsheets-anywhere target-postgres --job_id=test`
- note that the output does not include the expected error messages (`ERROR Unable to write Catalog entry for 'iso_countries' - it will be skipped due to error 'str' object has no attribute 'items'`) should appear in the logs

### Relevant logs and/or screenshots

### Possible fixes




### Further regression test
_Ensure we automatically catch similar issues in the future_

- [ ] Write additional adequate test cases and submit test results
- [ ] Test results should be reviewed by a person from the team",8
81592478,2021-03-24 20:50:58.624,Add Roadmap to handbook,https://gitlab.com/meltano/meta/-/issues/20,1
81299486,2021-03-19 16:25:28.270,Fix Zoom Recommendation,,1
81134488,2021-03-17 20:48:24.306,Add Meltano Backgrounds,,1
81130015,2021-03-17 19:22:48.882,Add Community Page,,1
80943029,2021-03-15 15:13:01.061,Add documentation update to cross-link info between Getting Started and CLI reference,Cross link info on the excludes selection and how to remove. https://gitlab.com/meltano/meltano/-/issues/2638#note_528335663,1
80941096,2021-03-15 14:51:00.488,Write Vision Summary blog post,Follow-up from #2633,2
80938996,2021-03-15 14:28:38.719,Add CODEOWNERS,We should add a codeowners file so that @meltano/core-team can merge into the handbook but still require approval for the main app,1
80515191,2021-03-08 22:45:18.545,Meltano Vision Work,"By Thursday:

* What could Meltano be in 5 years?
* What do we want it to be within the year?
* What's not part of the vision?
* What excites you about this space and your vision?",1
108841987,2022-05-20 01:37:56.829,Use latest Python 3.9 in CI,"[We had to pin Python 3.9 to `3.9.6`](https://gitlab.com/meltano/meltano/-/issues/2913) in our test matrix due to problems with dbt in that version. That makes our `3.9` docker images come with Python `3.9.6`.

This was later fixed upstream in [dbt's `0.20.2` release](https://github.com/dbt-labs/dbt-core/releases/tag/v0.20.2). That means we can go back to using the latest, `3.9.13` at the time of writing.",2
108836039,2022-05-19 20:50:01.939,`tap-quickbooks` should have `properties` capability instead of `catalog`,"- Came up recently in Slack: https://meltano.slack.com/archives/CMN8HELB0/p1652851641112949
- The tap uses `--properties`: https://github.com/hotgluexyz/tap-quickbooks/blob/9a9990601f02e91d9ef1357b91b69624eb9dab2b/tap_quickbooks/__init__.py#L272
- `discovery.yml` says it uses `--catalog`: https://gitlab.com/meltano/meltano/-/blob/6d96c9bc9ec5610d87dc01d568bbdb3b162210a4/src/meltano/core/bundle/discovery.yml#L1724",1
108699520,2022-05-17 18:33:12.801,Telemetry project counts inflated if using a custom project_id,"@edgarrmondragon helpfully pointed me to this block in our [tap-gitlab conversation](https://github.com/MeltanoLabs/tap-gitlab/pull/77#discussion_r875020636):

https://gitlab.com/meltano/meltano/-/blob/4c8cbc0bdb4db0248a378017c3615d88e8cf1878/src/meltano/core/tracking/ga_tracker.py#L84-95

That block apparently fails initialization of project ID _and creates a new one_ if the project ID is not a valid UUID. In my experience, this was getting written back to my project file and then I would continually revert it and it would reset again, etc. In CI, this probably gets a distinct project ID on each run - meaning each run through CI (and each job) looks like a distinct project.

This is very likely creating a significant inflation of our total domain project IDs and I think we should address as ~""urgency::high"". (The code was last touched 12+ months ago, or so, so I imaging this has been an issue for quite a while.)

cc @tayloramurphy, @pnadolny13, @edgarrmondragon, @DouweM",2
108687290,2022-05-17 14:16:41.037,Support a set operation on meltano job for updating tasks,"It was determined to be out of scope to add a `set` operation in https://gitlab.com/meltano/meltano/-/issues/2924

We should review the UX and the data we get on this new command to see the utility of adding an update type command for `meltano job`.",2
108479541,2022-05-12 23:14:17.860,Use `settings_service.feature_flag` convention for lockfiles,"The following discussion from !2596 should be addressed:

- [ ] @cjohnhanson started a [discussion](https://gitlab.com/meltano/meltano/-/merge_requests/2596#note_945226196): 

    > Optional follow-up: once !2643 is merged, update this to follow the new `settings_service.feature_flag` convention OR remove this flag parameter and use the feature flag within `ProjectAddService` instead.",4
108398946,2022-05-11 20:28:10.684,Add job support to schedules and to airflow,"In https://gitlab.com/meltano/meltano/-/issues/2924 we're adding support for top level jobs concept in meltano.yaml along with adding support for jobs expansion to `meltano run`. We also want to add job support to the `meltano schedule` command, and likely also alter how airflow interacts with schedules ala https://gitlab.com/meltano/files-airflow/-/blob/master/bundle/orchestrate/dags/meltano.py. 

## Some clarification on things that caused confusion in #2924.

1. We are NOT deprecating support `elt` schedules in favor of `run`, and `meltano schedule` including `meltano schedule run` will support both.
2. Airflow calls `meltano schedule run` [per schedule](https://gitlab.com/meltano/files-airflow/-/blob/master/bundle/orchestrate/dags/meltano.py#L99), it does NOT use `meltano elt` directly,  `meltano schedule run` uses MeltanoInvoker to spawn a `meltano elt` subprocess.
   - at a minimum `meltano schedule run` will need to support calling both `meltano elt` and `meltano run` based on the scheduled its being asked to run.

## Latest `meltano schedule` UX spec
(ala @tayloramurphy in https://gitlab.com/meltano/meltano/-/issues/2924#note_937990262)

```
meltano schedule <schedule_name> <job_name> <interval> # This is the default
meltano schedule job add <schedule_name> <job_name> <interval> # This is the same behavior as the above but [job] and [add] are default
meltano schedule elt <schedule_name> <extractor> <loader> <interval> [--transform={run,skip,only}] # new syntax for elt workflows Q: Does this break anything in the UI?
meltano schedule elt add <schedule_name> <extractor> <loader> <interval> [--transform={run,skip,only}] # same as above with the default add
meltano schedule <schedule_name> set <job_name> <interval> <environment>
meltano schedule <schedule_name> set name=<job_name> # this is different from the description b/c of the key (same for the next 2) - Q: Is the key required?
meltano schedule <schedule_name> set interval=<interval>
meltano schedule <schedule_name> set environment=<environment>
meltano schedule list [--format=json] Does this work with jobs that are scheduled via the job definition?
meltano schedule run <schedule_name>
```",8
108025823,2022-05-04 17:04:08.967,`meltano config airflow` fails for newly install Airflow plugin,"<!---
Please read this!

Before opening a new issue, make sure to search for keywords in the issues
filtered by the ""regression"" or ""bug"" label and verify the issue you're about to submit isn't a duplicate.

If you are submitting an issue with a tap, please include:

- account details
- target details
- entities selected  with meltano select (if you have selected any entities), as the bug may be related to a specific entity
- the full elt command you are running
- full output of the meltano elt command. Logs can get pretty long, so you can add the full log as a snippet in the Meltano project and add a link in the issue.
--->

### What is the current *bug* behavior?
_What is happening now?_

When running `meltano config airflow` to print the current Airflow configuration, the command fails with the following error:

```bash
[Errno 2] No such file or directory: '/Users/kp/Projects/squared/data/.meltano/run/airflow/airflow.cfg'
```

Once `meltano invoke airflow webserver` has been run once, the missing file is created and the command succeeds in printing the current config.

### What is the expected *correct* behavior?
_What should be happening?_

`meltano config airflow` should be able to print the current Airflow configuration regardless of the existence of Meltano-managed files.


### Steps to reproduce
_How one can reproduce the issue?_

```bash
meltano init airflow_bug

meltano add orchestrator airflow

meltano install

meltano config airflow
```

I am somewhat surprised that `meltano install` is necessary as `meltano config airflow` is just supposed to print the current config 🤔

### Relevant logs and/or screenshots
_Please use code blocks (\`\`\`) to format console output_


### Workaround

As a workaround, a user can run `meltano invoke airflow webserver`, which should fully initialize the airflow resources and prevent the error from triggering.


### Possible fixes
_If you can, link to the line of code that might be responsible for the problem or suggest a fix_



### Further regression test
_Ensure we automatically catch similar issues in the future_

- [ ] Write additional adequate test cases and submit test results
- [ ] Test results should be reviewed by a person from the team",4
107880640,2022-05-02 15:47:40.563,Document and lint against type annotation standards,"As we progressively type-annotate our codebase, we should document our type-related standards and expectations in our [Dev Standards](https://handbook.meltano.com/engineering/dev-standards).

If possible, we should also add a flake8 plugin to lint against these standards as part of pre-commit and CI.

This will help new engineers onboard faster and accelerate our progress towards being able to do static type checking as part of our dev and CI workflow.

Related to [this discussion on a recent MR](https://gitlab.com/meltano/meltano/-/merge_requests/2596#note_932443415).

CC @pandemicsyn @edgarrmondragon",8
107430881,2022-04-28 20:59:58.768,"""move"" and ""copy"" subcommands for ""meltano state""","This is a follow-up from: https://gitlab.com/meltano/meltano/-/issues/2754

We need to add the following subcommands to `meltano state`:
1. `copy`
   * `meltano state copy <JOB-ID-1> <JOB-ID-2>`
1. `move`
   * `meltano state move <JOB-ID-1> <JOB-ID-2>` (or `meltano state rename`)",4
107247045,2022-04-26 22:10:26.516,Make setting-level `env:` declaration a one-way injection into the plugin execution context,"Follows from #3431: We want the `env:` usage pattern to be fully unambiguous. The use case this exists for is so that plugins can receive injected environment variable values from Meltano.

1. The user sets the setting value in any supported way.
2. The plugin gets that setting value from the `env:`.
3. If an env var is already set by that name, its value will be replaced by the setting value.


For example: if the OS context has an env var set as `DEBUG_MODE=1` but my plugin has a setting with `env: DEBUG_MODE` and setting value of `0`, then the plugin when invoked will have `DEBUG_MODE=0`, which is the value that the plugin is intended to receive by the Meltano user. ~~Any other env vars from context will be ignored _unless_ they are referenced under `value:`, `env_aliases:`, or the `config:` entry.~~ Any other env vars from context will be ignored _**when initializing the plugins settings**_ _unless_ they are referenced under `value:`, `env_aliases:`, or the `config:` entry.",4
107246486,2022-04-26 21:47:20.962,dbt-snowflake fails with meltano run,"<!---
Please read this!

Before opening a new issue, make sure to search for keywords in the issues
filtered by the ""regression"" or ""bug"" label and verify the issue you're about to submit isn't a duplicate.

If you are submitting an issue with a tap, please include:

- account details
- target details
- entities selected  with meltano select (if you have selected any entities), as the bug may be related to a specific entity
- the full elt command you are running
- full output of the meltano elt command. Logs can get pretty long, so you can add the full log as a snippet in the Meltano project and add a link in the issue.
--->

### What is the current *bug* behavior?
_What is happening now?_

dbt-snowflake fails to run commands defined in discovery.yml when using `meltano run`

### What is the expected *correct* behavior?
_What should be happening?_

`meltano run` should be able to execute dbt-snowflake commands as defined in discovery.yml.

### Steps to reproduce
_How one can reproduce the issue?_


```bash
meltano init test
cd test
meltano add transformer dbt-snowflake
meltano invoke dbt-snowflake:deps
# 21:39:53  Running with dbt=1.0.5
# 21:39:53  Warning: No packages were found in packages.yml
meltano run dbt-snowflake:deps
# error
```



### Relevant logs and/or screenshots
_Please use code blocks (\`\`\`) to format console output_

```
2022-04-26T21:40:31.292906Z [info     ] Environment 'dev' is active
2022-04-26T21:40:32.305402Z [info     ] commands                       commands={0: 'deps'}
2022-04-26T21:40:34.292208Z [info     ] usage: dbt [-h] [--version] [-r RECORD_TIMING_INFO] [-d] cmd_type=command name=dbt-snowflake stdio=stderr
2022-04-26T21:40:34.292637Z [info     ]            [--log-format {text,json,default}] [--no-write-json] cmd_type=command name=dbt-snowflake stdio=stderr
2022-04-26T21:40:34.292745Z [info     ]            [--use-colors | --no-use-colors] [--printer-width PRINTER_WIDTH] cmd_type=command name=dbt-snowflake stdio=stderr
2022-04-26T21:40:34.292895Z [info     ]            [--warn-error] [--no-version-check] cmd_type=command name=dbt-snowflake stdio=stderr
2022-04-26T21:40:34.293012Z [info     ]            [--partial-parse | --no-partial-parse] [--use-experimental-parser] cmd_type=command name=dbt-snowflake stdio=stderr
2022-04-26T21:40:34.293159Z [info     ]            [--no-static-parser] [--profiles-dir PROFILES_DIR] cmd_type=command name=dbt-snowflake stdio=stderr
2022-04-26T21:40:34.293346Z [info     ]            [--no-anonymous-usage-stats] [-x] cmd_type=command name=dbt-snowflake stdio=stderr
2022-04-26T21:40:34.293586Z [info     ]            [--event-buffer-size EVENT_BUFFER_SIZE] cmd_type=command name=dbt-snowflake stdio=stderr
2022-04-26T21:40:34.293738Z [info     ]            {docs,source,init,clean,debug,deps,list,ls,build,snapshot,run,compile,parse,test,seed,run-operation} cmd_type=command name=dbt-snowflake stdio=stderr
2022-04-26T21:40:34.294010Z [info     ]            ...                 cmd_type=command name=dbt-snowflake stdio=stderr
2022-04-26T21:40:34.294178Z [info     ] dbt: error: unrecognized arguments: None cmd_type=command name=dbt-snowflake stdio=stderr
2022-04-26T21:40:34.407488Z [error    ] Block run completed.           block_type=InvokerCommand err=RunnerError('`dbt-snowflake deps` failed with exit code: 2') exit_codes={} set_number=0 success=False
Run invocation could not be completed as block failed: `dbt-snowflake deps` failed with exit code: 2
```

### Possible fixes
_If you can, link to the line of code that might be responsible for the problem or suggest a fix_



### Further regression test
_Ensure we automatically catch similar issues in the future_

- [ ] Write additional adequate test cases and submit test results
- [ ] Test results should be reviewed by a person from the team",2
107234777,2022-04-26 16:18:33.336,Clean up docs.meltano.com UI pages,"- Take new screenshots of UI to replace outdated ones
- Check language to make sure it actually works still
- Streamline if possible
- Add explainers akin to the method definitions on the CLI page that tell users how to use the UI",4
107078338,2022-04-23 23:11:08.151,New paradigm for bundling plugin dependencies and `files` bundles,"Today:


```yml
transformers:
  - name: dbt
    namespace: dbt
    docs: https://docs.meltano.com/guide/transformation
    repo: https://github.com/dbt-labs/dbt-core
    pip_url: dbt-core~=1.0.0 dbt-postgres~=1.0.0 dbt-redshift~=1.0.0 dbt-snowflake~=1.0.0 dbt-bigquery~=1.0.0
    settings:
    # ...
orchestrators:
  - name: airflow
    namespace: airflow
    docs: https://docs.meltano.com/guide/orchestration
    repo: https://github.com/apache/airflow
    pip_url: 'apache-airflow==2.1.2 --constraint https://raw.githubusercontent.com/apache/airflow/constraints-2.1.2/constraints-${MELTANO__PYTHON_VERSION}.txt'
    settings:
    # ...
# ...
files:
  - name: airflow
    namespace: airflow # << auto-installed with `dbt` due to matching `namespace`
    repo: https://gitlab.com/meltano/files-airflow
    pip_url: git+https://gitlab.com/meltano/files-airflow.git
    update:
      orchestrate/dags/meltano.py: true
  - name: dbt
    namespace: dbt # << auto-installed with `dbt` due to matching `namespace`
    repo: https://gitlab.com/meltano/files-dbt
    pip_url: git+https://gitlab.com/meltano/files-dbt.git@config-version-2
```

## The challenge

The number of collisions on namespace will potentially skyrocket one we complete #3283+. We can no longer reasonably scan all the plugins in the hub or in discovery.yml to identify matching namespaces and then have confidence that the matching plugins should be installed together. Instead, we need a more explicit mapping of files resources to the plugins that need those resources.

## Proposed refactor:

- The `namespace` field can be removed (optionally auto-calculated internally per #3374+).
- Files resources are now defined inline with the plugins themselves.

```yml
transformers:
  - name: dbt
    docs: https://docs.meltano.com/guide/transformation
    repo: https://github.com/dbt-labs/dbt-core
    pip_url: dbt-core~=1.0.0 dbt-postgres~=1.0.0 dbt-redshift~=1.0.0 dbt-snowflake~=1.0.0 dbt-bigquery~=1.0.0
    bundle: # or `depends_on:`
      files:
      - name: files-dbt  # Name might be optional, since the plugin has a name in its API definition.
        # This is the API endpoint
        ref: https://hub.meltano.com/meltano/api/v1/plugins/extractors/tap-zendesk--twilio-labs
orchestrators:
  - name: airflow
    docs: https://docs.meltano.com/guide/orchestration
    repo: https://github.com/apache/airflow
    pip_url: 'apache-airflow==2.1.2 --constraint https://raw.githubusercontent.com/apache/airflow/constraints-2.1.2/constraints-${MELTANO__PYTHON_VERSION}.txt'
    bundle:
      files:
      - name: airflow-files
        variant: meltanolabs
    settings:
    # ...
```

The files needed for the repo would then be self-contained within the plugin definition and are self-sufficient for installing the plugin, without needing external plugin to be installed.

## Moving to `copier`

Our current paradigm does not facilitate supporting changes over time to the files (the ""day 2"" problem). A related proposal would be to #3432+, but that can be delivered separately from this change.

## Standalone `files` plugins

We can still support standalone `files` plugins for use cases where it still makes sense to do so apart from auto-installed resources for a `utility`, `transformer`, or `orchestrator` plugin.

However, we would not post separate definitions to the hub for those `files` plugins which are best combined with their particular plugin.",8
106754807,2022-04-18 04:50:25.892,Fail adding `transform` plugin if no `transformer` is added,"Discovered from my dogfooding project (https://gitlab.com/meltano/meta/-/issues/234+), we currently auto-add the `model`, `transform`, and `transformer` plugins when adding extractors.

When adding extractors, we should not auto-add any other plugins:

- `model` plugins are being deprecated.
- `transform` plugins are being deprecated.
- `transformer` plugin can not be inferred to be `dbt`, since we are opting for adapter-specific transformer plugins now.",4
106752184,2022-04-18 02:38:42.052,Allow `--custom` when adding transformers,"I discovered in my dogfooding project (https://gitlab.com/meltano/meta/-/issues/234), that adding `--custom` transformers is explicitly disallowed as of now and this is problematic for the path we are going with #3298.

We should allow `transformer` plugins to be defined via `--custom` as we do today for `extractor` plugins and other types.",4
106667311,2022-04-14 20:18:50.938,metadata extra not honored by inherited plugin,"
### What is the current *bug* behavior?
If the metadata extra is defined in the base configuration of an inherited plugin it is not respected - the metadata from the base configuration of the parent plugin is used instead.



### What is the expected *correct* behavior?
The metadata extra from the inherited plugin definition should be used.



### Steps to reproduce
Using the attached [meltano.yml](/uploads/3ddeb9658b37693e2edfe65519e970c2/meltano.yml) run

`meltano --environment=prod run tap-mysql2 target-jsonl`

SomeTable2 will get replicated but with replication-method=LOG_BASED. If instead you run

`meltano --environment=dev run tap-mysql2 target-jsonl`

SomeTable2 will be replicated using replication-method=FULL_TABLE as expected. The point being that this appears to be a problem specifically with metadata defined on the base inherited plugin only.

Note also that if you change the metadata in the base parent plugin to not use a wildcard, i.e.

```   
    metadata:
      SchemaName-SomeTable:
        replication-method: LOG_BASED
```

The pipeline will fail with an error about the replication method not being defined for SomeTable2. I think that's relevant because it shows that the wildcard isn't overriding the inherited plugin configuration.

### Further regression test
_Ensure we automatically catch similar issues in the future_

- [ ] Write additional adequate test cases and submit test results
- [ ] Test results should be reviewed by a person from the team",4
106549559,2022-04-12 21:13:27.595,meltano add transformer dbt - extra lines in meltano.yml,"After running `meltano add transformer dbt` the `meltano.yml` related config looks like this

```yml
  transformers:
  - name: dbt
    pip_url: 'dbt-core~=1.0.0 dbt-postgres~=1.0.0 dbt-redshift~=1.0.0 dbt-snowflake~=1.0.0
      dbt-bigquery~=1.0.0

      '
  files:
  - name: dbt
    pip_url: git+https://gitlab.com/meltano/files-dbt.git@config-version-2
```

Note the 2/3 extra new lines",1
106532103,2022-04-12 14:24:44.233,Meltano UI pipelines tab broken since 1.99,"Pipelines tab gets stuck in a loading loop erroring out with:

```
TypeError: Cannot read properties of undefined (reading 'includes')
    at app.4c6077ae.js:1:181845
    at Array.map (<anonymous>)
    at i.cloneDeepPipelines (app.4c6077ae.js:1:181810)
    at nr.get (chunk-vendors.43453e12.js:7:30685)
    at nr.evaluate (chunk-vendors.43453e12.js:7:31686)
    at i.cloneDeepPipelines (chunk-vendors.43453e12.js:7:33379)
    at i.ys (app.4c6077ae.js:1:170909)
    at i.e._render (chunk-vendors.43453e12.js:7:23734)
    at i.r (chunk-vendors.43453e12.js:7:27825)
    at nr.get (chunk-vendors.43453e12.js:7:30685)
at @ chunk-vendors.43453e12.js:7
3app.4c6077ae.js:1 
```

Reason is this line here: https://gitlab.com/meltano/meltano/-/blob/master/src/webapp/src/views/Pipelines.vue#L44

We have schedules with unset `interval` keys, as we don't use Meltano as an orchestrator. Because that line tries to perform an `includes` check on an unset key, it causes a TypeError.",2
106361491,2022-04-08 20:23:21.434,"meltano run , multiple dbt commands leads to wrong dbt commands running","To reproduce run this
1. `meltano run dbt:compile dbt:run` 

Meltano will run `dbt:run` twice


My actual use case is
1. `meltano run dbt:drop_source_table tap-abc target-postgres dbt:run` 

Noticed run was happening twice, was pretty hard to notice this was happening.",2
106360778,2022-04-08 19:56:50.795,meltano init not creating DB,"This happens on windows, I'd assume not other OS's, the DB is not initting. `meltano, version 1.99.0` I'm not certain this is new behavior it could have been around all this time and I just missed putting an issue in (seems unlikely but it's possible) 

1. Run `meltano init test`
1. See output below
```
PS C:\code> meltano --log-level=debug init test-issue
Created test
Creating project files...
  test/
   |-- meltano.yml
   |-- README.md
   |-- requirements.txt
   |-- output\.gitignore
   |-- .gitignore
   |-- model\.gitkeep
   |-- extract\.gitkeep
   |-- load\.gitkeep
   |-- transform\.gitkeep
   |-- analyze\.gitkeep
   |-- notebook\.gitkeep
   |-- orchestrate\.gitkeep
Creating system database...2022-04-08T19:54:04.823778Z [info     ] DB connection failed. Will retry after 5s. Attempt 1/3
2022-04-08T19:54:09.841274Z [info     ] DB connection failed. Will retry after 5s. Attempt 2/3
2022-04-08T19:54:14.857067Z [info     ] DB connection failed. Will retry after 5s. Attempt 3/3
2022-04-08T19:54:19.872743Z [error    ] Could not connect to the Database. Max retries exceeded.
(sqlite3.OperationalError) unable to open database file
(Background on this error at: https://sqlalche.me/e/14/e3q8)

PS C:\code> meltano --version
meltano, version 1.99.0
```",2
106291221,2022-04-07 23:55:09.344,Docs: Document `env` usage in Environments,The [Environment page in the docs](https://docs.meltano.com/concepts/environments) is not currently doing a great work at explaining the effects of using the `env` mapping in an Environment.,1
106264545,2022-04-07 15:45:24.392,Incorrect `get` behaviour with default settings overriding environment-specific plugin settings,"<!---
Please read this!

Before opening a new issue, make sure to search for keywords in the issues
filtered by the ""regression"" or ""bug"" label and verify the issue you're about to submit isn't a duplicate.

If you are submitting an issue with a tap, please include:

- account details
- target details
- entities selected  with meltano select (if you have selected any entities), as the bug may be related to a specific entity
- the full elt command you are running
- full output of the meltano elt command. Logs can get pretty long, so you can add the full log as a snippet in the Meltano project and add a link in the issue.
--->

### What is the current *bug* behavior?

We have incorrect resolution order between default values at the plugin setting level (such as from `discovery.yml`) versus the settings specified in an environment.

If an environment setting is provided, that should take precedent over default values, which is not the case as of now.

This does affect plugin settings at the top-level `plugins` definitions (perhaps only in certain situations though?).

Using `set` to set the value of a plugin extra to a non-default value uses the `default_environment`, however using `set` to set the value _back_ to a default value (explicitly) does not remove the setting override from the default environment.

### What is the expected *correct* behavior?
_What should be happening?_

`set` should work exactly the same for default and non-default values.

### Steps to reproduce
_How one can reproduce the issue?_

As per the screenshot below:

```bash
# add file bundle
meltano add files airflow

# configure file bundle to disable updates
meltano config --plugin-type files airflow set _update ""orchestrate/dags/meltano.py"" false

# ""Current value is still: True (from the default)
# This is the incorrect behavior - env-level plugin settings should take precedence over setting defaults.

# check meltano.yml
cat meltano.yml

# configure file bundle to re-enable updates (using `set`)
meltano config --plugin-type files airflow set _update ""orchestrate/dags/meltano.py"" true

# check meltano.yml again
cat meltano.yml

# key `orchestrate/dags/meltano.py` remains set to `false` in default environment. 
```

### Relevant logs and/or screenshots
_Please use code blocks (\`\`\`) to format console output_

![Screenshot_2022-04-07_at_16.37.42](/uploads/85022ffbcb2b65aaeefafe8ea6ff900b/Screenshot_2022-04-07_at_16.37.42.png)

### Possible fixes
_If you can, link to the line of code that might be responsible for the problem or suggest a fix_



### Further regression test
_Ensure we automatically catch similar issues in the future_

- [ ] Write additional adequate test cases and submit test results
- [ ] Test results should be reviewed by a person from the team",4
106200554,2022-04-06 19:07:00.391,Add guard statements and feature flags for 2.0 release,"In planning for #3292+, the total amount of work for deprecations will be significant and will touch many files. To minimize merge conflicts, we can first create guard statements to raise exceptions if any deprecated code paths are touched. Combined with feature flags and/or a version check statement, we can smartly turn off _and_ turn on features in 2.0 as appropriate.

We can optionally have a `MELTANO_EXPERIMENTAL` environment variable that selectively allows new 2.0 features to be tested from pre-2.0 releases.

A large benefit of this approach is that we'd have reduced git branching, and reduced cost of resolving/managing merge conflicts.",4
106120813,2022-04-05 16:48:58.970,Active Environment Name As Env Variable,"## Proposal

Would expose `MELTANO_ENVIRONMENT` to child processes, even if environment is set via another method (`--environment` at the CLI or `default_environment` in `meltano.yml`.)

## Background

Based on this thread in slack https://meltano.slack.com/archives/C01TCRBBJD7/p1643919224929049

My original comment was:

> I'm looking for a way to detect the active environment using environment variables and I know you can set a meltano environment using --environment=prod or through a MELTANO_ENVIRONMENT. Its easy to solve if I use the second method but if I define it using the --environment argument there doesnt seem to be any environment variable set. Is there a way to consistently do this?

We also now have default environments so the environment can be set in the meltano.yml too.

For any down stream process that wants to call meltano commands like I do in the dbt airflow dag generator https://gitlab.com/pnadolny13/files-airflow-dbt/-/blob/master/bundle/orchestrate/dags/generator_cache_builder.py#L36 it would be helpful to have the meltano environment exposed in the environment variables context. If I call `meltano --environment=prod invoke airflow webserver` then the dag generator should be able to access `MELTANO_ENVIRONMENT`.

It might be an anti pattern but it gives more flexibility to users so we can customize more.",4
106039681,2022-04-04 14:33:52.931,Update dbt commands to use `--select` instead of `--models`,Since dbt v0.21 it's recommended to use the `--select` flag instead of `--models`. https://docs.getdbt.com/reference/node-selection/syntax,2
105841050,2022-03-31 19:33:54.773,Helpful --debug tips not listed,"```
visch@visch-ubuntu:~/git/tap-googleads$ pipx upgrade meltano
meltano is already at latest version 1.98.1 (location: /home/visch/.local/pipx/venvs/meltano)
```

```visch@visch-ubuntu:~/git/tap-googleads$ meltano invoke tap-googleads
Catalog discovery failed: command ['/home/visch/git/tap-googleads/tap-googleads.sh', '--config', '/home/visch/git/tap-googleads/.meltano/run/tap-googleads/tap.c180fcd0-2467-4c37-bbfb-eb5e3a8b0ef7.config.json', '--discover'] returned 1
```

The cmd line response here should have the helpful `log-level=debug` tip, not sure why it doesn't for this case.",1
105786732,2022-03-30 22:39:52.896,"Allow all plugins (utilities, custom plugins, etc.) to be configured in Meltano UI","The proposal here would be to expand the Meltano UI config experience to support interactive configuration of all plugin types.

Essentially the UI would operate the same way as today, except that _any_ plugin could be configured using the same interface.",8
105710071,2022-03-29 20:37:38.107,Mappers with custom names,"I know this usage isn't documented here https://docs.meltano.com/concepts/plugins#mappers , but based on my use of Meltano thus far having different names for Extractors and Loaders hasn't been any consequence in the past.  `transform-activedirectory` is a fork of https://github.com/MeltanoLabs/meltano-map-transform , with the only difference being that I am pointing to a custom SDK implementation due to https://gitlab.com/meltano/sdk/-/issues/355. 

meltano.yml
```yaml
  mappers:
  - name: activedirectory-transformer
    pip_url: git+ssh://git@gitlab.com/autoidm/transform-activedirectory.git
    executable: meltano-map-transform
    mappings:
    - name: adhex-2-uuid
      config:
        stream_maps:
          ""space"": {
            ""guid"": ""uuid(objectguid)""
          }
```





Mapper 'mapper-activedirectory' is not known to Meltano

Custom mapper doesn't work
Logs
```
PS C:\code\bamboohr2ad> meltano --log-level=debug install
2022-03-29T19:18:14.047257Z [debug    ] Creating engine <meltano.core.project.Project object at 0x000000B6955FABB0>@sqlite:///C:\code\bamboohr2ad/.meltano/meltano.db
2022-03-29T19:18:14.328268Z [debug    ] Starting new HTTPS connection (1): discovery.meltano.com:443
2022-03-29T19:18:14.429270Z [debug    ] https://discovery.meltano.com:443 ""GET /discovery.yml?project_id=b48f8a89-62e6-44b3-aaf5-5aafc1a0f0be HTTP/1.1"" 200 22844
2022-03-29T19:18:14.500270Z [debug    ] Encoding detection: ascii is most likely the one.
2022-03-29T19:18:15.927249Z [debug    ] Mapper 'mapper-activedirectory' is not known to Meltano
Traceback (most recent call last):
  File ""C:\Python39\lib\site-packages\meltano\core\utils\__init__.py"", line 300, in find_named
    return next(x for x in xs if x[""name""] == name)
StopIteration

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""C:\Python39\lib\site-packages\meltano\core\plugin_discovery_service.py"", line 247, in find_definition
    return find_named(self.get_plugins_of_type(plugin_type), plugin_name)
  File ""C:\Python39\lib\site-packages\meltano\core\utils\__init__.py"", line 302, in find_named
    raise NotFound(name, obj_type) from stop
meltano.core.utils.NotFound: mapper-activedirectory was not found.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""C:\Python39\lib\site-packages\meltano\cli\__init__.py"", line 47, in main
    cli(obj={""project"": None})
  File ""C:\Python39\lib\site-packages\click\core.py"", line 829, in __call__
    return self.main(*args, **kwargs)
  File ""C:\Python39\lib\site-packages\click\core.py"", line 782, in main
    rv = self.invoke(ctx)
  File ""C:\Python39\lib\site-packages\click\core.py"", line 1259, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""C:\Python39\lib\site-packages\click\core.py"", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""C:\Python39\lib\site-packages\click\core.py"", line 610, in invoke
    return callback(*args, **kwargs)
  File ""C:\Python39\lib\site-packages\meltano\cli\params.py"", line 23, in decorate
    return func(*args, **kwargs)
  File ""C:\Python39\lib\site-packages\meltano\cli\params.py"", line 56, in decorate
    func(project, *args, **kwargs)
  File ""C:\Python39\lib\site-packages\meltano\cli\install.py"", line 45, in install
    plugins = list(plugins_service.plugins())
  File ""C:\Python39\lib\site-packages\meltano\core\project_plugins_service.py"", line 285, in plugins
    for _, plugins in self.plugins_by_type(ensure_parent=ensure_parent).items()
  File ""C:\Python39\lib\site-packages\meltano\core\project_plugins_service.py"", line 267, in plugins_by_type
    return {
  File ""C:\Python39\lib\site-packages\meltano\core\project_plugins_service.py"", line 268, in <dictcomp>
    plugin_type: self.get_plugins_of_type(
  File ""C:\Python39\lib\site-packages\meltano\core\project_plugins_service.py"", line 254, in get_plugins_of_type
    self.ensure_parent(plugin)
  File ""C:\Python39\lib\site-packages\meltano\core\project_plugins_service.py"", line 377, in ensure_parent
    plugin.parent = self.get_parent(plugin)
  File ""C:\Python39\lib\site-packages\meltano\core\project_plugins_service.py"", line 360, in get_parent
    return self.discovery_service.get_base_plugin(plugin)
  File ""C:\Python39\lib\site-packages\meltano\core\plugin_discovery_service.py"", line 270, in get_base_plugin
    plugin = project_plugin.custom_definition or self.find_definition(
  File ""C:\Python39\lib\site-packages\meltano\core\plugin_discovery_service.py"", line 249, in find_definition
    raise PluginNotFoundError(PluginRef(plugin_type, plugin_name)) from err
meltano.core.plugin.error.PluginNotFoundError: Mapper 'mapper-activedirectory' is not known to Meltano

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""C:\Python39\lib\site-packages\meltano\cli\__init__.py"", line 55, in main
    raise CliError(str(err)) from err
meltano.cli.utils.CliError: Mapper 'mapper-activedirectory' is not known to Meltano
Mapper 'mapper-activedirectory' is not known to Meltano

The above exception was the direct cause of the following exception:
```


To make this work I had to do this

```yaml
  mappers:
  - name: meltano-map-transformer
    variant: meltano
    pip_url: git+ssh://git@gitlab.com/autoidm/transform-activedirectory.git
    executable: meltano-map-transform
    mappings:
    - name: adhex-2-uuid
      config:
        stream_maps:
          ""space"": {
            ""guid"": ""uuid(objectguid)""
          }
```

Which is fairly hacky :shrug:",2
105652933,2022-03-29 03:16:59.839,Improve Makefile behavior,"Our Makefile doesn't have a help option AND defaults to executing a build if invoked without arguments. Makefile help/docs can be generated if you run `make explain_makefile` but you have to know that exists (or read the Makefile, in which case you no longer need the explainer). 
With some very minor changes (essentially this bit from https://gitlab.com/pandemicsyn/llamalake/-/blob/main/Makefile#L7 + a few inline comments), we could provide a help option, and use that as the default target:

```
(melty-3.8) ➜ make
api                            build api
base_image                     builds meltano/base
build                          build the ui and api
bundle                         clean up static assets and rerun UI build
clean                          clean statically generated assets
docker_images                  build docker base_image and prod_image
docker_sdist
docs_image
explain_makefile               launch a web server further explaining this makefile.
freeze_db
lint                           run python and eslint tasks
lint_eslint                    run eslint
lint_python                    run json/yaml validation and execute tox to perform python lint tasks
lock                           run poetry lock
prod_image                     builds meltano/meltano the AIO production image that includes static UI artifacts
release                        perform a release, should only be performed as part of the CI pipeline
show_lint                      show python and eslint ralated commands
test                           run tests
ui                             build the UI
```

And explicitly support `help` arg via

```

(melty-3.8) ➜ make help
api                            build api
base_image                     builds meltano/base
build                          build the ui and api
bundle                         clean up static assets and rerun UI build
clean                          clean statically generated assets
docker_images                  build docker base_image and prod_image
docker_sdist
docs_image
explain_makefile               launch a web server further explaining this makefile.
freeze_db
lint                           run python and eslint tasks
lint_eslint                    run eslint
lint_python                    run json/yaml validation and execute tox to perform python lint tasks
lock                           run poetry lock
prod_image                     builds meltano/meltano the AIO production image that includes static UI artifacts
release                        perform a release, should only be performed as part of the CI pipeline
show_lint                      show python and eslint ralated commands
test                           run tests
ui                             build the UI
```",1
105509947,2022-03-25 16:27:39.165,Bug - Missing Anonymous Schedule Events,Related to: https://gitlab.com/meltano/meltano/-/issues/3333,1
105354716,2022-03-23 14:04:14.250,meltano run error: 'SubprocessStreamProtocol' object has no attribute '_closed',"### What is the current *bug* behavior?

> I’m trying out the new meltano run command and I’m getting this error:
'SubprocessStreamProtocol' object has no attribute '_closed'
I don’t have environments set up currently
I’m using target-postgres (pipelinewise) and tap-mysql (singer)
Works fine with etl FWIW.
I think it’s the target that’s failing but am not sure

via https://meltano.slack.com/archives/C01TCRBBJD7/p1647644988807389



### What is the expected *correct* behavior?
_What should be happening?_



### Steps to reproduce
_How one can reproduce the issue?_



### Relevant logs and/or screenshots

```
2022-03-23T03:54:22.383009Z [debug    ] Deleted configuration at /opt/meltano/.meltano/run/tap-mysql/tap.f3cb2165-e227-4ffb-9276-524d205a263e.config.json
2022-03-23T03:54:22.383883Z [debug    ] Deleted configuration at /opt/meltano/.meltano/run/target-postgres/target.4daa2906-118d-476b-a50d-562be5def8c3.config.json
2022-03-23T03:54:22.384867Z [debug    ] 'SubprocessStreamProtocol' object has no attribute '_closed'
Traceback (most recent call last):
  File ""/home/analytics/.cache/pypoetry/virtualenvs/corsair-meltano-8vM4tV9J-py3.7/lib/python3.7/site-packages/meltano/cli/__init__.py"", line 47, in main
    cli(obj={""project"": None})
  File ""/home/analytics/.cache/pypoetry/virtualenvs/corsair-meltano-8vM4tV9J-py3.7/lib/python3.7/site-packages/click/core.py"", line 829, in __call__
    return self.main(*args, **kwargs)
  File ""/home/analytics/.cache/pypoetry/virtualenvs/corsair-meltano-8vM4tV9J-py3.7/lib/python3.7/site-packages/click/core.py"", line 782, in main
    rv = self.invoke(ctx)
  File ""/home/analytics/.cache/pypoetry/virtualenvs/corsair-meltano-8vM4tV9J-py3.7/lib/python3.7/site-packages/click/core.py"", line 1259, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/home/analytics/.cache/pypoetry/virtualenvs/corsair-meltano-8vM4tV9J-py3.7/lib/python3.7/site-packages/click/core.py"", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/home/analytics/.cache/pypoetry/virtualenvs/corsair-meltano-8vM4tV9J-py3.7/lib/python3.7/site-packages/click/core.py"", line 610, in invoke
    return callback(*args, **kwargs)
  File ""/home/analytics/.cache/pypoetry/virtualenvs/corsair-meltano-8vM4tV9J-py3.7/lib/python3.7/site-packages/meltano/cli/params.py"", line 23, in decorate
    return func(*args, **kwargs)
  File ""/home/analytics/.cache/pypoetry/virtualenvs/corsair-meltano-8vM4tV9J-py3.7/lib/python3.7/site-packages/meltano/cli/params.py"", line 56, in decorate
    func(project, *args, **kwargs)
  File ""/home/analytics/.cache/pypoetry/virtualenvs/corsair-meltano-8vM4tV9J-py3.7/lib/python3.7/site-packages/meltano/core/utils/__init__.py"", line 71, in wrapper
    return run_async(func(*args, **kwargs))
  File ""/home/analytics/.cache/pypoetry/virtualenvs/corsair-meltano-8vM4tV9J-py3.7/lib/python3.7/site-packages/meltano/core/utils/__init__.py"", line 50, in run_async
    loop.run_until_complete(future)
  File ""/usr/local/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete
    return future.result()
  File ""/home/analytics/.cache/pypoetry/virtualenvs/corsair-meltano-8vM4tV9J-py3.7/lib/python3.7/site-packages/meltano/cli/run.py"", line 84, in run
    await _run_blocks(parsed_blocks)
  File ""/home/analytics/.cache/pypoetry/virtualenvs/corsair-meltano-8vM4tV9J-py3.7/lib/python3.7/site-packages/meltano/cli/run.py"", line 93, in _run_blocks
    await blk.run()
  File ""/home/analytics/.cache/pypoetry/virtualenvs/corsair-meltano-8vM4tV9J-py3.7/lib/python3.7/site-packages/meltano/core/block/extract_load.py"", line 406, in run
    await self.execute()
  File ""/home/analytics/.cache/pypoetry/virtualenvs/corsair-meltano-8vM4tV9J-py3.7/lib/python3.7/site-packages/meltano/core/block/extract_load.py"", line 396, in execute
    await manager.run()
  File ""/home/analytics/.cache/pypoetry/virtualenvs/corsair-meltano-8vM4tV9J-py3.7/lib/python3.7/site-packages/meltano/core/block/extract_load.py"", line 591, in run
    await self._wait_for_process_completion(self.elb.head)
  File ""/home/analytics/.cache/pypoetry/virtualenvs/corsair-meltano-8vM4tV9J-py3.7/lib/python3.7/site-packages/meltano/core/block/extract_load.py"", line 670, in _wait_for_process_completion
    await self._handle_head_completed(current_head, start_idx)
  File ""/home/analytics/.cache/pypoetry/virtualenvs/corsair-meltano-8vM4tV9J-py3.7/lib/python3.7/site-packages/meltano/core/block/extract_load.py"", line 697, in _handle_head_completed
    await next_head.stdin.wait_closed()
  File ""/usr/local/lib/python3.7/asyncio/streams.py"", line 323, in wait_closed
    await self._protocol._closed
AttributeError: 'SubprocessStreamProtocol' object has no attribute '_closed'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/home/analytics/.cache/pypoetry/virtualenvs/corsair-meltano-8vM4tV9J-py3.7/lib/python3.7/site-packages/meltano/cli/__init__.py"", line 55, in main
    raise CliError(str(err)) from err
meltano.cli.utils.CliError: 'SubprocessStreamProtocol' object has no attribute '_closed'
'SubprocessStreamProtocol' object has no attribute '_closed'
```



### Possible fixes
_If you can, link to the line of code that might be responsible for the problem or suggest a fix_



### Further regression test
_Ensure we automatically catch similar issues in the future_

- [ ] Write additional adequate test cases and submit test results
- [ ] Test results should be reviewed by a person from the team",2
104642226,2022-03-11 20:07:51.129,Executable for `meltano-map-transformer` should be `meltano-map-transform`,"<!---
Please read this!

Before opening a new issue, make sure to search for keywords in the issues
filtered by the ""regression"" or ""bug"" label and verify the issue you're about to submit isn't a duplicate.

If you are submitting an issue with a tap, please include:

- account details
- target details
- entities selected  with meltano select (if you have selected any entities), as the bug may be related to a specific entity
- the full elt command you are running
- full output of the meltano elt command. Logs can get pretty long, so you can add the full log as a snippet in the Meltano project and add a link in the issue.
--->

### What is the current *bug* behavior?

Meltano is trying to call the wrong executable, `meltano-map-transformer`.

### What is the expected *correct* behavior?

Meltano should call `meltano-map-transform`, even though that's not the plugin name.

See in the repo: https://github.com/MeltanoLabs/meltano-map-transform/blob/134642804db3da800ed0cdc5fe9fee63584542ad/pyproject.toml#L61

### Steps to reproduce

1. try running the `meltano-map-transform` with any tap and target

### Relevant logs and/or screenshots

```
meltano --log-level debug run tap-mongodb filter-stream target-postgres

Run invocation could not be completed as block failed: Cannot start plugin meltano-map-transformer: Executable 'meltano-map-transformer' could not be found. Mapper 'meltano-map-transformer' may not havebeen installed yet using `meltano install mapper meltano-map-transformer`, or the executable name may be incorrect.
```

### Possible fixes

Change the `executable` in `discovery.yml`.


### Further regression test
_Ensure we automatically catch similar issues in the future_

- [ ] Write additional adequate test cases and submit test results
- [ ] Test results should be reviewed by a person from the team",1
104505826,2022-03-09 20:07:13.575,Consider removing `asynctest.CoroutineMock` in favor of `mock.AsyncMock` in our tests,"## Why?

- The last release of [`asynctest`](https://pypi.org/project/asynctest/) was on May 2019
- Since Python 3.8, [`AsyncMock`](https://docs.python.org/3/library/unittest.mock.html#unittest.mock.AsyncMock) is shipped with `unittest.mock`.
- For compatibility with Python 3.7, we can still use the third-party [`mock`](https://github.com/testing-cabal/mock/blob/master/CHANGELOG.rst) library.

## Considerations

- `CoroutineMock` is used in all of the following files

   - [tests/meltano/core/test_plugins_test_service.py](https://gitlab.com/meltano/meltano/-/blob/master/tests/meltano/core/test_plugins_test_service.py)
   - [tests/meltano/core/runner/test_runner.py](https://gitlab.com/meltano/meltano/-/blob/master/tests/meltano/core/runner/test_runner.py)
   - [tests/meltano/core/plugin/test_airflow.py](https://gitlab.com/meltano/meltano/-/blob/master/tests/meltano/core/plugin/test_airflow.py)
   - [tests/meltano/core/plugin/singer/test_tap.py](https://gitlab.com/meltano/meltano/-/blob/master/tests/meltano/core/plugin/singer/test_tap.py)
   - [tests/meltano/core/block/test_extract_load.py](https://gitlab.com/meltano/meltano/-/blob/master/tests/meltano/core/block/test_extract_load.py)
   - [tests/meltano/core/block/test_singer.py](https://gitlab.com/meltano/meltano/-/blob/master/tests/meltano/core/block/test_singer.py)
   - [tests/meltano/cli/test_invoke.py](https://gitlab.com/meltano/meltano/-/blob/master/tests/meltano/cli/test_invoke.py)
   - [tests/meltano/cli/test_select.py](https://gitlab.com/meltano/meltano/-/blob/master/tests/meltano/cli/test_select.py)
   - [tests/meltano/cli/test_run.py](https://gitlab.com/meltano/meltano/-/blob/master/tests/meltano/cli/test_run.py)
   - [tests/meltano/cli/test_elt.py](https://gitlab.com/meltano/meltano/-/blob/master/tests/meltano/cli/test_elt.py)
   - [tests/meltano/cli/test_config.py](https://gitlab.com/meltano/meltano/-/blob/master/tests/meltano/cli/test_config.py)

   which will have to be prettified in order for our linter checks to pass.",4
104495633,2022-03-09 16:53:21.991,"When installing a custom extractor / loader, git repository URL points to Meltano gitlab namespace","This is a confusing user experience because users may try to install from a URL that does not exist. https://gitlab.com/meltano/meltano/-/blob/master/src/meltano/cli/utils.py#L119 is where this is getting printed. As a short-term fix we should make this much more generic.

![Screen_Shot_2022-03-09_at_10.51.35_AM](/uploads/579f2f6eb1e74b2d519f5fdb0fc23733/Screen_Shot_2022-03-09_at_10.51.35_AM.png)",1
104423740,2022-03-08 21:05:12.598,Consider displaying test duration information in our CI,"Some unit tests in our CI jobs seem to take significantly longer than the rest to complete, so it'd be useful to identify them.

This should be as easy as adding the `--durations=10 --durations-min=1.0` flags.

Ref: https://docs.pytest.org/en/6.2.x/usage.html?highlight=duration#profiling-test-execution-duration",2
104422465,2022-03-08 20:28:47.288,Consider using a `https` scheme in default Snowplow collector endpoint,"<!---
Please read this!

Before opening a new issue, make sure to search for keywords in the issues
filtered by the ""regression"" or ""bug"" label and verify the issue you're about to submit isn't a duplicate.

If you are submitting an issue with a tap, please include:

- account details
- target details
- entities selected  with meltano select (if you have selected any entities), as the bug may be related to a specific entity
- the full elt command you are running
- full output of the meltano elt command. Logs can get pretty long, so you can add the full log as a snippet in the Meltano project and add a link in the issue.
--->

### What is the current *bug* behavior?
_What is happening now?_

Default Snowplow event emitter is created for an `http` endpoint when it should be `https`.

### What is the expected *correct* behavior?
_What should be happening?_

The default value of `snowplow.collector_endpoints` should contain an `https` endpoint.

Note that `http` may be OK if a redirect is added on the SnowcatCloud side.

### Steps to reproduce
_How one can reproduce the issue?_

NA

### Relevant logs and/or screenshots
_Please use code blocks (\`\`\`) to format console output_

NA

### Possible fixes
_If you can, link to the line of code that might be responsible for the problem or suggest a fix_

Change the value in `src/meltano/core/bundle/settings.yml`

### Further regression test
_Ensure we automatically catch similar issues in the future_

- [ ] Write additional adequate test cases and submit test results
- [ ] Test results should be reviewed by a person from the team",1
104345828,2022-03-07 23:30:58.085,Limit the Snowplow tracker logs,"<!---
Please read this!

Before opening a new issue, make sure to search for keywords in the issues
filtered by the ""regression"" or ""bug"" label and verify the issue you're about to submit isn't a duplicate.

If you are submitting an issue with a tap, please include:

- account details
- target details
- entities selected  with meltano select (if you have selected any entities), as the bug may be related to a specific entity
- the full elt command you are running
- full output of the meltano elt command. Logs can get pretty long, so you can add the full log as a snippet in the Meltano project and add a link in the issue.
--->

### What is the current *bug* behavior?
_What is happening now?_

The snowplow tracker library is printing logs on every CLI invocation that is tracked. This is a bit distracting and not something we want users to see all the time.

### What is the expected *correct* behavior?
_What should be happening?_

Commands should not display tracking logs by default.

### Steps to reproduce
_How one can reproduce the issue?_

1. Run any CLI command that is currently tracked in GA
1. Don't use any custom `logging.yaml` so the the default logging config is used
1. Notice the log lines at the end re Snowplow tracking coming from the libray

### Relevant logs and/or screenshots
_Please use code blocks (\`\`\`) to format console output_

![image](/uploads/30ff0d66caaccec6287ebcaf3828fdee/image.png)

### Possible fixes
_If you can, link to the line of code that might be responsible for the problem or suggest a fix_

Set the level of the `snowplow_tracker.emitters` logger in the default config to `ERROR`.

With that change, these lines can probably be removed:

- https://gitlab.com/meltano/meltano/-/blob/cli-init-experience/src/meltano/cli/initialize.py#L36 (see https://gitlab.com/meltano/meltano/-/merge_requests/2562)
- https://gitlab.com/meltano/meltano/-/blob/02a512b62ac5a8c63a0aff25763cde915fdeec12/tests/meltano/core/test_tracking.py#L44-45

### Further regression test
_Ensure we automatically catch similar issues in the future_

- [ ] Write additional adequate test cases and submit test results
- [ ] Test results should be reviewed by a person from the team",2
103992260,2022-03-02 23:08:14.679,Plugin config in Environments requires a `config` value or `setting` definition in top level plugin declaration,"<!---
Please read this!

Before opening a new issue, make sure to search for keywords in the issues
filtered by the ""regression"" or ""bug"" label and verify the issue you're about to submit isn't a duplicate.

If you are submitting an issue with a tap, please include:

- account details
- target details
- entities selected  with meltano select (if you have selected any entities), as the bug may be related to a specific entity
- the full elt command you are running
- full output of the meltano elt command. Logs can get pretty long, so you can add the full log as a snippet in the Meltano project and add a link in the issue.
--->

### What is the current *bug* behavior?
_What is happening now?_

From @pnadolny13 in https://gitlab.com/meltano/meltano/-/issues/3299#note_860273439

> So it seems like current behavior is that you cant introduce configs at the environment level unless they were already defined in the top level config or theres an explicit setting definition.

### What is the expected *correct* behavior?
_What should be happening?_

Config values defined _only_ in an Environment should work without requiring the same config to have a value or a `setting` to exist in the top-level plugin definition.

### Steps to reproduce
_How one can reproduce the issue?_

Add a config to a plugin _only_ in an Environment:

```shell
meltano --environment=dev config my-plugin set ""my_config"" ""value""
```

Meltano fails to recognize and expose this config in the runtime plugin environment.

### Relevant logs and/or screenshots
_Please use code blocks (\`\`\`) to format console output_

```shell
# This fails because 'SQLFLUFF_SF_USER' is referenced in my dbt but its not set
meltano invoke sqlfluff lint

# This fails because 'SQLFLUFF_SF_USER' is referenced in my dbt but its not set
meltano --environment=dev invoke sqlfluff lint

# This fails because 'SQLFLUFF_SF_USER' is referenced in my dbt but its not set
meltano --environment=prod invoke sqlfluff lint
```

### Possible fixes
_If you can, link to the line of code that might be responsible for the problem or suggest a fix_



### Further regression test
_Ensure we automatically catch similar issues in the future_

- [ ] Write additional adequate test cases and submit test results
- [ ] Test results should be reviewed by a person from the team


### Work Around Solutions

Either add an explicit setting to your plugin as shown in https://gitlab.com/meltano/meltano/-/issues/3299#note_859299619.

Or add the config key to the top level (not in an environment) plugin configuration, then Meltano recognizes it and reads the environment specific config properly. As described in https://gitlab.com/meltano/meltano/-/issues/3299#note_860273439.",8
103987084,2022-03-02 20:29:44.326,Create a plan to bring REST API to parity with CLI,"@pandemicsyn: based on a convo with @aaronsteers I'll focus on two topic areas:

1. What (if any) immediate deficits do we have in the API that should be address.
2. What would a v2 of the API look like, what decisions do we need to make, and how do we get there.",4
103973908,2022-03-02 15:55:11.870,Bug - API Install Error,I noticed in this slack thread https://meltano.slack.com/archives/C01TCRBBJD7/p1646191929223089 that looks like they had an issue installing via the API due to a typo https://gitlab.com/meltano/meltano/-/blob/master/src/meltano/api/controllers/plugins.py#L124.,4
103918583,2022-03-01 21:12:44.476,"Adapter specific versions of Postgres, Redshift, and Bigquery","Follows from #3105+ 


This issue is planned as a follow-up to the initial `dbt-snowflake` release.

To have parity, we likely need these three:

- `dbt-postgres`
- `dbt-redshift`
- `dbt-bigquery`

Possible:

- `dbt-spark[PyHive]
- `dbt-presto`

Full list of dbt Labs supported adapters https://docs.getdbt.com/docs/available-adapters",8
103849288,2022-02-28 18:11:47.803,"Formally remove support for ""Dashboards"", ""Explore"", and ""Models""","These three relate to BI-focused features where we (Meltano) are now suggesting open source 3rd party solutions. We can therefor deprecate these features and move faster towards BI/dashboard interop with less user confusion:

- Remove support ""Models"", which are BI metadata models in yaml.
- Remove support ""Dashboards"", which are Meltano-rendered BI reports.
- Remove support for ""Explore"", which is an interactive analysis/query experience.

Promoted alternatives:

- Lightdash (#2786) has a Yaml-based metadata model similar to our ""Models"" approach, but inline with dbt project definitions.
- Superset (#2605) is a popular open source BI platform, which rich dashboard support.
- mProve (#3224) is an open source reporting tool that is entirely text-based, meaning it works plays with DevOps, Git, CI/CD, and PR review processes.

Phased deprecation:

- In a prior Meltano release, we toggled the visibility of Dashboard/Explore features in Meltano UI, setting them to 'off' by default.",8
103074190,2022-02-25 22:21:55.794,"In adswerve/target-bigquery, state objects are merged","<!---
Please read this!

Before opening a new issue, make sure to search for keywords in the issues
filtered by the ""regression"" or ""bug"" label and verify the issue you're about to submit isn't a duplicate.

If you are submitting an issue with a tap, please include:

- account details
- target details
- entities selected  with meltano select (if you have selected any entities), as the bug may be related to a specific entity
- the full elt command you are running
- full output of the meltano elt command. Logs can get pretty long, so you can add the full log as a snippet in the Meltano project and add a link in the issue.
--->

### What is the current *bug* behavior?
_What is happening now?_

`target-bigquery`, default variant `adswerve` merges state objects, causing partition state created by the SDK to be transformed into a format it can not parse.


### What is the expected *correct* behavior?
_What should be happening?_

The target should not manipulate state.

### Steps to reproduce
_How one can reproduce the issue?_

Use a tap with partitioned state (e.g. `MeltanoLabs/tap-github` with `adswerve/target-bigquery` and notice the transformed state object.

### Relevant logs and/or screenshots
_Please use code blocks (\`\`\`) to format console output_

See: https://gitlab.com/meltano/sdk/-/issues/300+

### Possible fixes
_If you can, link to the line of code that might be responsible for the problem or suggest a fix_

Default the target's `merge_state_messages` setting to false.

### Further regression test
_Ensure we automatically catch similar issues in the future_

- [ ] Write additional adequate test cases and submit test results
- [ ] Test results should be reviewed by a person from the team",1
102959619,2022-02-23 23:01:08.045,Support `meltano add` directly from Hub,"(Probably) blocked by #3031. 

We'd like users to be able to add MeltanoHub plugins directly from the `meltano add` command.

Once this is working, the Hub can be updated with the install instructions on each detail page.

For instance, similar to the `choco install ...` CLI guidance on Chocolatey detail pages:

<details><summary>Click to expand</summary>

![image](/uploads/90016121dc45b9a0d726c8124a7e78ed/image.png)

</details>

... we'd have the text:

> Install with Meltano:
>
> ```console
> meltano add extractor tap-gitlab --variant=MeltanoLabs
> ```

... or with an http ref:

> Install with Meltano:
>
> ```console
> meltano add extractor tap-gitlab --from_ref=https://hub.meltano.com/singer/taps/tap-gitlab--meltanolabs.yml
> ```

We'll likely need to first build #3031+, since this is the issue which decouples us from `discovery.yml`. We could go in another order, but then the inference is that the entire plugin definition would need to live in their `meltano.yml` file, which is parity with `--custom` but doesn't give a similar experience versus `discovery.yml`.",8
102950448,2022-02-23 19:07:44.114,--debug-level=True help function didn't help a new Meltano user,"Was just debugging meltano with a new Meltano user. They hit an exception with tap-csv, had the standard ""failed while in discovery"" which pointing to logs (which they found), and then they tried to run with --log-level=DEBUG but it wasn't clear where to put --log-level=DEBUG so they did something like this

`meltano elt tap-csv target-jsonl --job-id=abc --log-level=DEBUG`

Easy fix is to probably just extend the message a bit to give an example of where to put --log-level? Easy MR to make but not sure if you'll agree!",1
102939378,2022-02-23 15:58:26.834,Bring down existing Snowplow Infra,"Since infra was successfully created for https://gitlab.com/meltano/meltano/-/issues/3016+, we should bring it down now that we're committed to https://gitlab.com/meltano/meltano/-/issues/3243+

cc @aaronsteers @tayloramurphy",1
102835472,2022-02-22 05:29:58.241,Flake8 --diff not working as expected from forks,"Following from the latest on community-contributed MR !2540, it seems the default branch targeting implemented in !2543 is not working as expected from a fork. Rather than compare `upstream/master` with `forkname/branchname`, the pipeline seems to be comparing `forkname/master` with `forkname/branchname`.

```
$ git fetch origin $CI_DEFAULT_BRANCH; # collapsed multi-line command
From https://gitlab.com/vischous/meltano
 * branch                master     -> FETCH_HEAD
Comparing to master
$ declare FILES=$(git diff --name-only ${CI_COMMIT_SHA} $REF | grep ""\.py$"")
$ if [ -n ""${FILES}"" ]; then # collapsed multi-line command
Changed files are\nsrc/meltano/__init__.py
src/meltano/api/app.py
src/meltano/api/controllers/orchestrations.py
src/meltano/api/security/forms.py
src/meltano/cli/__init__.py
src/meltano/cli/add.py
src/meltano/cli/cli.py
...
```

To resolve this, we probably need to somehow explicitly compare with `upstream/master` or a similar reference, and to do so in a way that also works on the main `meltano` repo, which would not have an `upstream` reference.

Cc @edgarrmondragon, @pandemicsyn",2
102823769,2022-02-21 22:31:34.822,Review all docs links in code to ensure they point to docs.meltano.com,Based on https://gitlab.com/meltano/meltano/-/merge_requests/2540,1
102818215,2022-02-21 19:09:14.377,"Freeze Gitlab docker images as stable, rather than continually updating","Blocks https://gitlab.com/meltano/meltano/-/merge_requests/2542

Before we can direct users to leverage our Gitlab-hosted docker images, we need to resolve a potential issue where `latest*` and `v1.xx.0*` images are apparently being constantly updated on every build to `master`.

Probably the solution is to have a more clear delineation between which images are ""dev/test"" images needed by later CI steps, and which images are ""published/stable"" images that can be relied upon by users with production workloads.

Notes:

- We can _optionally_ keep `latest` and `latest*` images linked to master CI builds, with the proper caveat/disclaimer to users that they are non-stable releases for the purposes of previewing unreleased features. (Not for production workloads.)
- Versioned images `*v1.xx.0*` should definitely not be updated continually with new `master` updates. The timestamp on those images reflects them being continually updated as of now.",4
102816963,2022-02-21 18:32:52.189,"Full check for broken links in Python text, especially links to docs.meltano.com","As per #3263, it seems we may have a number of undetected 404 errors and/or broken links - especially in Python code which doesn't have any kind of auto-404 detection. A one-time regex or code search audit is probably warranted, as suggested by Derek in the related MR. 

~""urgency::higher""",2
102631385,2022-02-17 15:47:55.356,Update the Getting Started guide in the docs now that environments are created for you,"Once !2526 ships, users will no longer need to create their own environments, as documented in the [Getting Started guide](https://docs.meltano.com/getting-started#add-an-environment). This should be updated to just `list` functions, to introduce users to the created environments.

Thanks @pnadolny13 for spotting this 👏",2
102463633,2022-02-15 09:06:57.843,Nested values in array settings don't have environment variables expanded,"Related to #2366 

### What is the current *bug* behavior?

I was having trouble using environment variables in the `meltano.yml` file, and it seems they are only supported for top-level configuration options.

If I define `SOME_ENV_VAR=SOME_VALUE` in the `.env` file and I write this configuration yaml:
```yaml
plugins:
  extractors:
    - name: tap-spreadsheets-anywhere
      variant: ets
      pip_url: git+https://github.com/ets/tap-spreadsheets-anywhere.git
      config:
        test: $SOME_ENV_VAR
        tables:
          - path: $SOME_ENV_VAR
            name: my_table
```

Then this is the actual configuration used by meltano:
```
$ meltano config tap-spreadsheets-anywhere
{
  ""tables"": [
    {
      ""path"": ""$SOME_ENV_VAR"",
      ""name"": ""my_table""
    }
  ],
  ""test"": ""SOME_VALUE""
}
```

And the `tables.path` variable is not set.



### What is the expected *correct* behavior?

The environment variable `SOME_ENV_VAR` should expend to `SOME_VALUE` in the `tables` array as well, and we should have:
```
$ meltano config tap-spreadsheets-anywhere
{
  ""tables"": [
    {
      ""path"": ""SOME_VALUE"",
      ""name"": ""my_table""
    }
  ],
  ""test"": ""SOME_VALUE""
}
```

### Steps to reproduce

- add the configuration above in `meltano.yml`
- add the line `SOME_ENV_VAR=SOME_VALUE` to your `.env` file
- see the result with `meltano config tap-spreadsheets-anywhere`


### Relevant logs and/or screenshots
_Please use code blocks (\`\`\`) to format console output_



### Possible fixes
_If you can, link to the line of code that might be responsible for the problem or suggest a fix_



### Further regression test
_Ensure we automatically catch similar issues in the future_

- [ ] Write additional adequate test cases and submit test results
- [ ] Test results should be reviewed by a person from the team",4
102292357,2022-02-11 19:38:48.785,Improve plugin command error message to clarify usage syntax,"Related to https://gitlab.com/meltano/meltano/-/issues/3237#note_840251651 where @kgpayne said:

>Due to our custom syntax for commands, using the executable name is going to be equally jarring when users get to executing `meltano invoke great_expectations:checkpoint`, which is not functionality of the `great_expectations` executable.

I had a recent experience that I could see tripping people up. I wanted to use `dbt docs generate` and `dbt docs serve` which are not supported as `dbt:x` syntax because its not listed in the [discovery.yml](https://gitlab.com/meltano/meltano/-/blob/master/src/meltano/core/bundle/discovery.yml#L3073). This does not work:

```
meltano invoke dbt:docs generate
```
So I was thinking about adding an extra command to the plugin config like `docs_generate` that runs `docs generate` under the hood but then I realized that this works:

```
meltano invoke dbt docs generate
```

I'm not sure this is a problem but with such a minor difference in syntax I could see it becoming confusing whether your running a meltano command or raw command directly to the package. On one hand its great to allow commands to pass through so all the functionality is preserved but on the other its not clear in what context the command is being run. Maybe just having a warning that a ""pass through"" command is being used would be helpful, idk but wanted to call it out.",2
102287699,2022-02-11 17:36:08.176,"Meltano UI tracking consent ""learn more"" link not found","The link to ""learn more"" in our tracking consent popup actually references a link that no longer exists. While it doesn't directly result in a 404 (we redirect to the docs), the link just takes you to the main page. We link to https://docs.meltano.com/reference/settings#send_anonymous_usage_stats

Ref: https://gitlab.com/meltano/meltano/-/blob/master/src/webapp/src/utils/setupToasted.js#L54",1
102233218,2022-02-10 21:04:24.292,`meltano test` erroring because unrelated Airflow isnt installed,"<!---
Please read this!

Before opening a new issue, make sure to search for keywords in the issues
filtered by the ""regression"" or ""bug"" label and verify the issue you're about to submit isn't a duplicate.

If you are submitting an issue with a tap, please include:

- account details
- target details
- entities selected  with meltano select (if you have selected any entities), as the bug may be related to a specific entity
- the full elt command you are running
- full output of the meltano elt command. Logs can get pretty long, so you can add the full log as a snippet in the Meltano project and add a link in the issue.
--->

### What is the current *bug* behavior?
_What is happening now?_

When running my great_expectation tests using the `meltano test` command I get an error saying that Airflow isn't installed:
```
Executable 'airflow' could not be found. Orchestrator 'airflow' may not have been installed yet using `meltano install orchestrator airflow`, or the executable name may be incorrect.
```

Its correct, Airflow is not installed which is on purpose because I'm not using it. It almost looks like its trying to run Airflow instead of great expectations. If I use `meltano invoke` instead of `meltano test` it works properly.

### What is the expected *correct* behavior?
_What should be happening?_

The test command shouldnt require anything thats not used.

### Steps to reproduce
_How one can reproduce the issue?_

meltano, version 1.94.0

The meltano.yml is in the [squared repo](https://gitlab.com/meltano/squared/-/blob/master/data/meltano.yml#L314).

```bash
meltano install utility great_expectations
meltano test great_expectations:test_ga_raw
```

### Relevant logs and/or screenshots
_Please use code blocks (\`\`\`) to format console output_

```
meltano --log-level=debug --environment=prod test great_expectations:test_ga_raw
2022-02-10T20:56:27.053642Z [info     ] Environment 'prod' is active
2022-02-10T20:56:27.118585Z [debug    ] Creating engine <meltano.core.project.Project object at 0x108105340>@sqlite:////Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/meltano.db
2022-02-10T20:56:30.498011Z [debug    ] Starting new HTTPS connection (1): www.meltano.com:443
2022-02-10T20:56:30.740590Z [debug    ] https://www.meltano.com:443 ""GET /discovery.yml HTTP/1.1"" 301 162
2022-02-10T20:56:30.746774Z [debug    ] Starting new HTTPS connection (1): meltano.com:443
2022-02-10T20:56:30.925250Z [debug    ] https://meltano.com:443 ""GET /discovery.yml HTTP/1.1"" 200 150378
2022-02-10T20:56:33.280922Z [debug    ] Using selector: KqueueSelector
2022-02-10T20:56:33.544037Z [debug    ] Created configuration at /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/run/tap-slack/tap.6f331ecc-78d8-457f-9cd7-da3407b40502.config.json
2022-02-10T20:56:33.544399Z [debug    ] Could not find tap.properties.json in /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/extractors/tap-slack/tap.properties.json, skipping.
2022-02-10T20:56:33.544767Z [debug    ] Could not find tap.properties.cache_key in /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/extractors/tap-slack/tap.properties.cache_key, skipping.
2022-02-10T20:56:33.545172Z [debug    ] Could not find state.json in /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/extractors/tap-slack/state.json, skipping.
2022-02-10T20:56:33.545688Z [debug    ] Deleted configuration at /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/run/tap-slack/tap.6f331ecc-78d8-457f-9cd7-da3407b40502.config.json
2022-02-10T20:56:33.687214Z [debug    ] Created configuration at /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/run/tap-google-analytics/tap.d8453ec6-bfd5-4e47-94fb-39ea5582f544.config.json
2022-02-10T20:56:33.687515Z [debug    ] Could not find tap.properties.json in /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/extractors/tap-google-analytics/tap.properties.json, skipping.
2022-02-10T20:56:33.687929Z [debug    ] Could not find tap.properties.cache_key in /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/extractors/tap-google-analytics/tap.properties.cache_key, skipping.
2022-02-10T20:56:33.688462Z [debug    ] Could not find state.json in /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/extractors/tap-google-analytics/state.json, skipping.
2022-02-10T20:56:33.688938Z [debug    ] Deleted configuration at /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/run/tap-google-analytics/tap.d8453ec6-bfd5-4e47-94fb-39ea5582f544.config.json
2022-02-10T20:56:33.903914Z [debug    ] Created configuration at /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/run/tap-athena/tap.445100d9-f2d1-4844-917d-9a9c532e746d.config.json
2022-02-10T20:56:33.904343Z [debug    ] Could not find tap.properties.json in /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/extractors/tap-athena/tap.properties.json, skipping.
2022-02-10T20:56:33.904788Z [debug    ] Could not find tap.properties.cache_key in /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/extractors/tap-athena/tap.properties.cache_key, skipping.
2022-02-10T20:56:33.905197Z [debug    ] Could not find state.json in /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/extractors/tap-athena/state.json, skipping.
2022-02-10T20:56:33.905749Z [debug    ] Deleted configuration at /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/run/tap-athena/tap.445100d9-f2d1-4844-917d-9a9c532e746d.config.json
2022-02-10T20:56:34.604627Z [debug    ] Created configuration at /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/run/tap-athena-metrics/tap.445100d9-f2d1-4844-917d-9a9c532e746d.config.json
2022-02-10T20:56:34.604922Z [debug    ] Could not find tap.properties.json in /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/extractors/tap-athena-metrics/tap.properties.json, skipping.
2022-02-10T20:56:34.605316Z [debug    ] Could not find tap.properties.cache_key in /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/extractors/tap-athena-metrics/tap.properties.cache_key, skipping.
2022-02-10T20:56:34.605739Z [debug    ] Could not find state.json in /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/extractors/tap-athena-metrics/state.json, skipping.
2022-02-10T20:56:34.606120Z [debug    ] Deleted configuration at /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/run/tap-athena-metrics/tap.445100d9-f2d1-4844-917d-9a9c532e746d.config.json
2022-02-10T20:56:35.287970Z [debug    ] Created configuration at /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/run/tap-athena-audit/tap.445100d9-f2d1-4844-917d-9a9c532e746d.config.json
2022-02-10T20:56:35.288635Z [debug    ] Could not find tap.properties.json in /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/extractors/tap-athena-audit/tap.properties.json, skipping.
2022-02-10T20:56:35.289147Z [debug    ] Could not find tap.properties.cache_key in /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/extractors/tap-athena-audit/tap.properties.cache_key, skipping.
2022-02-10T20:56:35.289577Z [debug    ] Could not find state.json in /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/extractors/tap-athena-audit/state.json, skipping.
2022-02-10T20:56:35.290027Z [debug    ] Deleted configuration at /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/run/tap-athena-audit/tap.445100d9-f2d1-4844-917d-9a9c532e746d.config.json
2022-02-10T20:56:35.538957Z [debug    ] Created configuration at /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/run/tap-gitlab/tap.ee5fe64e-82ba-46f6-91bf-4702651b3f56.config.json
2022-02-10T20:56:35.539424Z [debug    ] Could not find tap.properties.json in /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/extractors/tap-gitlab/tap.properties.json, skipping.
2022-02-10T20:56:35.539939Z [debug    ] Could not find tap.properties.cache_key in /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/extractors/tap-gitlab/tap.properties.cache_key, skipping.
2022-02-10T20:56:35.540434Z [debug    ] Could not find state.json in /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/extractors/tap-gitlab/state.json, skipping.
2022-02-10T20:56:35.540914Z [debug    ] Deleted configuration at /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/run/tap-gitlab/tap.ee5fe64e-82ba-46f6-91bf-4702651b3f56.config.json
2022-02-10T20:56:35.728993Z [debug    ] Created configuration at /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/run/tap-github/tap.967c975f-5d17-4b2f-8ebb-1e190f09dd51.config.json
2022-02-10T20:56:35.729744Z [debug    ] Could not find tap.properties.json in /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/extractors/tap-github/tap.properties.json, skipping.
2022-02-10T20:56:35.730428Z [debug    ] Could not find tap.properties.cache_key in /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/extractors/tap-github/tap.properties.cache_key, skipping.
2022-02-10T20:56:35.730804Z [debug    ] Could not find state.json in /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/extractors/tap-github/state.json, skipping.
2022-02-10T20:56:35.731299Z [debug    ] Deleted configuration at /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/run/tap-github/tap.967c975f-5d17-4b2f-8ebb-1e190f09dd51.config.json
2022-02-10T20:56:35.814594Z [debug    ] Variable '$MELTANO_LOAD_SCHEMA' is missing from the environment.
2022-02-10T20:56:35.872057Z [debug    ] Variable '$MELTANO_LOAD_SCHEMA' is missing from the environment.
2022-02-10T20:56:35.872957Z [debug    ] Created configuration at /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/run/target-athena/target.c7c90a13-b895-4dbe-a230-a92621dfce59.config.json
2022-02-10T20:56:35.873424Z [debug    ] Deleted configuration at /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/run/target-athena/target.c7c90a13-b895-4dbe-a230-a92621dfce59.config.json
2022-02-10T20:56:36.185910Z [debug    ] Variable '$MELTANO_LOAD_SCHEMA' is missing from the environment.
2022-02-10T20:56:36.303360Z [debug    ] Variable '$MELTANO_LOAD_SCHEMA' is missing from the environment.
2022-02-10T20:56:36.304557Z [debug    ] Created configuration at /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/run/target-athena-slack/target.c7c90a13-b895-4dbe-a230-a92621dfce59.config.json
2022-02-10T20:56:36.304999Z [debug    ] Deleted configuration at /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/run/target-athena-slack/target.c7c90a13-b895-4dbe-a230-a92621dfce59.config.json
2022-02-10T20:56:36.649319Z [debug    ] Variable '$MELTANO_LOAD_SCHEMA' is missing from the environment.
2022-02-10T20:56:36.801696Z [debug    ] Variable '$MELTANO_LOAD_SCHEMA' is missing from the environment.
2022-02-10T20:56:36.803103Z [debug    ] Created configuration at /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/run/target-athena-gitlab/target.c7c90a13-b895-4dbe-a230-a92621dfce59.config.json
2022-02-10T20:56:36.803507Z [debug    ] Deleted configuration at /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/run/target-athena-gitlab/target.c7c90a13-b895-4dbe-a230-a92621dfce59.config.json
2022-02-10T20:56:37.132393Z [debug    ] Variable '$MELTANO_LOAD_SCHEMA' is missing from the environment.
2022-02-10T20:56:37.255502Z [debug    ] Variable '$MELTANO_LOAD_SCHEMA' is missing from the environment.
2022-02-10T20:56:37.256656Z [debug    ] Created configuration at /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/run/target-athena-github/target.c7c90a13-b895-4dbe-a230-a92621dfce59.config.json
2022-02-10T20:56:37.257158Z [debug    ] Deleted configuration at /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/run/target-athena-github/target.c7c90a13-b895-4dbe-a230-a92621dfce59.config.json
2022-02-10T20:56:37.305616Z [debug    ] Variable '$MELTANO_LOAD_SCHEMA' is missing from the environment.
2022-02-10T20:56:37.339673Z [debug    ] Variable '$MELTANO_LOAD_SCHEMA' is missing from the environment.
2022-02-10T20:56:37.340456Z [debug    ] Created configuration at /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/run/target-yaml/target.5fd05845-2e24-4ca1-9150-60a62a033fd7.config.json
2022-02-10T20:56:37.340977Z [debug    ] Deleted configuration at /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/run/target-yaml/target.5fd05845-2e24-4ca1-9150-60a62a033fd7.config.json
2022-02-10T20:56:37.563369Z [debug    ] Variable '$MELTANO_LOAD_SCHEMA' is missing from the environment.
2022-02-10T20:56:37.635584Z [debug    ] Variable '$MELTANO_LOAD_SCHEMA' is missing from the environment.
2022-02-10T20:56:37.636865Z [debug    ] Created configuration at /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/run/target-yaml-metrics/target.5fd05845-2e24-4ca1-9150-60a62a033fd7.config.json
2022-02-10T20:56:37.637378Z [debug    ] Deleted configuration at /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/run/target-yaml-metrics/target.5fd05845-2e24-4ca1-9150-60a62a033fd7.config.json
2022-02-10T20:56:37.855876Z [debug    ] Variable '$MELTANO_LOAD_SCHEMA' is missing from the environment.
2022-02-10T20:56:37.927241Z [debug    ] Variable '$MELTANO_LOAD_SCHEMA' is missing from the environment.
2022-02-10T20:56:37.928092Z [debug    ] Created configuration at /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/run/target-yaml-audit/target.5fd05845-2e24-4ca1-9150-60a62a033fd7.config.json
2022-02-10T20:56:37.928589Z [debug    ] Deleted configuration at /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/run/target-yaml-audit/target.5fd05845-2e24-4ca1-9150-60a62a033fd7.config.json
2022-02-10T20:56:37.963193Z [debug    ] Invoking: ['/Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/orchestrators/airflow/venv/bin/airflow', '--help']
2022-02-10T20:56:37.963398Z [debug    ] Env: {""REMOVED""}
2022-02-10T20:56:37.972430Z [debug    ] Deleted configuration at /Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/run/airflow/airflow.cfg
2022-02-10T20:56:37.974237Z [debug    ] Executable 'airflow' could not be found. Orchestrator 'airflow' may not have been installed yet using `meltano install orchestrator airflow`, or the executable name may be incorrect. 
Traceback (most recent call last):
  File ""/Users/pnadolny/.virtualenvs/squred/lib/python3.8/site-packages/meltano/core/plugin_invoker.py"", line 270, in _invoke
    yield (popen_args, popen_options, popen_env)
  File ""/Users/pnadolny/.virtualenvs/squred/lib/python3.8/site-packages/meltano/core/plugin_invoker.py"", line 282, in invoke_async
    return await asyncio.create_subprocess_exec(
  File ""/Users/pnadolny/.pyenv/versions/3.8.12/lib/python3.8/asyncio/subprocess.py"", line 236, in create_subprocess_exec
    transport, protocol = await loop.subprocess_exec(
  File ""/Users/pnadolny/.pyenv/versions/3.8.12/lib/python3.8/asyncio/base_events.py"", line 1630, in subprocess_exec
    transport = await self._make_subprocess_transport(
  File ""/Users/pnadolny/.pyenv/versions/3.8.12/lib/python3.8/asyncio/unix_events.py"", line 197, in _make_subprocess_transport
    transp = _UnixSubprocessTransport(self, protocol, args, shell,
  File ""/Users/pnadolny/.pyenv/versions/3.8.12/lib/python3.8/asyncio/base_subprocess.py"", line 36, in __init__
    self._start(args=args, shell=shell, stdin=stdin, stdout=stdout,
  File ""/Users/pnadolny/.pyenv/versions/3.8.12/lib/python3.8/asyncio/unix_events.py"", line 789, in _start
    self._proc = subprocess.Popen(
  File ""/Users/pnadolny/.pyenv/versions/3.8.12/lib/python3.8/subprocess.py"", line 858, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File ""/Users/pnadolny/.pyenv/versions/3.8.12/lib/python3.8/subprocess.py"", line 1704, in _execute_child
    raise child_exception_type(errno_num, err_msg, err_filename)
FileNotFoundError: [Errno 2] No such file or directory: '/Users/pnadolny/Documents/Git/GitLab/squared/data/.meltano/orchestrators/airflow/venv/bin/airflow'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/Users/pnadolny/.virtualenvs/squred/lib/python3.8/site-packages/meltano/cli/__init__.py"", line 47, in main
    cli(obj={""project"": None})
  File ""/Users/pnadolny/.virtualenvs/squred/lib/python3.8/site-packages/click/core.py"", line 829, in __call__
    return self.main(*args, **kwargs)
  File ""/Users/pnadolny/.virtualenvs/squred/lib/python3.8/site-packages/click/core.py"", line 782, in main
    rv = self.invoke(ctx)
  File ""/Users/pnadolny/.virtualenvs/squred/lib/python3.8/site-packages/click/core.py"", line 1259, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/Users/pnadolny/.virtualenvs/squred/lib/python3.8/site-packages/click/core.py"", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/Users/pnadolny/.virtualenvs/squred/lib/python3.8/site-packages/click/core.py"", line 610, in invoke
    return callback(*args, **kwargs)
  File ""/Users/pnadolny/.virtualenvs/squred/lib/python3.8/site-packages/meltano/cli/params.py"", line 23, in decorate
    return func(*args, **kwargs)
  File ""/Users/pnadolny/.virtualenvs/squred/lib/python3.8/site-packages/meltano/cli/params.py"", line 56, in decorate
    func(project, *args, **kwargs)
  File ""/Users/pnadolny/.virtualenvs/squred/lib/python3.8/site-packages/meltano/cli/validate.py"", line 101, in test
    exit_codes = run_async(_run_plugin_tests(session, collected.values()))
  File ""/Users/pnadolny/.virtualenvs/squred/lib/python3.8/site-packages/meltano/core/utils/__init__.py"", line 50, in run_async
    loop.run_until_complete(future)
  File ""/Users/pnadolny/.pyenv/versions/3.8.12/lib/python3.8/asyncio/base_events.py"", line 616, in run_until_complete
    return future.result()
  File ""/Users/pnadolny/.virtualenvs/squred/lib/python3.8/site-packages/meltano/cli/validate.py"", line 117, in _run_plugin_tests
    return {runner.plugin_name: await runner.run_all(session) for runner in runners}
  File ""/Users/pnadolny/.virtualenvs/squred/lib/python3.8/site-packages/meltano/cli/validate.py"", line 117, in <dictcomp>
    return {runner.plugin_name: await runner.run_all(session) for runner in runners}
  File ""/Users/pnadolny/.virtualenvs/squred/lib/python3.8/site-packages/meltano/core/validation_service.py"", line 82, in run_all
    async with self.invoker.prepared(session):
  File ""/Users/pnadolny/.virtualenvs/squred/lib/python3.8/site-packages/async_generator/_util.py"", line 34, in __aenter__
    return await self._agen.asend(None)
  File ""/Users/pnadolny/.virtualenvs/squred/lib/python3.8/site-packages/meltano/core/plugin_invoker.py"", line 176, in prepared
    await self.prepare(session)
  File ""/Users/pnadolny/.virtualenvs/squred/lib/python3.8/site-packages/meltano/core/plugin_invoker.py"", line 158, in prepare
    async with self.plugin.trigger_hooks(""configure"", self, session):
  File ""/Users/pnadolny/.virtualenvs/squred/lib/python3.8/site-packages/async_generator/_util.py"", line 34, in __aenter__
    return await self._agen.asend(None)
  File ""/Users/pnadolny/.virtualenvs/squred/lib/python3.8/site-packages/meltano/core/behavior/hookable.py"", line 87, in trigger_hooks
    await self.__class__.trigger(self, f""before_{hook_name}"", *args, **kwargs)
  File ""/Users/pnadolny/.virtualenvs/squred/lib/python3.8/site-packages/meltano/core/behavior/hookable.py"", line 115, in trigger
    raise err
  File ""/Users/pnadolny/.virtualenvs/squred/lib/python3.8/site-packages/meltano/core/behavior/hookable.py"", line 107, in trigger
    await hook_func(target, *args, **kwargs)
  File ""/Users/pnadolny/.virtualenvs/squred/lib/python3.8/site-packages/meltano/core/plugin/airflow.py"", line 72, in before_configure
    handle = await invoker.invoke_async(
  File ""/Users/pnadolny/.virtualenvs/squred/lib/python3.8/site-packages/meltano/core/plugin_invoker.py"", line 282, in invoke_async
    return await asyncio.create_subprocess_exec(
  File ""/Users/pnadolny/.virtualenvs/squred/lib/python3.8/site-packages/async_generator/_util.py"", line 53, in __aexit__
    await self._agen.athrow(type, value, traceback)
  File ""/Users/pnadolny/.virtualenvs/squred/lib/python3.8/site-packages/meltano/core/plugin_invoker.py"", line 272, in _invoke
    raise ExecutableNotFoundError(
meltano.core.plugin_invoker.ExecutableNotFoundError: Executable 'airflow' could not be found. Orchestrator 'airflow' may not have been installed yet using `meltano install orchestrator airflow`, or the executable name may be incorrect.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/Users/pnadolny/.virtualenvs/squred/lib/python3.8/site-packages/meltano/cli/__init__.py"", line 55, in main
    raise CliError(str(err)) from err
meltano.cli.utils.CliError: Executable 'airflow' could not be found. Orchestrator 'airflow' may not have been installed yet using `meltano install orchestrator airflow`, or the executable name may be incorrect.
Executable 'airflow' could not be found. Orchestrator 'airflow' may not have been installed yet using `meltano install orchestrator airflow`, or the executable name may be incorrect.
```


### Possible fixes
_If you can, link to the line of code that might be responsible for the problem or suggest a fix_



### Further regression test
_Ensure we automatically catch similar issues in the future_

- [ ] Write additional adequate test cases and submit test results
- [ ] Test results should be reviewed by a person from the team",4
102170275,2022-02-09 21:01:33.149,Add `--interactive --all` CLI flags in `meltano config set`,"Based on recent discussions around the difficulties in knowing _which_ settings to configure when adding a new plugin, we should add an interactive mode for setting all setting for a plugin, or any specific settings value.

Something like...

```console
> meltano config dbt-snowflake set --interactive --all

You are now configuring the ""dbt-snowflake"" transformer plugin.
For help, please refer to plugin documentation: https://...

We will now step through each setting. Leave any entry blank to skip, 
or enter a new value. Password values will automatically be hidden 
from display for any settings of type 'password'.

Press Ctrl+C to cancel at any time.

Setting #1: project_dir
Description: The root dbt path.
Type: string
Default value: $MELTANO_PROJECT_ROOT/transform
Current value: (none)
New Value: _ 

(skipped)

....

Setting #7: snowflake_user
Description: The username to use in Snowflake authentication.
Type: string
Default value: (none)
Current value: (none)
New Value: snowflake_prod_user

Value saved successfully to meltano.yml!

Setting #8: snowflake_password
Description: The password to use in Snowflake authentication.
Type: password
Default value: $MELTANO_PROJECT_ROOT/transform
Current value: (none)
New Value: ********** 

Value saved successfully to Meltano System DB!

```

## Spec Details

- Interactive behavior is triggered bia the `--interactive` flag in `meltano config ... set`.
- Interactive behavior can be triggered for a single setting or with `--all` to set all settings associated with the given plugin.
- The `--all` option _requires_ the `--interactive` flag. If `--all` is specified without `--interactive`, Meltano should fail.
- After each setting is set, Meltano should respond with `Value saved successfully to _<store>_!`
  - The store could be `.env`, the SystemDB, or to Meltano.yml.
  - Confirming and saving after each setting is set gives real-time feedback to the user about changes they have applied, and allows the remaining operation to be canceled without loss of any setting values already saved.
- Individual settings should be skippable.
- Password fields should either print asterisks or simply not echo anything while users are typing.
- The new setting value should not be provided inline at the command line if `--interactive` is set. So, passing the setting value and also passing `--interactive` should cause Meltano to fail.

## Ability to ""skip"" settings

Skipping a setting may not be intuitive if done inline with the setting value input.

```
Setting #1: project_dir
Description: The root dbt path.
Type: string
Default value: $MELTANO_PROJECT_ROOT/transform
Current value: (none)
Would you like to set a new value? [Y/n]: n

Skipped.

Setting #2: snowflake_user
Description: The username to use in Snowflake authentication.
Type: string
Default value: (none)
Current value: (none)
Would you like to set a new value? [Y/n]: Y
New Value? []: snowflake_prod_user

Value saved successfully to meltano.yml!

...
```

Without a separate prompt to skip or set the value, the user would generally have to type something like an empty string or `s` for skip, or similar. However, those are potentially valid answers to the new value prompt.

## Phases

- The initial merge should at minimum provide `--all --interactive` support.
- Optionally, setting only one value (`--interactive` without `--all`) can be included in initial scope or added in a subsequent iteration.
  - When setting only a single value, presumably we do not need to allow the ""skip"" option described above, since the user can simply abort (`ctrl+c`) to cancel if they do not want the value set.
- Not required in this first iteration, but a future iteration should require users to first define a setting's type if the type is not yet set. Would be a path for resolving #2768+. For example, if the user runs `meltano config tap-snowflake set --interactive snowflake_password`, that should require the user to _first_ define the setting `kind` (kind is `password` in this case), which then drives correct parsing and storage of the provided value.",8
102165182,2022-02-09 18:41:53.274,Support `--store=keyring` in `meltano config`,"As proposed by Derek [in Slack](https://meltano.slack.com/archives/CFG3C3D1Q/p1644431033442989), we could use the [keyring](https://pypi.org/project/keyring/) Python library to store secrets in native OS-level keyrings.

Would add this option alongside others listed in https://docs.meltano.com/reference/command-line-interface#config:

```console
meltano config <plugin> set --store=keyring <name> <value>
```

![image](/uploads/5b404fa0765f20fe78b80c2a5c6daf79/image.png)",4
102018671,2022-02-07 19:16:19.025,Perform API endpoint inventory/discovery,"The meltano web API is a bit of an unknown/dark territory for a lot of us. There are quite a few API endpoints that the UI leverages, but they're really not documented at all. There are no API docs, sample requests/responses, expected status codes, etc. We should run a research spike and inventory what API endpoints are available to us so we gain a better understanding of the current state of the API is today.

Objectives for this research spike would be to produce:

- [ ] List of API endpoints/paths roughly documented in this issue for folks to review.
- [ ] Basic summary of what the endpoints do or support (producing a small docstring update MR to add basic method descriptions might be nice).
- [ ] Document any obvious API deficits, or quick wins you may come across.
- [ ] Document any large unknowns in this issue. 
- [ ] Document any other ""at first glance"" or ""next steps"" recommendations that might arise.
- [ ] Recommendation for a path forward for auto-generating API docs.

Note from AJ:

Given the short turnaround for our meeting on the 16th (See @tayloramurphy's comment in thread), this probably should to be timeboxed as a `2` (max a few hours investment) for the first iteration.",2
101916274,2022-02-04 22:27:42.503,[Contributed MR]: Allow user to configure Sendgrid asm_group_id,"Attached Community-Contributed MR: !2520+

<!---
Please read this!

Before opening a new issue, make sure to search for keywords in the issues
filtered by the ""regression"" or ""bug"" label and verify the issue you're about to submit isn't a duplicate.

If you are submitting an issue with a tap, please include:

- account details
- target details
- entities selected  with meltano select (if you have selected any entities), as the bug may be related to a specific entity
- the full elt command you are running
- full output of the meltano elt command. Logs can get pretty long, so you can add the full log as a snippet in the Meltano project and add a link in the issue.
--->

### What is the current *bug* behavior?
_What is happening now?_



### What is the expected *correct* behavior?
_What should be happening?_



### Steps to reproduce
_How one can reproduce the issue?_
Meltano the org appears to have hardcoded their own Sendgrid account's unsubscribe group, which causes header errors when using Subscriptions with a different Sendgrid account


### Relevant logs and/or screenshots
https://gitlab.com/meltano/meltano/-/blob/master/src/meltano/api/mail.py#L14

### Possible fixes
Allow the use of an environmental variable like `mail_unsubscribe_group`, `asm_group_id` (sendgrid's name for it), `sendgrid_group_id`, etc.



### Further regression test
_Ensure we automatically catch similar issues in the future_

- [ ] Write additional adequate test cases and submit test results
- [ ] Test results should be reviewed by a person from the team",2
101905837,2022-02-04 18:38:30.274,Release v1.95.0,"[//]: # (NOTE: This Release template is for Admin-Use only. If you've reached this template in error, please select another template from the list.)

## Evergreen Releases - Prep Steps:

An `Evergreen` release process means we are _always_ releasing. We open a new release ticket as soon as we've completed the prior release. (It's therefore the final step in this checklist.)

## ""Evergreen Prep"" Checklist

- [x] Open this Issue
- [x] Indicate the version to be released here in the issue's title `Release vX.Y.Z`
    - If the release number changes (from minor to major or patch, for instance), update the version here and in the issue description.

### Readiness Checklist:

`Engineering` team, to get ready for the upcoming release:

1. [x] Ensure any [already-merged commits](https://gitlab.com/meltano/meltano/-/commits/master) since the last release have [Changelog](https://gitlab.com/meltano/meltano/-/blob/master/CHANGELOG.md) entries (excepting non-user-impacting commits, such as docs fixes).
2. [x] Create a comment in the `#engineering-team` slack channel with pending-but-not-merged MRs, potentially shipping. (Aka, the ""burndown"" list.)
    - Otherwise a comment that all known merge candidates are already merged.
3. [x] Create or link to a summary of MRs merged and/or expected in the `#marketing` Slack channel, with an `@channel` mention.

### Release Checklist

Rotating `assignee`, on the morning of the release:

1. [x] Changelog updates and version bump:
    1. [x] Check the [pending MRs](https://gitlab.com/meltano/meltano/-/merge_requests?sort=updated_desc) to make sure nothing expected is still waiting to be merged.
    2. [x] Create a [new branch](https://gitlab.com/meltano/meltano/-/branches/new) named `release/vX.Y.Z` and a corresponding MR with the `Release` MR template.
    3. An automated pipeline (linked to the branch prefix `release/v*`) will
    immediately and automatically bump the version and flush the changelog.
        - [x] Check this box to confirm the automated changelog flush and version bump are correct.
        - You _do not_ need to wait for the CI pipeline. (An identical CI pipeline is already included in the below.)
2. [x] [Cut a release tag](https://gitlab.com/meltano/meltano/-/tags/new) from your `release/vX.Y.Z` branch _(not from `main`)_ named `vX.Y.Z` with Message=`Release vX.Y.Z`
    1. In response to new tag creation, these steps are performed automatically in Gitlab pipelines:
        1. Abort if tag `vX.Y.Z` does not match output from `poetry version --short`
        2. Test _everything_.
        3. Publish to PyPi <!-- Meltano-only: and Docker -->.
    2. Validate publish once the pipeline finishes. (While the process is running, you can continue with next steps, such as changelog grooming.)
        1. [x] Check this box when the tag's [pipeline](https://gitlab.com/meltano/meltano/-/pipelines) has completed (eta 40-60 minutes).
        2. [x] Check this box when [PyPi publish](https://pypi.org/project/meltano/#history) is confirmed.
        <!-- Meltano-only: 5. [ ] Check this box when [Docker publish]() is confirmed. -->
3. Groom the changelog:
    1. [x] Compare the [Changelog](https://gitlab.com/meltano/meltano/-/blob/master/CHANGELOG.md) against the `main` branch [commit history](https://gitlab.com/meltano/meltano/-/commits/master) and add any significant user-impacting updates (excluding docs and website updates, for instance).
    3. [x] Review the Changelog for readability and typoes, committing fixes or updates if needed.
    2. [ ] Final changelog review:
        - Open the Changelog in preview mode, mouse over each link and ensure tooltip descriptions match the resolved issue.
        - Check contributor profile links to make sure they are correct.
    3. [x] Merge the resulting MR to `main` with the merge commit message `Release vX.Y.Z`
    4. [x] [Open the next `Release` issue](https://gitlab.com/meltano/meltano/-/issues/new?issue) using the `Release` template.

### Announcements, Marketing, and Promotion

`Marketing` or `Product` team:

1. [ ] Post-release announcement steps:
    1. [x] Post announcement to Meltano slack: `#announcements`
    <!-- SDK only: 2. [ ] Cross-post (share) to `#sdk` -->
    3. Copy-paste to:
       - [x] `Singer` slack: `#meltano` <!-- SDK only: `#singer-sdk` -->
       - [x] `dbt` slack: `#tools-meltano`
    4. [ ] Blog post
    5. [ ] Tweet the blog post

----------------",2
101808386,2022-02-03 12:04:44.695,Resolve ambiguity in current supported ways to set environment variables inside the plugin env via meltano.yml,"# Meltano Configuration

Configuration management is a central part of the value provided to users of Meltano. Intention:

> Meltano is responsible for managing the configuration of all of a project's plugins. It knows what settings are supported by each plugin, and how and when different types of plugins expect to be fed that configuration.

## Use Cases

### Discovery

Discovery is how Meltano ""knows what settings are supported by each plugin, and how and when different types of plugins expect to be fed that configuration"". This is done by:

- Defining the settings available/expected to be configured for a given plugin.
- Defining the environment variables available/expected to be configured for a given plugin via the `env` settings [key](https://gitlab.com/meltano/meltano/-/blob/master/schema/discovery.schema.json#L653).
- Defining default values for settings via the `value` [key](https://gitlab.com/meltano/meltano/-/blob/master/schema/discovery.schema.json#L598). Note, default values can contain templated string references to environment variables (e.g. `$MELTANO_PROJECT_ROOT/utilities/great_expectations`).
	- Which environment is `MELTANO_PROJECT_ROOT` expected to be in? The Meltano runtime or the plugin runtime? 🤯 
- Defining env var maps to predictable names via the `env_aliases` [key](https://gitlab.com/meltano/meltano/-/blob/master/schema/discovery.schema.json#L661) for reuse across types of plugins (now marked as deprecated 🤦‍♂️). (e.g. `TAP_POSTGRESS_HOST` > `PG_ADDRESS`).
- Defining settings aliases via the `aliases` [key](https://gitlab.com/meltano/meltano/-/blob/master/schema/discovery.schema.json#L581) as alternative names by which the setting can be configured in `meltano.yml` and the CLI. This is often done to 'prettify' or make uniform with other plugin variants (e.g. `postgres_host` > `host`). **Note:* Meltano does not currently look for env vars of the form `<plugin name>_<alias name>` by default (though sunch env vars can be specified as `env_aliases` manually), nor does it appear to check or warn of clashes as the result of settings referenced in multiple ways (by its `name`, `aliases` or `env_aliases`).

In addition to the behaviours implemented as part of discovery, Meltano has 2 other builtin mechanisms for handling/discovering settings:

- [Pipeline environment variables](https://docs.meltano.com/guide/integration#pipeline-environment-variables) define settings that are passed from the environment of one plugin to the next in a pipeline (e.g. `extractor>loader>transformer`).
- [Plugin extras](https://docs.meltano.com/guide/configuration#plugin-extras) define settings used to configure the behaviour of specific plugin types _before_ and _after_ 3rd party code is invoked. This works by passing `plugin_extra` settings for use by all subclasses of [`BasePlugin`](https://gitlab.com/meltano/meltano/-/blob/master/src/meltano/core/plugin/base.py#L246) (e.g. extras available to the [`SingerTap`](https://gitlab.com/meltano/meltano/-/blob/master/src/meltano/core/plugin/singer/tap.py#L104) allow users to [override](https://gitlab.com/meltano/meltano/-/blob/master/src/meltano/core/plugin/singer/tap.py#L302) the `catalog.json` path used during Tap invocation).

### Configuration

Configuration is how end-users populate discovered settings with their own values. Config is done by:

- Using the `meltano config ...` CLI and `meltano.yml` to configure the above settings by name.
- Using environment variables in the Meltano runtime to configure the above settings via a `<PLUGIN NAME>_<SETTING_NAME>` env var naming convention. Environment variables can either be `export`'d into the Meltano runtime or coded in a `.env` file automatically discovered by Meltano.
-  Using values stored in the Meltano [system database](https://docs.meltano.com/concepts/project#system-database) to configure settings by name (also available through the UI).
-  Using the `env:` key of the [environments](https://docs.meltano.com/concepts/environments) feature to inject arbitrary environment variables into a plugins environment. These env vars will also be passed to subsequent stages in a pipeline according to the [pipeline environment variables](https://docs.meltano.com/guide/integration#pipeline-environment-variables) feature.
	- How are duplicates/clashes handled?
-  Setting env vars in the Meltano runtime environment matching names specified in setting `env:` and `env_aliases:` discovery keys (that do not have to conform to the `<PLUGIN NAME>_<SETTING_NAME>` conventions) to inject values into the plugins environment. **Note:** this is now an undesired behaviour, as per [#2984](https://gitlab.com/meltano/meltano/-/issues/2984).
-  Referencing provided [pipeline environment variables](https://docs.meltano.com/guide/integration#pipeline-environment-variables) in plugin tools code (e.g. the [provided profiles.yml](https://gitlab.com/meltano/files-dbt/-/blob/master/bundle/transform/profile/profiles.yml))
-  Using `plugin_extras` to configure the pre and post invocation behaviours of plugins.
-  Mapping a setting to an environment variable by adding a `my_setting: ${MY_ENV_VAR}` mapping under the `config:` block in `meltano.yml` or via the CLI using `meltano config <plugin> set my_setting ""${MY_ENV_VAR}""`.
	- Which environment is `MY_ENV_VAR` expected to be in? The Meltano runtime or the plugin runtime? 🤯 

## FAQ's

- What is the difference between `config` and `settings`?

	Settings are specified as part of plugin definitions for the purposes of discovery. Config is the mechanism by which the fields described as settings are populated with values.

	Whilst `settings` usually only appear in `discovery.yml`, custom plugins added in `meltano.yml` places `settings` and `config` in the same view, which is completely baffling.

- How does `config` specified in `meltano.yml` arrive in files inside a plugins runtime? e.g. Singers' `config.json` or dbts' `profiles.yml`?

	For Singer the `SingerPlugin` class `src/meltano/core/plugin/singer/base.py` has a method (decorated as a `before_configure` Hook) for [writing a valid config.json file]() and another for [removing the file after](). Config arrives via an instance of the `PluginSettingsService` instantiated with both project and plugin.
	
	For dbt (specifically `profiles.yml` the approach is slightly different. dbt supports the retrieval of env vars within its config files (using jinja templating syntax). Therefore we need only match the env vars specified in the [provided profiles.yml](https://gitlab.com/meltano/files-dbt/-/blob/master/bundle/transform/profile/profiles.yml) to those available inside the Transformers environment. To smooth over this injection of env vars, Meltano makes available all the env vars provided to the Loader available to the Transformer as well, as part of the [pipeline environment feature](https://docs.meltano.com/guide/integration#pipeline-environment-variables). Currently we prescribe env vars of the form `PG_*` in the file bundle, but any matching env var name could be used (e.g. `TARGET_POSTGRES_*` to match the Loader env var convention).

- How do we determine what `settings`, specified in discovery, should be environment variables vs. which should be injected into files?

	This is implemented using the `env:` and `env_aliases:` keys in `discovery.yml`. Details as per Douwes comment [here]:
	
	> The behaviour around `env` and `env_aliases` is implemented in [`SettingsService.setting_env_vars`](https://gitlab.com/meltano/meltano/-/blob/master/src/meltano/core/plugin/settings_service.py#L63), which takes a `for_writing` boolean (which is `False` by default). Writing here means writing into the plugin's execution env, not-writing means reading from Meltano's execution env to populate a setting (that may then be written into the plugin execution env later). That `for_writing` flag is passed to [`SettingDefinition.env_vars`](https://gitlab.com/meltano/meltano/-/blob/master/src/meltano/core/setting_definition.py#L174) as `include_custom`. Following the logic there shows that when reading from the env, only the auto generated `<prefix>_<name>` is checked, and when writing into the env, env and env_aliases are used as well. I think the logic is that we want Meltano users to always use `<prefix>_<name>` consistently, but the plugin itself may expect a different env var.
	
	This description does not appear to explain the case described by [#2984](https://gitlab.com/meltano/meltano/-/issues/2984), which sparked our efforts to deprecate `env:` and `env_aliases:` in the first place 🤷‍♂️

## Proposed Next Steps

- To resolve  [#2984](https://gitlab.com/meltano/meltano/-/issues/2984) I believe we should remove the ability of the `SettingsService` to retrieve for injection any env vars _other than_ those matching the `<plugin name>_<setting name>` convention from the Meltano runtime environment. This meets Douwes intent that:

	> we want Meltano users to always use `<prefix>_<name>` consistently, but the plugin itself may expect a different env var.""

- Implement a check to determine if any arbitrary env vars configured in `environments.[].env` do not clash with `env:` and `env_aliases:` defined in discovery. Warn or error if clashes are found.

- Provide guidance/docs on the usage on `env:` and `env_aliases:` keys in discovery. AJ suggests:
	
	> General plugins vs Singer plugins: General plugins must get some of there config from env vars; Singer plugins should not need env vars - but some do. Following from this:

	> - It's a healthy pattern for General plugins to declare `env:` entries to pass along settings to the plugin context (but probably not `env_aliases:`).
	> - It's a discouraged pattern for Singer plugins to declare `env:` entries. (We'll support if needed, but we should keep an eye on them and this should be the exception rather than the rule.)
	> - To avoid confusion around which env entries are needed for Singer plugins to function - versus which are convenient aliases, I think we should remove `env_alias:` entries from Singer plugins, replacing them with env: entries and an explanatory comment with issue link perhaps.

### Description V1

Quick summary of recent findings:

1. `env_aliases` / `env` - These are the same functionally. (New info to everyone!) - these fit the description as recently added in the JSON Schema for `env:`:
    - > `An environment variable that will be initialized with the setting's value whenever the plugin is invoked. This is used to inject environment variables into the plugin's execution context. If the named environment variable is already defined at runtime, Meltano will use the passed value from context instead of the configured value.`
    - Essentially a two-way read and write to/from the env variable.
2. The `env_alias` and `env` entries also have a special role in seeding `dbt_profiles.yml`. Specifically `PG_ADDRESS`, etc. I think (to confirm?) this is part of the reason dbt cannot be run on its own: because if the target wasn't invoked, those settings don't exist.
3. Settings aliases - newly discovered! Similar to `env_aliases` but at the setting level.
    - To be confirmed: if these are compatible with env injection in the form `<PLUGIN_NAME>_<SETTING_ALIAS>`. More in #3209.
4. `config` vs `settings`: `settings` are the plugin definition (optionally with default values) and `config` is one project's settings values.
5. When a user wants to map a setting to an environment variable, the recommended way of doing so is probably to add a `my_setting: ${MY_ENV_VAR}` mapping under the `config:` block.
6. General plugins vs Singer plugins: General plugins _must_ get some of there config from env vars; Singer plugins should not need env vars - but some do. Following from this:
    - It's a healthy pattern for General plugins to declare `env:` entries to pass along settings to the plugin context (but probably not `env_aliases:`).
    - It's a discouraged pattern for Singer plugins to declare `env:` entries. (We'll support if needed, but we should keep an eye on them and this should be the exception rather than the rule.)
    - To avoid confusion around which env entries are needed for Singer plugins to function - versus which are convenient aliases, I think we should remove `env_alias:` entries from Singer plugins, replacing them with `env:` entries and an explanatory comment with issue link perhaps.

------------------------

As per office hours 2022/02/02, the current mechanism of declaring an environment variable to be populated within plugin `settings` using `env:` and `env_alias:` causes confusion to even experienced Meltano users.

From the jsonschema:

```json
""env"": {
    ""type"": ""string"",
    ""description"": ""An environment variable that will be initialized with the setting's value whenever the plugin is invoked. This is used to inject environment variables into the plugin's execution context. If the named environment variable is already defined at runtime, Meltano will use the passed value from context instead of the configured value."",
    ""examples"": [
        ""DBT_PROFILES_DIR"",
        ""GE_HOME""
    ]
}

""env_aliases"": {
    ""type"": ""array"",
    ""description"": ""Deprecated. Use to delegate alternative environment variables for overriding this setting's value"",
    ""examples"": [
        ""TAP_ACCOUNT_ID"",
        ""TAP_ACCOUNT_IDENTIFIER""
    ],
    ""items"": {
        ""type"": ""string""
    }
}
```

Specifically, from `env`,:

> If the named environment variable is already defined at runtime, Meltano will use the passed value from context instead of the configured value.

This causes unexpected behaviours for users not aware of this dual behaviour. We propose that, in stead of relying on `env:` for both purposes (sourcing values to inject _and_ determining the target env var name to inject into), we simply remove the sourcing behaviour. This would mean Meltano would no longer search its runtime environment for matching env var names to retrieve values, effectively making any settings with `env:` specified an ""environment variable type setting"" (as suggested by @tayloramurphy [here](https://gitlab.com/meltano/meltano/-/merge_requests/2505#note_829519083)). Sourcing of values would only be possible via the standard mechanisms; explicit `config:` or env vars in the Meltano runtime of the form `<plugin name>_<setting name>`.

`env_aliases` might be kept to allow one setting to be mapped to multiple env vars. E.g.

```yaml
settings:
- name: aws_profile
  env: AWS_PROFILE
  env_aliases: [AWS_DEFUALT_PROFILE]
```

or both could be replaced with an array-type `env_vars:` e.g.:

```yaml
settings:
- name: aws_profile
  env_vars: [AWS_PROFILE, AWS_DEFUALT_PROFILE]
```

It is also worth considering if these settings should be injected as keys in config files either. Not doing so would further increase the specificity of the settings function as ""an environment variable created from a Meltano setting"".

### Possible Challenges

- Users can also set environment variables directly using the `env:` key in the `--environment` feature. This produces two ways to achieve the same outcome, and may cause conflicts and confusion to users. This can be handled with a resolution order (preferring settings above environment variables) and/or by warning/erroring if the same env var is specified by the two methods.",4
101800409,2022-02-03 10:42:29.871,Restore heading anchor links adjacent to headings in the Meltano docs,"Looks like the Meltano doc heading anchors are gone/hidden in the new docs site 🤔 They are still available at `<h1>` level by copying links for the table of contents, but it would be great to add anchors next to headings (and subheadings - all of the `<h*>` tags) for linking to specific sections of the docs.",4
101669858,2022-02-01 15:43:24.301,Adopt `pip-licenses` in place of LicenceFinder for tracking python dependency licenses in the Meltano project,Blocked by #3198,8
101669811,2022-02-01 15:42:13.643,Contribute `GitLabLicenseComplianceReportTable` to `pip-licenses`,"This issue is to track contribution of a `GitLabLicenseComplianceReportTable` class to the `pip-licenses` codebase, to allow direct interchangeability between `pip-licenses` and GitLabs `licence_management` tool.

As per [this discussion](https://gitlab.com/meltano/meltano/-/merge_requests/2500#note_820960272), LicenseFinder and the wrapper (called `license_management`) provided by GitLab i) do not support poetry natively and ii) only detect 2/3rds of our total licenses. A brief evaluation of `pip-licenses` detected all but 1 of our dependencies licenses, and supports poetry (by virtue of running within a python virtualenv). However, to adopt `pip-licenses` and still use the License Compliance UI and MR features in GitLab, we need to format the results produced by `pip-licenses` in the same format as the report outputted by the `license_management` wrapper. This can be done by contributing a new report class to `pip-licenses` for optionally outputting the required GitLab artefact.",8
101487305,2022-01-28 19:28:42.416,Meltano Test bug with files bundles,"<!---
Please read this!

Before opening a new issue, make sure to search for keywords in the issues
filtered by the ""regression"" or ""bug"" label and verify the issue you're about to submit isn't a duplicate.

If you are submitting an issue with a tap, please include:

- account details
- target details
- entities selected  with meltano select (if you have selected any entities), as the bug may be related to a specific entity
- the full elt command you are running
- full output of the meltano elt command. Logs can get pretty long, so you can add the full log as a snippet in the Meltano project and add a link in the issue.
--->

This is based on [this slack thread](https://meltano.slack.com/archives/C013EKWA2Q1/p1643388924407829).

### What is the current *bug* behavior?
_What is happening now?_

When I try to use the `test` feature with dbt I get a message saying:
`Command 'test_publish_hub' could not be found. File bundle 'dbt' does not define any commands.`. It looks like its iterating my plugins by name and getting to my file bundle named `dbt` before it gets to my transformer named `dbt`. If I remove the files bundle, my tests work. If I rename my files bundle I get errors.

```
  transformers:
  - name: dbt
    pip_url: git+https://github.com/Tomme/dbt-athena.git@c8e0458f85aaa8f9b693f99903ba95c41b90573b
    config:
      target: athena
    commands:
      test_publish_hub: test --models marts.publish.meltano_hub.*
  files:
  - name: dbt
    pip_url: git+https://gitlab.com/meltano/files-dbt.git@config-version-2
```

### What is the expected *correct* behavior?
_What should be happening?_

The test feature shouldnt consider tests in the files plugin section. Maybe larger than that it might need to accept a plugin.plugin_name combo (i.e. meltano test transformer.dbt:test_123) if a project has multiple plugins with the same name.

### Steps to reproduce
_How one can reproduce the issue?_

Install dbt then add a command starting with the test prefix and run `meltano test dbt:test_name`.


### Relevant logs and/or screenshots
_Please use code blocks (\`\`\`) to format console output_



### Possible fixes
_If you can, link to the line of code that might be responsible for the problem or suggest a fix_



### Further regression test
_Ensure we automatically catch similar issues in the future_

- [ ] Write additional adequate test cases and submit test results
- [ ] Test results should be reviewed by a person from the team",4
101474780,2022-01-28 15:08:35.507,Improve documentation around what a namespace is and does,"Reading through the documentation, we don't do a good job of explaining what `namespace` does or means. There's no mental model we're trying to build for the user. When searching for namespace we show the user:

* https://docs.meltano.com/tutorials/postgres-with-postgres#run-meltano-elt
* https://docs.meltano.com/guide/transformation#adding-a-transform-to-your-meltano-project
* https://docs.meltano.com/concepts/project#custom-plugin-definitions
* https://docs.meltano.com/the-project/contributor-guide#plugin-definitions
* https://docs.meltano.com/concepts/plugins#package_name-extra
* https://docs.meltano.com/guide/containerization#gitlab-cicd

But none of those explains what it's doing or why one needs it!",2
101419508,2022-01-27 21:48:24.198,Improve Documentation on Configuring Settings for Custom Plugins,"https://docs.meltano.com/guide/configuration#:~:text=If%20we%E2%80%99re%20dealing%20with%20a%20custom%20plugin%20instead

When configuring a custom plugin, the documentation is not clear that the settings configuration needs to be added for Meltano to pick up Environment Variable values for plugin settings. If settings configuration is not added, then plugin config values need to be added directly to meltano.yml to work.

Documentation should clarify that plugin settings need to be configured for Meltano to pickup environmental variables.",1
101390898,2022-01-27 14:22:28.205,Upgrade PIP added to docs,"A user of Meltano follows a new path to get started. While I haven't worked with folks who have used the new path from the new website, I have seen folks who are new to Python fail at the `pip install meltano` stage due to pip not being up to date (Every user I've worked with without fail who isn't from Python fails here, and needs my help, had 4 people fail here 1 of which being a dev with>10 year experiance, all of these devs were not python devs) , the error received when this happens is not very clear as it's normally due to some dependency resolution issue that's fixed in newer versions of pip. 

1. https://docs.meltano.com/getting-started
1. https://docs.meltano.com/guide/installation

And anywhere else a new venv is created we should direct people to update their pip version. Thoughts? Easy to update but wanted to be sure you're all onboard

Related to !2480",1
101340106,2022-01-26 23:09:13.832,Support for disabling elt.log creation,"The `meltano elt` command will naively write out a log file on every invocation, even in the presence of a logging.yaml config. The elt.log has no built-in support for log rotation/max size/etc and users may not even always be aware of its existence. In situations where disk space is limited, this could cause issues. 

Since we support custom log configs via logging.yaml now, we could give users an option to explicitly disable elt.log generation.",4
101322585,2022-01-26 18:04:26.911,discovery.yml missing from meltano.com,"With the update to the new meltano.com marketing we're no longer pushing a discovery.yml and requests return a 404:

```
(melty-3.8) ➜  run-dev2 curl -I https://meltano.com/discovery.yml
HTTP/2 404
server: nginx
date: Wed, 26 Jan 2022 18:03:39 GMT
content-type: text/html; charset=UTF-8
content-length: 71557
vary: Accept-Encoding
vary: Accept-Encoding
expires: Wed, 11 Jan 1984 05:00:00 GMT
link: <https://meltano.com/wp-json/>; rel=""https://api.w.org/""
x-powered-by: WP Engine
x-cacheable: non200
cache-control: max-age=600, must-revalidate
x-cache: HIT: 48
x-cache-group: normal
```",4
101260718,2022-01-26 04:45:57.345,Missing output folder,"<!---
Please read this!

Before opening a new issue, make sure to search for keywords in the issues
filtered by the ""regression"" or ""bug"" label and verify the issue you're about to submit isn't a duplicate.

If you are submitting an issue with a tap, please include:

- account details
- target details
- entities selected  with meltano select (if you have selected any entities), as the bug may be related to a specific entity
- the full elt command you are running
- full output of the meltano elt command. Logs can get pretty long, so you can add the full log as a snippet in the Meltano project and add a link in the issue.
--->

### What is the current *bug* behavior?
_What is happening now?_
when use the command `meltano init` output folder is not created and when try to use target-jsonl the process fail for that reason



### What is the expected *correct* behavior?

Meltano init command should create the folder



### Steps to reproduce

```bash
$ meltano init example
$ meltano add extract tap-shopify
$ meltano add loader target-jsonl
$ meltano run tap-shopify target-jsonl
```

### Possible fixes

Just create output folder into the init command


### Further regression test

- [ ] Add output folder creation with init command",1
101047911,2022-01-21 14:37:59.216,Problem with `env` key names at the plugin level,"Based on a thread from https://gitlab.com/meltano/meltano/-/issues/3173#note_817409586 and from https://gitlab.com/meltano/meltano/-/merge_requests/2489#note_808785679, it would be ideal to be able to set `env` keys at the plugin level vs the current state where it can only be done at the environment level.

The `env` key does seem to work at the plugin level but it prefixes the key in a way that makes it not useful (i.e. `TAP_SLACK__ENV_*`).

Ideally in combination with https://gitlab.com/meltano/meltano/-/issues/3173#note_817409586 we will be able to override environment variables in descending order, where the plugin `env` key is the lowest level and takes precedence. An environment plugin `env` takes precedence over a top level plugin `env`.

@aaronsteers wdyt?",4
101006476,2022-01-20 22:18:18.278,Support dbt v1.0,,4
101006028,2022-01-20 22:00:03.714,Can CI (securely) auto-deploy sample of the latest Meltano UI?,"Opening this issue to discuss if/how we could setup a PR-specific deployment endpoint that would allow quick preview and testing of Meltano UI updates.

Currently, I think we need to either share screenshots or build and deploy locally on our own workstations.",8
101004612,2022-01-20 21:09:05.275,Change instructions to recommend pipx instead of manually-created virtual environments,"Before joining Meltano, I'd created !1960+ to recommend installations with pipx instead of pip and manually-generated virtual environments. This came up again today and (now that !2496+ is fixed), I'd like to revisit/resurrect the pipx-based install guidance.

## Why this matters

I think we lose people when we ask them to create their own virtual environments. Also - then anything installed into that virtual environment is unavailable until the environment is manually activated, which requires memorizing the path to the activation script, and the whole process is just more cumbersome than I'd like.

Pipx was directly created to solve this problem:

- Virtual environments are created automatically.
- Shims are installed automatically to PATH, so you never need to manually activate the environments.
- The syntax is the same to install and upgrade, except one character: `pip3 install meltano` -> `pipx install meltano`.

Cc @edgarrmondragon",4
101001505,2022-01-20 19:40:37.477,Formalize rules that govern environment variable inheritance and precedence,"I've had a couple use cases where I'd like to set different environment variables at different levels. For sensitive credentials I set them in the .env while non-sensitive environment specific settings (like schema_name) get set at the meltano.yml environment level.

The functionality I want is to be able to combine both sensitive and non-sensitive variables for use in a plugin as an environment variable. A use case is in the squared repo where we have an Athena connection string used in Great Expectations and Superset:

`awsathena+rest://[YOUR_AWS_KEY]:[YOUR_AWS_SECRET]@athena.us-east-2.amazonaws.com/[YOUR_SCHEMA_NAME]?s3_staging_dir=s3://[YOUR_S3_PATH]`

I want to be able to set the secret keys in my .env and the path and schema in my meltano.yml environment. Right now templating is supported in meltano.yml for config values using environment variables but for some plugins, especially utilities, configs arent useful and we need to be building environment variables that the utility can access. Templating doesnt seem to be supported at the `env` level underneath the `environment` key. So right now I can set secrets in the .env and non-sensitive variables at the `environment.env` level in meltano.yml but I have no way of blending the two to pass along to a plugin, so I end up building the connection string multiple times in the plugins vs defining it once.

Ideas: Add an `env` key to plugin configs that allows you to use templating to build a new environment variable at the plugin/environment level. Or...allow templating at the current `env` key level based on whats in your .env file.

## Update (2022-05-05)

Scope of this issue is now to:

* Write tests based on the spec in the [Miro Board](https://miro.com/app/board/uXjVO3j_ADw=/)
* Fix anything that fails tests
* Update documentation to clarify inheritance

## Update (2022-05-03)

**Relevant [Miro Board](https://miro.com/app/board/uXjVO3j_ADw=/) where we're whiteboarding all of this**

## Spec Update (2022-04-08)

Note: this updated spec follows from my comment here: https://gitlab.com/meltano/meltano/-/issues/3173#note_817409586

### Problem statement:

Currently, env variables at different levels of the code cannot necessarily rely on each other or build upon each other.

### Proposed solution:

We would define environment variable inheritance to be valid across these layers, in this strict order:

- Level 1: terminal context (inherited either via `.env` or via shell env vars)
- Level 2: top-level `env:` declarations within `meltano.yml`
- Level 3: environment-level `env:` declarations.
- Level 4: `env:` declarations within top-level plugin definitions.
- Level 5: `env:` declarations within environment-scoped plugin definitions.

### Example

This shows how inheritance would be expected to function across the 5 named levels. (Could be a unit test during development.)

`.env`

```sh
# Level 1: terminal context
# Inherits from none
LEVEL_NUM=""1""
STACKED=""1""
```

```yml
env:
  # Level 2: top-level `env:`
  # Inherits from terminal context
  LEVEL_NUM: ""2""                  #  '2'
  STACKED: ""${STACKED}2""          # '12'
plugins:
  extractors:
    tap-foobar:
      env:
        # Level 4: plugin-level `env:`
        # Inherits from a environment-level `env:` if an environment is active
        # Inherits directly from top-level `env:` if no environment is active
        LEVEL_NUM: ""4""            #    '4'
        STACKED: ""${STACKED}4""    # '1234'
environments:
  prod:
    env:
      # Level 3: environment-level `env:`
      # Inherits from top-level `env:`
      LEVEL_NUM: ""3""              #   '3'
      STACKED: ""${STACKED}3""      # '123'
  plugins:
    extractors:
      tap-foobar:
        env:
          # Level 5: environment-level plugin `env:`
          # Inherits from (global) plugin-level `env:`
          LEVEL_NUM: ""5""          #     '5'
          STACKED: ""${STACKED}5""  # '12345'
```

## FAQ

**Q. Why does the global plugin level config (level 4) inherit from the environment-level config?**

A: This is counterintuitive at first but actually quite necessary.

Imagining that the plugin settings globally include a call to `$SNOWFLAKE_USERNAME` and imagine that each environment has its own value for `SNOWFLAKE_USERNAME: ___` in the respective `env:`. We don't want each environment to have to redundantly declare the plugin properties but we _do_ want to use the `$SNOWFLAKE_USERNAME` value that is most appropriate from the given execution context. Hence, the global-level plugin can be defined once to leverage whatever value is in `$SNOWFLAKE_USERNAME` and the plugin's configuration will still be seeded properly according to which environment is active.

## Providing level info for research/debugging

Perhaps in the `dry-run` proposal in #3146, it would be good to print where each environment variables are sourced. We will need to do this without exposing sensitive values.

So, if the example above were real, we might get debug info such as:

```
meltano run --dry tap-foobar target-snowflake
Printing debug info in response to `--dry` CLI argument. No actions will be executed.

The plugin `tap-foobar` was declared with the following context:

- env:LEVEL_NUM - derived from the following, in order of precedence:
    - environments[prod].plugins[tap-foobar].env[LEVEL_NUM]
    - plugins[tap-foobar].env[LEVEL_NUM]
    - environments[prod].env[LEVEL_NUM]
    - env[LEVEL_NUM]
    - shell.env[LEVEL_NUM]
...
```

Or perhaps a more realistic example:

```
meltano run --dry tap-foobar target-snowflake
Printing debug info in response to `--dry` CLI argument. No actions will be executed.

The plugin `tap-foobar` was declared with the following context:

- 'tap-foobar.config[username]' was derived from the following, in order of precedence:
    - environments[prod].env[FOOBAR_USERNAME]
    - env[FOOBAR_USERNAME]
...
```

In the above example, the 'prod' declaration of `FOOBAR_USERNAME` clobbers the global one. The user can infer from this printout that their `prod`-level config is working correctly, and if they were to remove the prod-level config, they'd get the value of the top-level `env:` declaration.

## Related Topics

More discussion on this can and probably should be moved to a follow on issue, but for completeness:

### Further detail and settings-level `env:` declarations (`level 6`)

Not listed above, but a plugin setting may have `env:` declared, which operates differently from the other layers. This `plugin.settings[].env:` declaration simply accepts a string as env variable name, and the specified setting value will get injected and sent _into_ the plugin's subprocess. 

- This functionality is required in order to have plugins behave as expected. Non-singer plugins don't receive a `config.json` so the env variable injection is the substitute.
- I think the `env:` function at the setting layer should _always_ be a pure child process injection (clobbering any existing env var but not reading from it).
- We should probably stop this layer 6 from _also_ sourcing data from layers 1-5, and instead make it a pure one-way injection into the plugin context. As described in #3447+
- I do not know if the current status quo for an existing `env` variable value will clobber an explicitly defined config value, or if the config value would clobber the env variable value, essentially replacing the env var value within the plugin's child process.",8
100981585,2022-01-20 14:00:40.215,Legacy python 3.6 compatibility code,"With the dropping of python 3.6 support in #2991, I also had a very quick search through the codebase to find comments that referenced 3.6 compatibility hacks:

```
(melty-3.8) ➜  meltano git:(2991-python-3-6-eol) ✗ grep ""3\.6"" * -r | egrep -v ""\.js|.vue|.lock|\.svg|.node_modules|\.log|Binary file|CHANGELOG""
src/meltano/core/plugin_install_service.py:        # TODO: use dataclasses.dataclass for this when 3.6 support is dropped
tests/meltano/cli/test_run.py:            # or is hack to work around python 3.6 failures
tests/meltano/cli/test_run.py:            # or is hack to work around python 3.6 failures
tests/meltano/cli/test_run.py:            # or is hack to work around python 3.6 failures
tests/meltano/cli/test_run.py:            # or is hack to work around python 3.6 failures
tests/meltano/cli/test_run.py:            # or is hack to work around python 3.6 failures

# and heres some other code paths that reference 3.7
src/meltano/core/block/extract_load.py:    with suppress(AttributeError):  # `wait_closed` is Python 3.7+
src/meltano/core/logging/utils.py:            with suppress(AttributeError):  # `wait_closed` is Python 3.7+
src/meltano/core/runner/singer.py:            with suppress(AttributeError):  # `wait_closed` is Python 3.7+
src/meltano/core/utils/__init__.py:    # and inspired by Python 3.7's `asyncio.run`
```

There's probably a few more here and there, but these are the most obvious ones that pop'd up in my quick searches.",4
100664094,2022-01-14 17:32:22.739,Update vuepress to support netlify deployment,"Related to #3147 (confidential due to some provision/security related topics). We're migrating our docs site to netlify and will be serving docs from a new subdomain of https://docs.netlify.com. 

Since we've long been using Netlify for doc previews on pull requests, Netlify is actually already configured to build on MR's, and deploy on merges to master - so we're already good to go as far as build/deploy goes.

The only real update required is to our vuepress structure and potentially some redirects on the netlify side.",4
100613872,2022-01-13 21:43:26.052,Follow-up from #2716: clickable urls in --help,"The following discussion from !2492 should be addressed:

- [ ] @edgarrmondragon started a [discussion](https://gitlab.com/meltano/meltano/-/merge_requests/2492#note_809858053):  (+3 comments)

    > My only nit with these links is that `click` seems to like wrapping them and so clicking them doesn't work:
    > 
    > ```console
    > $ meltano environment --help
    > Usage: meltano environment [OPTIONS] COMMAND [ARGS]...
    > 
    >   Manage Environments.
    > 
    >   Read more at https://meltano.com/docs/command-line-
    >   interface.html#environment
    > 
    > Options:
    >   --database-uri TEXT  System database URI.
    >   --help               Show this message and exit.
    > 
    > Commands:
    >   add     Add a new environment.
    >   list    List available environments.
    >   remove  Remove an environment.
    > ```",1
100613862,2022-01-13 21:42:09.876,Consider removing the `--verbose` option from the CLI since it's not used,"The following discussion from !2492 should be addressed:

- [ ] @edgarrmondragon started a [discussion](https://gitlab.com/meltano/meltano/-/merge_requests/2492#note_809836105):  (+2 comments)

    > Should we remove this if it's not used?",1
100612090,2022-01-13 20:45:31.531,Documented environment alias `MELTANO_LOG_LEVEL` setting for cli.log_level is not used,"The [documented alias `MELTANO_LOG_LEVEL`](https://meltano.com/docs/settings.html#cli-log-level) for log level is ignored by Meltano:

```console
$ meltano --version
meltano, version 1.90.1

$ MELTANO_LOG_LEVEL meltano invoke tap-gitlab
<no effect, same output>

$ MELTANO_CLI_LOG_LEVEL meltano invoke tap-gitlab
<debug output 👍‍>
```

The fix might be as simple as adding the `env_aliases` entry in [`setting.yml`](https://gitlab.com/meltano/meltano/-/blob/567b33a561b50863a2071dee913b810e04134f38/src/meltano/core/bundle/settings.yml#L29).

![Screen_Shot_2022-01-13_at_14.40.57](/uploads/61daf91b03bb4a4bfc534f0ab1a32de0/Screen_Shot_2022-01-13_at_14.40.57.png)",1
100267931,2022-01-07 17:54:54.909,Add support in `meltano run` for --dry,"Undocumented, but supported, by elt is the ability to trigger a dry run to show what would happen if it were a real run.

* `meltano run --dry tap-gitlab...`

See https://gitlab.com/groups/meltano/-/epics/135#note_803736558 for more context",2
100264929,2022-01-07 16:44:05.351,Google Analytics Tracking For `meltano run`,"It doesn't look like we have tracking set up for the new `meltano run` command. I'd be interested in seeing the initial usage there and what people are doing with it.

I assumed that any new CLI commands should have tracking so I'm labeling as a bug but I dont know if thats a fair assumptions.",2
100114167,2022-01-04 22:31:32.990,Setting config value with $ in the value fails due to Bash Expansion,"Steps to reproduce
1.  `meltano config target-snowflake set password ""@$a""` 

env file contains

`TARGET_SNOWFLAKE_PASSWORD=""@""` 

Should be set to `@$a`

--

Two reasons to do this in my opinion
1. Security - bash history shouldn't have your passwords inside of them
2. To prevent bash from expanding the values


I think we could do something like wait for STDIN data to come through to fix this (just like what happens if you run `cat` )",1
100098041,2022-01-04 18:56:33.989,Enable environment ENVARs to be set from command line,"The following discussion from !2478 should be addressed:

- [ ] @pnadolny13 started a [discussion](https://gitlab.com/meltano/meltano/-/merge_requests/2478#note_801189369):  (+2 comments)

    > @aaronsteers @edgarrmondragon @pandemicsyn do any of you know if theres a way for me to do this with CLI commands? The [dbt bundle accesses](https://gitlab.com/meltano/files-dbt/-/blob/master/bundle/transform/profile/profiles.yml#L20) these settings as environment variables and the only way I could find to set these was to introduce the environments features and add the `env` key with local settings but I didnt see a CLI command for this so had to instruct users to manually update the yaml.",4
99327575,2021-12-18 07:08:26.555,Broken link in Environments page,The link pointing to the CLI reference is broken: https://meltano.com/docs/environments.html#commands-with-support-for-environment,1
99302641,2021-12-17 23:30:05.860,Bump pyhumps to v3.0.x,"This should help a contributor address a bug they're seeing: https://gitlab.com/meltano/meltano/-/merge_requests/2406#note_729844878:

> this seems to be a bug in the old version of pyhumps that we're using:
> 
> **pyhumps==1.2.2** (what he have)
> 
> ```python
> >>> humps.decamelize(""SOME_ENV_VALUE"")
> 'som_e__en_v__valu_e'
> ```
>
> **pyhumps==3.0.2** (latest)
>
> ```python
> >>> humps.decamelize(""SOME_ENV_VALUE"")
> 'SOME_ENV_VALUE'
> ```
>
> So changing the version constraint in `pyproject` to `pyhumps = ""^3.0.0""` should and running `poetry lock` should do it.
>
> We could also do it on our end by merging this change from a different MR and rebasing this one.",2
99298476,2021-12-17 22:43:20.299,Add Kubernetes Helm Chart to Documentation,Related to #2149.,2
99283509,2021-12-17 19:43:52.964,Support incremental jobs for `meltano run`,"Spun out from #3094.

This items are necessary for incremental jobs.

1. Job ID support via `--job-id-prefix` xor `--job-id-name-pattern`
    - May be sourced based on the environment name - or via a CLI flag for invocations which do not have `--environment` set. (https://gitlab.com/meltano/meltano/-/issues/2301#note_750941505)
1. Top-level `--no-state-update` to not save state. (https://gitlab.com/meltano/meltano/-/merge_requests/2432#note_757893658)
1. Top-level `--full-refresh` to ignore saved state (which forces a full refresh). (https://gitlab.com/meltano/meltano/-/merge_requests/2432#note_757893658)

Of these three, I think the first is highest priority, while the second and third could optionally be delayed. In special circumstances where advanced control over state is needed, users can still fall back to `meltano elt` to get those functions.

Proposed syntax:

Job ID:

Set it at the front of a run or automatically get one based on the tasks run.

* `meltano run --job-id=<id> tap-gitlab...`

State:

We _could_ set state requirements right after `run`. For many tools (dbt included) there is a concept of incremental vs full-refresh runs. Those commands could propagate down to the specific plugins. Default is an incremental run.

* `meltano run --no-state-update tap-gitab ...`
* `meltano run --full-refresh tap-gitlab ...`",8
99179624,2021-12-16 16:33:33.922,"Inadvertent ELBContextBuilder use in meltano run, results in exception when tap run_discover is called","When running a tap that performs discovery with debug mode enabled, an exception is triggered because discovery mode in the plugin is looking for a debug logger but the base_output_logger isn't accessible:

```
Traceback (most recent call last):
  File ""/Users/fhines/projects/meltano/src/meltano/core/block/singer.py"", line 261, in start
    self.process_handle = await self.invoker.invoke_async(
  File ""/Users/fhines/projects/meltano/src/meltano/core/plugin_invoker.py"", line 277, in invoke_async
    async with self._invoke(*args, **kwargs) as (
  File ""/Users/fhines/.pyenv/versions/3.8.12/envs/melty-3.8.12/lib/python3.8/site-packages/async_generator/_util.py"", line 34, in __aenter__
    return await self._agen.asend(None)
  File ""/Users/fhines/projects/meltano/src/meltano/core/plugin_invoker.py"", line 262, in _invoke
    async with self.plugin.trigger_hooks(""invoke"", self, args):
  File ""/Users/fhines/.pyenv/versions/3.8.12/envs/melty-3.8.12/lib/python3.8/site-packages/async_generator/_util.py"", line 34, in __aenter__
    return await self._agen.asend(None)
  File ""/Users/fhines/projects/meltano/src/meltano/core/behavior/hookable.py"", line 87, in trigger_hooks
    await self.__class__.trigger(self, f""before_{hook_name}"", *args, **kwargs)
  File ""/Users/fhines/projects/meltano/src/meltano/core/behavior/hookable.py"", line 115, in trigger
    raise err
  File ""/Users/fhines/projects/meltano/src/meltano/core/behavior/hookable.py"", line 107, in trigger
    await hook_func(target, *args, **kwargs)
  File ""/Users/fhines/projects/meltano/src/meltano/core/plugin/singer/tap.py"", line 270, in discover_catalog_hook
    await self.discover_catalog(plugin_invoker)
  File ""/Users/fhines/projects/meltano/src/meltano/core/plugin/singer/tap.py"", line 315, in discover_catalog
    await self.run_discovery(plugin_invoker, catalog_path)
  File ""/Users/fhines/projects/meltano/src/meltano/core/plugin/singer/tap.py"", line 363, in run_discovery
    _debug_logging_handler(self.name, plugin_invoker, handle.stderr)
  File ""/Users/fhines/projects/meltano/src/meltano/core/plugin/singer/tap.py"", line 52, in _debug_logging_handler
    if not plugin_invoker.context or not plugin_invoker.context.base_output_logger:
AttributeError: 'ELBContextBuilder' object has no attribute 'base_output_logger'
```",2
99111820,2021-12-15 21:27:47.594,"Discuss: what to call ""inline transformers"" aka ""stream maps"" aka ""map transforms""","In preparation for #2299 (and/or the similar #2300), we should discuss what to call these things. We especially should consider how the naming decisions will impact (1) the new user experience and (2) possibly overlap or confusion regarding other existing plugins and interactions.

## Background Context

### History of ""Stream Maps"" naming within the SDK

When developing the Stream Maps feature in the SDK, one very important paradigm which allowed that effort to move forward was the concept of the ""Map"" process of a ""Map-Reduce"" paradigm. While the word ""tranformation"" can mean anything in the set of operations {Map/Reduce/Lookup/Filter/etc.}, we are limited in an inline capacity to basically these two: Map and Filter. (Lookups are possible in an inline capacity but they are very different than lookups which occur in SQL, and as such they are extremely expensive and require a robust caching strategy that is far beyond our scope.)

So, ""Stream Maps"" naming or ""Map Transforms"" naming was intended to highlight the focus on operations which can be performed scalably in an _inline_ capacity. **Unlike ""true"" transform tools like dbt, we do not have all of the data available before we execute; we have only one record at a time and we must be able to treat each record in isolation before passing it downstream (or not).**

Legacy ETL tools tried to do any and all type of ""big T"" transform functions `{Map/Reduce/Lookup/Filter/etc.}` before passing data downstream to the target, which although more rare now is still a problem in the industry using existing tools like Informatica, SSIS, Alteryx, Azure Data Factory, etc. This ""traditional/legacy ETL"" approach is an anti-pattern for very good reason and we should be very clear we are _not_ supporting it.

### Why not just call them ""transformations"" or ""transforms""

If applying a little ""m"" for a lightweight inline map transform, `ELT` becomes `EmLT`. Note the little `m` and the big `T` are entirely different capabilities, with different performance constraints and resource availability. We _do not_ want to confuse users by saying this is okay: `ETLT` - this would be a huge step backwards in terms of DataOps best practices, and would cause detrimental confusion to end users if they think there's no difference between ""transform"" in the middle of EL and the ""transform"" in the SQL/dbt layer.

When we speak of these conversationally, we use the word ""transformations"" often. However, the name collision with dbt (the *true* ""T"" in ""ELT"" pipeline, is extremely problematic - the `m` and `T` perform very different functions, with different physics constraints, latency expectations, and host server hardware profiles. They also have very different risk profiles for data loss in case of code errors or other unexpected failure.

## Option A: `mappers` are a new plugin type, with associated `mappings` entries in `meltano.yml`

Similar to as written up in #2299:

```shell
meltano add mapper meltano-map-transform
meltano add mapper pipelinewise-transform-field
```

```yml
# meltano.yml
mappers:
- name: meltano-map-transform
  pip_url: meltano-map-transform
  config:                         # Optionally, a default config.
    # ...
- name: pipelinewise-transform-field
  pip_url: pipelinewise-transform-field
extractors:
- name: tap-gitlab
  # ...
  mappings:
  - name: pii-hasher               # The name of the map transform to apply.
    mapper: meltano-map-transform  # The mapper plugin to use.
    config:                        # What will be sent to the transformer in a config.json file.
      stream_maps:
        customers:
          id_hashed: md5(record['id'])
          id: None
```

Option B: Transformers are of a `type`: `streaming` or `build`

Another (admittedly less preferred) option would be to allow `mappers` into the family of `transformer` utilities but with the addition of a new `type` attribute which specifically distinguishes them from other transformation types.

The `dbt` transformer would be something to indicate it has all data landed in the target upfront, such as `build` or `post-load`.

The `mapper` transformers would have some `type` value indicating their place as map-transforms and not full fledge data build/transform/aggregation tools. Possible values for `type` could be: `streaming`, `inline`, `mapper`, or `stream_map`.

cc @tayloramurphy, @edgarrmondragon, @pandemicsyn, @kgpayne",4
99104978,2021-12-15 18:28:30.924,Add a default Environment when `meltano init` is invoked,"### Problem to solve

Increase awareness and use of the [`Environments` feature](https://gitlab.com/meltano/meltano/-/issues/2869) by including a default `environments: ...` entry in `meltano.yml` for new projects.

### Target audience

Users starting fresh new Meltano projects.

### Further details
(Include use cases, benefits, and/or goals)

- New projects get the benefits of having at least one declared environment in `meltano.yml`.

### Proposal

An initial set of environments that might be sensible is:

```yaml
environments:
- name: dev
  config:
    plugins: {}
  env: {}
- name: prod
  config:
    plugins: {}
  env: {}
```

### What does success look like, and how can we measure that?

We should see an increased use of `environments` for new projects.

### Regression test
(Ensure the feature doesn't cause any regressions)
- [ ] Write adequate test cases and submit test results
- [ ] Test results should be reviewed by a person from the team

### Links / references",2
99104583,2021-12-15 18:17:31.244,Add a default logging.yaml when `meltano init` is invoked,"### Problem to solve

Now that structured logs have been added to Meltano by https://gitlab.com/meltano/meltano/-/issues/2772, it'd be good if new Meltano projects were initialized with a `logging.yaml` containing some good defaults.

### Target audience

Users starting fresh new Meltano projects.

### Further details

- Logging configuration is present from the start
- Users are aware of configurable logging by virtue of the file being present

### Proposal

The initial `logging.yaml` should probably have both the option for colored logs during development, and JSON logs for production scenarios.

The file should probably include a header with some instructions, links, etc.

### What does success look like, and how can we measure that?

Users start using and configuring structured logging in their Meltano projects. This increased use should bring more contributions as bug reports and feature proposals.

### Regression test
(Ensure the feature doesn't cause any regressions)
- [ ] Write adequate test cases and submit test results
- [ ] Test results should be reviewed by a person from the team

### Links / references

_Please note that this was taken from GitLab, to be changed accordingly_",4
99035176,2021-12-14 18:52:17.350,Deprecate `env_aliases` from Meltano,"Our latest `discovery.yml` file contains ~~36 refs to `env:` and~~ 73 references to `env_aliases:`. I propose we remove these from `discovery.yml` and instead direct users to use the explicit environment variable logic: `<plugin_name>_<setting_name>`.

Optionally, we can preserve this functionality in case users want to add their own mappings within their `meltano.yml` files, but to my understanding, this is fully redundant with just setting the config value to be equal to the value (e.g. `value: $MY_ENV_VAL` is basically equivalent to adding the `env: MY_ENV_VAL` or `env_aliases: [MY_ENV_VAL]`).

As I understand the evolution of this featureset, the custom-named env variable aliases existing prior to the newer `<plugin_name>_<setting_name>` parsing convention. Since adding that newer convention, I think we have seen this convention go mainstream for most use cases, and we run into problems with those legacy variable names being confusing user experience if the ingestion is not expected, and/or if the order of resolution is surprising to users. See #2984 for more info on this.

## Update (2022-01-24)

Following from the notes in !2507+, we think that `env:` is serving a different function, and so this deprecation should only affect `env_aliases`. Further discussion re: `env:` can happen in !2507 and then we'll feed the findings/outcomes back into this issue.

## Best reason not to build

This is a breaking change for users who are using the older (custom) environment variable names for their projects. 

That said, it should be a simple fix to _either_ add the alias back to the `meltano.yml` file (as the `value:` or mapped in `env:`/`env_aliases:`) or just use the newer `<plugin_name>_<setting_name>` variable names.",8
98980367,2021-12-14 03:36:02.867,Research spike: can targets be tested with a zero-length input stream?,"In regards to applying a generic test to targets, there is some discussion regarding whether we would need to physically create a new table in the target in order to perform the test. Douwe had posited that it may be possible to test basic connectivity information by simply sending a zero-length input stream.

I (AJ) hadn't considered that option before, and it seems a simple enough test/spike to perform before implementing a version that would necessarily create side-effects in  data landing zones which are likely considered ""production"" by our users.

Discussion here on a side-effect-free approach: https://gitlab.com/meltano/meltano/-/issues/1370#note_748076418

And ""Option B"" here: https://gitlab.com/meltano/meltano/-/issues/1370#note_747150933",1
98904180,2021-12-12 21:32:56.794,hardcoded decoding as ascii when log-level=debug,"<!---
Please read this!

Before opening a new issue, make sure to search for keywords in the issues
filtered by the ""regression"" or ""bug"" label and verify the issue you're about to submit isn't a duplicate.

If you are submitting an issue with a tap, please include:

- account details
- target details
- entities selected  with meltano select (if you have selected any entities), as the bug may be related to a specific entity
- the full elt command you are running
- full output of the meltano elt command. Logs can get pretty long, so you can add the full log as a snippet in the Meltano project and add a link in the issue.
--->

### What is the current *bug* behavior?

I'm running 
```
meltano --log-level=debug select tap-google-sheets --list --all
```
on a sheet that contains emoji characters, Meltano CLI crashes trying to decode UTF stream as ASCII.

### What is the expected *correct* behavior?

Meltano shouldn't crash when handling non-ascii stream in debug mode

### Steps to reproduce

retrieve a catalog that contains non-ascii characters, in debug mode


### Relevant logs and/or screenshots

```
Traceback (most recent call last):
  File ""/home/matte/.local/pipx/venvs/meltano/lib/python3.9/site-packages/meltano/cli/__init__.py"", line 53, in main
    raise CliError(str(err)) from err
meltano.cli.utils.CliError: 'ascii' codec can't decode byte 0xc3 in position 83: ordinal not in range(128)
```

### Possible fixes


https://gitlab.com/meltano/meltano/-/blob/master/src/meltano/core/plugin/singer/tap.py#L45

`file_like_obj.write(data.decode(""ascii"") if write_str else data)`

I guess the encoding should be read from the environment?

### Further regression test
_Ensure we automatically catch similar issues in the future_

- [ ] Write additional adequate test cases and submit test results
- [ ] Test results should be reviewed by a person from the team",2
98849479,2021-12-10 18:39:21.266,Multi-yaml Config does not respect the $MELTANO_PROJECT_ROOT env var,"<!---
Please read this!

Before opening a new issue, make sure to search for keywords in the issues
filtered by the ""regression"" or ""bug"" label and verify the issue you're about to submit isn't a duplicate.

If you are submitting an issue with a tap, please include:

- account details
- target details
- entities selected  with meltano select (if you have selected any entities), as the bug may be related to a specific entity
- the full elt command you are running
- full output of the meltano elt command. Logs can get pretty long, so you can add the full log as a snippet in the Meltano project and add a link in the issue.
--->

### What is the current *bug* behavior?

Multi-yaml Config glob resolves relative to _current_ dir even when the Meltano Project root is overridden using `$MELTANO_PROJECT_ROOT` env var.

```yaml
include_paths:
  - ./*.meltano.yml
```

![Screenshot_2021-12-10_at_18.08.22](/uploads/1bad0dd929c8424f8288ed01762d3efc/Screenshot_2021-12-10_at_18.08.22.png)


### What is the expected *correct* behavior?

Globs should be relative to `$MELTANO_PROJECT_ROOT` if it is provided.



### Steps to reproduce

See example above 👆



### Relevant logs and/or screenshots
_Please use code blocks (\`\`\`) to format console output_



### Possible fixes
_If you can, link to the line of code that might be responsible for the problem or suggest a fix_

Probably need to do some path munging [here](https://gitlab.com/meltano/meltano/-/blob/master/src/meltano/core/project_files.py#L93) to produce absolute paths relative to the defined project root.

### Further regression test
_Ensure we automatically catch similar issues in the future_

- [ ] Write additional adequate test cases and submit test results
- [ ] Test results should be reviewed by a person from the team",4
98775777,2021-12-09 19:29:35.351,Pin meltano/tap-salesforce to tag v1.4.27,"An update to allow schema selection [might soon be merged](https://gitlab.com/meltano/tap-salesforce/-/merge_requests/14)
and it may not be considered a simple patch, so we should pin the current latest ([`v1.14.7`](https://gitlab.com/meltano/tap-salesforce/-/tags/v1.4.27)) tag so users migrate at their own pace to the newer version that supports schema selection.",2
98720552,2021-12-09 03:11:00.514,Pinned git tag adswerve/target-bigquery does not exist,"<!---
Please read this!

Before opening a new issue, make sure to search for keywords in the issues
filtered by the ""regression"" or ""bug"" label and verify the issue you're about to submit isn't a duplicate.

If you are submitting an issue with a tap, please include:

- account details
- target details
- entities selected  with meltano select (if you have selected any entities), as the bug may be related to a specific entity
- the full elt command you are running
- full output of the meltano elt command. Logs can get pretty long, so you can add the full log as a snippet in the Meltano project and add a link in the issue.
--->

### What is the current *bug* behavior?

Users are not able to install the pinned version/tag of `adswerve/target-bigquery`

### What is the expected *correct* behavior?

Users should be able to install `target-bigquery`.

### Steps to reproduce

1. Try installing the `adswerve` variant of `target-bigquery` with the default _discovered_ `pip_url`.

### Relevant logs and/or screenshots


```shell
Loader 'target-bigquery' could not be installed: failed to install plugin 'target-bigquery'.
  Running command git clone --filter=blob:none -q https://github.com/adswerve/target-bigquery.git /private/var/folders/w4/qkk0hzgd7h59kkrlkxr9ly380000gq/T/pip-req-build-qi2be3sa
  WARNING: Did not find branch or tag 'v0.11.3', assuming revision or ref.
  Running command git checkout -q v0.11.3
  error: pathspec 'v0.11.3' did not match any file(s) known to git
WARNING: Discarding git+https://github.com/adswerve/target-bigquery.git@v0.11.3. Command errored out with exit status 1: git checkout -q v0.11.3 Check the logs for full command output.
ERROR: Command errored out with exit status 1: git checkout -q v0.11.3 Check the logs for full command output.

Failed to install plugin(s)
```

Slack conversation: https://meltano.slack.com/archives/C01TCRBBJD7/p1639015962380400

### Possible fixes

It's a typo, so pin the `0.11.3` tag instead.

### Further regression test

- [ ] Write additional adequate test cases and submit test results
- [ ] Test results should be reviewed by a person from the team",1
98558010,2021-12-06 22:48:32.028,Move to adapter-specific installations of dbt (Snowflake),"_Note: In addition to the below description, this proposal is intended to resolve https://gitlab.com/meltano/meltano/-/issues/3098+_

--------------------------

The dbt docs are now recommending users to install an adapter-specific version of dbt rather than the generic `dbt` or `dbt-core` packages. This is true even for mainstream packages like Postgres. https://docs.getdbt.com/dbt-cli/install/pip

## Proposal: use an adapter-specific plugin name

These examples all use Snowflake as an example, but we would support 4 adapters at launch (same as supported today):

- `dbt-bigquery`
- `dbt-redshift`
- `dbt-postgres`
- `dbt-snowflake`

### Installation

```console
> meltano add transformer dbt-snowflake
Installing `dbt-snowflake`...
Install completed.
```

### New user experience 

1. A new user installing `dbt-snowflake` for the first time, would following the installation instructions above: `meltano add transformer dbt-snowflake`.
2. After installing the dbt-snowflake transformer, they'd have 3 auto-created profiles in the auto-installed `profiles.yml` for `dbt`: 'prod', 'beta', and 'dev' - defaulting to `dev`.
3. Without any changes to `profiles.yml`, without setting any environment variables, and without needing to invoke a tap or target, they'd have their dbt environment configurable via the `config:` block in the `dbt-snowflake` plugin definition, inside `meltano.yml`.

### Migration experience

1. An existing Meltano user who already has the dbt plugin installed will perform a new install: `meltano install transformer dbt-snowflake`.
2. This will keep their existing `meltano.yml` file as-is and will likewise not affect their existing profiles file.
3. They will now have a new `dbt-snowflake` plugin and a new profiles file: `${MELTANO_PROJECT_ROOT}/transforms/profiles/snowflake/profiles.yml`.
4. The `dbt-snowflake` plugin will point to the _new_ profiles directory and the old `dbt` plugin will still point to the _old_ profiles directory.
5. As a one time config, the user will set any necessary config in `meltano.yml` in order to properly seed dbt profile info with the `dbt-snowflake` settings config.
6. Whenever the user is ready, they may optionally uninstall their older `dbt` plugin entry and delete the older `profiles.yml` file. (Side-by-side execution should work in the meanwhile.)

## As related to #3098 and missing special environment variables:

If we do this right, it's very likely we can also solve #3098 at the same time as launching new adapter-specific dbt plugins.

- All env variables currently injected by the `extractor` or `loader` will instead be declared as `settings` registered with `env:` mappings, with their values set via standard `config:` blocks.
    - Aka: No ""magic"" environment variables are needed, all settings go through through plugin config.
- Migrating to adapter-specific dbt entries comes along with new adapter-specific `profiles.yml` files.
- The migration from generic `dbt` plugin to the adapter specific ones is a one-time breaking change which allows us to flush the tech debt without breaking existing installations using the legacy `dbt` plugin definition.

### Messaging re `meltano run dbt:run`

After releasing this, we'd recommend users to migrate to adapter-specific dbt plugins if they want to use `dbt:run` and other dbt-related invocations as part of `meltano run`. (Hence, a resolution to #3098.) This would be a one time and _voluntary_ breaking change, after which they'd (1) be on the latest version of dbt and (2) have a more resilient and robust `dbt` experience in Meltano overall.

## Python updates under the hood

Internally:

Any logic internal to Meltano core which is hardcoded to the `dbt` plugin name would need to be replaced with logic that accepts plugins with the name `dbt*`.

## Pointing to adapter-specific profiles folders

Important to note is that dbt only accepts overrides for the _directory_ containing `profiles.yml`, and does not allow you to point directly to a single file. For this reason I'm proposing the files bundles have `profiles.yml` files with paths in the format `${MELTANO_PROJECT_ROOT}/transform/profiles/<adapter>/profiles.yml` and then `DBT_PROFILES_DIR` for `dbt-snowflake` (for instance) would default to the directory `${MELTANO_PROJECT_ROOT}/transform/profiles/snowflake`.

## Accompanying `files` Bundles:

This should be reviewed and tested along with:

- `files-dbt-biquery`: TODO
- `files-dbt-postgres`: TODO
- `files-dbt-redshift`: TODO
- `files-dbt-snowflake`: https://gitlab.com/meltano/files-dbt-snowflake/-/merge_requests/2

## Usability: Invocation via `dbt:run` vs `dbt-snowflake:run`

If we install adapter-specific plugins as distinct `transformer` plugins, presumably this also makes their invocation slightly more verbose.

- On the 'pros' side: this makes it very simple to support multiple dbt adapters in a single project, with no ambiguity. For instance: `dbt-snowflake:run` and `dbt-athena:run` would both ""just work"".
- On the 'cons' side: invocation is more wordy, and any sample code like `meltano run dbt:run dbt:test` is no longer copy-paste-able to other projects.

@kgpayne's suggestion in #3220+ could mitigate this. Ideally, if dev time were not a factor, we'd probably alias the first dbt installation as 'dbt' and then subsequent installations would _not_ be aliased.

## Previous proposals:

<details><summary>Click to expand</summary>

### Option 1: Prompt during installation. (Replaced with 1B)

<details><summary>Click to expand</summary>

```console
> meltano add transformer dbt

Please select from the following list:
1. Redshift
2. Snowflake
3. Postgres
4. BigQuery
5. SQLite

To install more than one adapter, use commas as separators.

> 5

Installing `dbt-sqlite`...
Install completed.
```

```console
> meltano add transformer dbt --adapter=snowflake
Installing `dbt-snowflake`...
Install completed.
```


### Option 2: Read from meltano environment.

If we add the ability to specific a `primary_loader` to each environment, we can then check if the required adapter is installed in the `dbt` venv, installing it if not yet added. This could also be used to select or generate a proper profile from the target definition (more discussion about auto-managed profiles in #3104).

</details>",8
