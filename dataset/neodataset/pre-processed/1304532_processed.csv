,issuekey,storypoints,context
0,26249792,1.0,remove sslmate verification record dns terraform env config state note import record dns terraform config obtain sslmate verification record maintain api sslmate terraform they need remove config terraform state actually delete
1,26230890,3.0,rca short outage please note incident relate sensitive datum security related consider label issue mark confidential incident gitlab com gl infra summary we short outage relate issue redis cluster cause elevated memory usage affect web team attribution minute downtime degradation m for calculate duration event use platform metrics dashboard look appdex slo violation impact metrics start following what impact incident service depend redis work outage affect service who impact incident user how incident impact customer error how attempt access impact service feature how customer affect how customer try access impact service feature include additional metric relevance provide relevant graph help understand impact incident dynamic detection response start following how incident detect pingdom alert immediate report user do alarming work expect yes how long start incident detection how long detection remediation be issue response incident bastion host access service available relevant team memeber page able root cause analysis the purpose document understand reason cause incident create mechanism prevent recur future a root cause person way writing refer system context specific actor follow s blameless manner core root cause analysis for necessary start incident question happen keep iterate ask time while hard rule time help question deep find actual root cause keep min come answer consider follow different branch example usage s the vehicle start problem why the battery dead why the alternator function why the alternator belt break why the alternator belt useful service life replace why the vehicle maintain accord recommend service schedule fifth root cause what go start follow identify thing work expect any additional out go particularly what improve start follow use root cause analysis explain improve prevent happen be improve detection time detection be improve response time response be exist issue prevent incident reduce impact do indication knowledge incident place corrective action annotation feature flag metric oom event sure proper memory saturation alert redis research fail fix send redis log elastic upgrading redis high allow memory purge option deallocate memory available redis add redis memory fragmentation alert add specialise redis memory saturation metric guideline blameless rca guideline s
2,26226264,3.0,improve project restoration runbook while work i need spend lot time research db restoration step cover detail runbook we add documentation find right base backup variable need set pipeline necessary timeout setting let job complete gcp project restore instance run etc thankfully branch but merge instruction contain typo instead instead
3,26225375,3.0,add external access kubernetes prometheus we need simplified way directly access prometheus server console document script proxy tunneling access create web service auth access
4,26222269,4.0,clean recording rule kubernetes recording rule fail kubernetes label difference chef vs fix rule work split filter incompatible rule this break item
5,26170502,5.0,deploy redundant praefect node gstg load balancer spawn discussion praefect readiness review let try deploy redundant praefect node gcp load balancer avoid introduce new spof infrastructure x terraform change terraform change x chef repo mr
6,26160887,1.0,dns record modification domain ux like run research ux prototype poc need domain host we domain redirect pyjamas site can subdomain allocate prototype it great add dns zone gcp go forward people add second level subdomain sub domain point instance ps the redirect site tls fix
7,26121604,1.0,discuss how slo alert ci runners noisy actionable the slo alert ci runners meet reasonable standard actionable alert we know i want jot specific thing well this alert unreasonably low signal noise ratio the apdex score drop slo time hour last long pagerduty alert screenshot this alert runbook the pagerduty alert refer troubleshooting service ci file exist runbook repo the pagerduty alert give recommend response amount look metric log sign abusive behavior high request rate slow response dependent service that good generic advice slo violation vague actionable what dashboard useful troubleshoot ci runners the ci runners service ci dashboard show workload relate metric queue size percentile duration recently complete job job count runner node the service platform metrics dashboard latency apdex graph type set ci runner should slo reframe way make achievable the following initial thought i believe advocate improve probably specific idea improvement the ci runner workload largely outside control user define job time complete depend external resource outside control visibility example would slo violate sufficiently large number job run sleep reframing slo term address controllable aspect platform execution job goal achievable what work agreement ci runner platform describe end user condition job execution abort job timeout deadline usage quota machine resource cpu network bandwidth max connection count max disk iops etc we benefit compare set quota platform provider gcp aws heroku etc other platform provider share problem need set achievable slo resilient abusive behavior support auto scale auto termination auto rate limiting both auto scaling build automatic defensive mitigation abuse require have specific goal latency target unconstrained input workload more prescriptively describe service level super helpful make concrete proposal automate tactic achieve goal improve cost efficiency reduce toil cc
8,26102795,1.0,add slack integration sentry problem server error collect sentry appropriate team learn error timely basis solution i like configure sentry rich slack integration this follow step i create slack application configuration document the piece datum complete integration id client d secret client secret token verification token these value need add sentry chef recipe the value variable store team vault record slack app secure sentry once value add sentry sentry restart add workspace page work allow configure rest alert rule
9,26063153,2.0,kubernete ingress crash the new kubernete ingress gitlab services manage gitlab kubernetes integration keep crash it unclear problem integration app version kubernete run the migration aws gke roll solve the ingress pod update replicas available pod warning and pod logs receive sigterm shut shut controller queue update status ingress rule remove remove address ingress status update ingress version gitlab production production auto deploy status stop nginx process the ingress question
10,26061435,2.0,investigate intermittent response time spike concurrently affect multiple service hunt clue intermittent slowness report during day maybe internal user report observe brief period slow performance web ui git push these report far lack detail review frontend metric reveal brief large spike percentile response time record haproxy several haproxy backend service pool concurrently report spike imply share dependency specific service stress this issue aim document finding far try identify internal service stress spike if pattern emerge try determine nature bottleneck high overall request rate high rate slow costly request contention machine resource lock contention saturation resource pool etc
11,26060003,6.0,scale review app runner right job stick wait single runner pick review app job i think scale bulk time download artifact we need core machine handle
12,26055944,5.0,investigate run discourse gke we look migrate gke this issue track research go able test project can use autodevops i think probably use autodevops build deployment discourse but i investigate work
13,26055145,3.0,ssl certificate expire the ssl certificate expire
14,26038539,1.0,enable http compression summary http compression enable see report pagespeed proposal enable gzip compression nginx this greatly improve performance cause issue
15,26034432,2.0,request access restore project ongre team we need access team order forward issue we access restore project
16,26008352,1.0,rca oct outage please note incident relate sensitive datum security related consider label issue mark confidential incident summary during production change chef run sudo unable create file require run rake command set yarn this cause application unable serve request properly to specifically command use yarn depend specific file update trigger when chef run sudo version file update specific command initialize yarn run file permission issue subsequent run chef root trigger initialization command specific file update affect team attribution minute downtime degradation for calculate duration event use platform metrics dashboard look appdex slo violation impact metrics start following what impact incident service outage sub service brown exposure sensitive datum who impact incident external customer internal customer specific team how incident impact customer prevent x incorrect display y how attempt access impact service feature how customer affect how customer try access impact service feature include additional metric relevance provide relevant graph help understand impact incident dynamic detection response start following how incident detect do alarming work expect how long start incident detection how long detection remediation be issue response incident bastion host access service available relevant team memeber page able root cause analysis the purpose document understand reason cause incident create mechanism prevent recur future a root cause person way writing refer system context specific actor follow s blameless manner core root cause analysis for necessary start incident question happen keep iterate ask time while hard rule time help question deep find actual root cause keep min come answer consider follow different branch example usage s return error why the yarn install run why chef attempt run create file folder why chef client run sudo permission execute user home folder why the chef recipe assume rake command access folder why chef usually run root login chef client daemon what go start follow identify thing work expect any additional out go particularly what improve start follow use root cause analysis explain improve prevent happen be improve detection time detection be improve response time response be exist issue prevent incident reduce impact do indication knowledge incident place corrective action list issue create corrective action incident for issue include follow bare issue link issue label action include estimate date completion corrective action incldue name individual own delivery corrective action guideline blameless rca guideline s
17,26006259,8.0,multiple configuration issue prometheus kubernetes x failing rule evaluation show log x no monitoring prometheuses run cluster op environment monitor exist prometheus server we configure prometheus pods run gprd run currently gitlab monitoring promethe operator completely unaware missing redundancy x no external mechanism reach prometheus endpoint cluster we expose thano endpoint one utilize kube port forward reach prometheus endpoint x scrape datum appear inconsistent metric scrape kubernetes appear capture metric course time example query gitlab m x deduplication mulitple prometheus instance occur example query count gitlab gke container endpoint env environment image instance job namespace pod prometheus provider region service return multiple datum point gitlab monitoring promethe gitlab monitoring promethe all contribute awkward metric time specifically note container registry dashboard metric appear duplicate time impact ability properly monitor cluster confidence
18,25999110,2.0,extend certificate updater update expire gcp lb certificate as suggest today dna meeting good addition certificate updater able update gcp lb certificate
19,25931768,1.0,remove testbe env pubsub beat variable as note spurious pubsubbeat config testbe env it remove
20,25928449,2.0,need rebalance the gitaly nodes cross disk usage threshold need project move server for reference i know sre oncall normal workflow team
21,25928397,3.0,move terraform gcs instance service account project module while clean terraform code gitlab env secret bucket duplicate project storage bucket module block leftover create iam bind grant access terraform service account gce vm instance with secret bucket move project module sense iam bind old module move implicate manage terraform service account project export account email id terraform configuration access remote state lookup binding stay storage bucket module i inspect terraform state file gstg gprd appear service account currently manage terraform easily import env project configuration project module update add resource project module terraform account remove storage bucket module add project module import service account terraform state env project import resource terraform state env project bump module version project storage bucket environment
22,25912154,2.0,rollout thanos thano include number improvement include new sidecar flag time allow easily test change depend prometheus retention see
23,25875261,3.0,setup log praefect gstg as readiness review setup log praefect since deploy gstg setup log infrastructure verify replicate later gstg we need x a pubsub host x a recipe x add recipe praefect host
24,25845962,2.0,remove sidekiq mtail metric sidekiq internal metric job remove mtail collection datum
25,25813869,3.0,create storage node staging environment create new storage node staging environmnent order support synthetic system halt testing
26,25783498,1.0,service resource deprecate the resource project module deprecate warning deprecate user report issue dependent service resolvable please use module this resource remove version
27,25776897,3.0,rca incident gitlab com gl infra summary find container stop fail initial login triage affect team attribution minute downtime degradation minute timeline utc eoc alert utc eoc acknowledge pagerduty alert utc eoc determine exist discourse container restart utc eoc initiate rebuild discourse container destroy old rebuild restart new initiate bootstrap process utc rebuild process accidentally terminate utc rebuild restart utc container rebuild complete application begin serve request impact metrics start following what impact incident service outage sub service brown exposure sensitive datum who impact incident external customer internal customer specific team how incident impact customer prevent x incorrect display y how attempt access impact service feature how customer affect how customer try access impact service feature include additional metric relevance provide relevant graph help understand impact incident dynamic detection response start following how incident detect do alarming work expect how long start incident detection how long detection remediation be issue response incident bastion host access service available relevant team memeber page able root cause analysis the purpose document understand reason cause incident create mechanism prevent recur future a root cause person way writing refer system context specific actor follow s blameless manner core root cause analysis for necessary start incident question happen keep iterate ask time while hard rule time help question deep find actual root cause keep min come answer consider follow different branch example usage s the vehicle start problem why the battery dead why the alternator function why the alternator belt break why the alternator belt useful service life replace why the vehicle maintain accord recommend service schedule fifth root cause what go start follow identify thing work expect any additional out go particularly what improve start follow use root cause analysis explain improve prevent happen be improve detection time detection be improve response time response be exist issue prevent incident reduce impact do indication knowledge incident place corrective action list issue create corrective action incident for issue include follow bare issue link issue label action include estimate date completion corrective action incldue name individual own delivery corrective action guideline blameless rca guideline s
28,25776600,2.0,test effect make kernel watchdog patient nfs timeout api xx node goal determine make watchdog patient change failure mode kernel panic reboot fail synchronous write operation determine fail nfs write impactful rails app api xx vms kernel panic automatic reboot example if filesystem read preferable recovery require manual intervention method outline check nfs timeout follow arithmetic outline base nfs manpage base empirical testing the relevant mount option centisecond second the manpage say retry linearly add timeo timeout retrie mean try total so try wait second try wait second try wait second fail total patience second on non production vm try synchronous write nfs file sabotage write way nfs client unaware maybe client iptable rule drop outgoing incoming nfs traffic disable nic let kernel know sever tcp connection fail intended way this test aim synthetically confirm reproduce observed behavior adjust kernel watchdog timeout second patient nfs timeout second i think repeat synthetic test do vm fail write instead panic reboot be nfs volume mount read write mode revert sabotage do vm nfs mount usable if test api xx vms rails app functional reboot vm background in track gcp vm spontaneous reboot a subset reboot soft lockup kernel panic in week start track unplanned reboot particular flavor occur api xx web xx vms these vms regularly use nfs volume relevant we ample evidence gcp platform drop network connectivity network peer last ten second recover some soft lockup event sign network connectivity disruption precede kernel panic so far soft lockup event share nearly identical kernel stack trace suggest event common cause at time panic hung process hog cpu implicitly prevent cpu watchdog thread running stall wait synchronous write system as working hypothesis hung process write syscall leave process uninterruptible sleep prevent process include watchdog schedule vcpu we know write syscall file descriptor refer it point file nfs file network socket because pathology far affect host nfs possibility likely if hung process stall write nfs adjust watchdog timer patient filesystem timeout outcome change kernel panic fail write a write failure log kernel reliably detectable clear interpret kernel panic but need know fail write lead long last application problem automatically reboot vm this issue test effect fail nfs write api xx vm determine outcome diagnostically useful preferable reboot availability standpoint what soft lockup a soft lockup kernel panic trigger kernel detect cpus appear hang the detection mechanism processor cpu core vcpu dedicated watchdog thread reset count timer time run if processor fail run watchdog thread timer reach zero kernel take sign extreme duress possibly bug normally scheduler easily able issue time slice watchdog thread heavy load so kernel panic call soft lockup event
29,25771098,5.0,synthetic system halt test staging the stor gprd project storage node currently experience repeat unexplained system halt in order support migration balance large project stor gprd stor gprd important collect datum expect source system halt middle project migration plan step undertake orer accomplish test scenario documenting consequence expect failure focus issue plan x create new file storage node enable node storage gitlab admin console create handful maybe project begin migration project stor gstg node meanwhile shell session stor gstg system invoke following command sudo sysctl sudo sysctl echo c sudo tee sysrq trigger now observe serial port log stor gstg analyze migration log system log determine consequence system halt project migration
30,25734517,1.0,add metadata terraform manage project some gcp project manage project terraform module some manage manually there currently easy way tell ask check different repository i propose alter project module add metadata project indicate manage terraform manually change this simple add terraform manage tag like resource
31,25724069,3.0,provision properly during incident create end keep create point there issue initial provisioning we hit manual rerun chef client successful then gitaly complain opt gitlab git data repository directory miss the directory actually level high opt gitlab git data miss it manually create gitaly complain opt gitlab git data repository create what remain verify fully operational investigate directory create place find chef code example investigate
32,25716261,4.0,track spf record lookup timeout track issue
33,25695650,1.0,fix terraform ci permission while attempt apply job return follow error hcl error error create disk googleapi error require permission project gitlab production zone b disk sv gprd log more detail reason forbid message require permission project gitlab production zone b disk sv gprd log reason forbid message require permission project gitlab production zone b disk sv gprd log module api line resource resource error error create disk googleapi error require permission project gitlab production zone c disk sv gprd log more detail reason forbid message require permission project gitlab production zone c disk sv gprd log reason forbid message require permission project gitlab production zone c disk sv gprd log module api line resource resource while i manually apply change resolve error near term need update permission assign terraform ci service account assign permission note long term fix
34,25607220,4.0,plan synthesize system halt stage storage node project migration the stor gprd node usage exceed slo target however continue experience unexpected system halt nearly daily basis in order determine level severity understand consequence system halt midst procedure copy file storage node necessary design replication scenario staging environment
35,25603289,2.0,investigate exporting stackdriver metrics kernel panic reboot kibana prometheus problem summary currently comment collector track reboot manually parse stackdriver log rather check daily let automate collection datum point count log kibana exporter aggregate prometheus i strong preference kibana prometheus depend log format i expect prove easy accomplish if true choose path effort definition do this issue primarily focus determine log forwarding metric aggregation achieve the work develop query write runbook pointing dashboard scope require close issue if write runbook create dashboard achievable minimal effort track otherwise new issue open link
36,25593292,3.0,document patroni pgbouncer consul design troubleshooting runbooks discover open ticket they link hr per need update runbooks include information patroni rely consul health check leader election specific step diagnose troubleshoot issue a good bit information capture and change complete below issue author valuable add howto troubleshooting section runbooks project repository
37,25591865,3.0,move kubernetes we currently run stackdriver exporter project vm this make sharde exporter stackdriver type prefix difficult move kubernetes greatly simplify sharding reduce resource waste
38,25563008,5.0,reduce patroni sensitivity transient consul serfcheck failure goal tune replace consul serfcheck patroni leader lose ttl expire the serfcheck currently detect client reach patroni node disable serfcheck mean patroni long know failover client reach however unreliable network transient serfcheck failure cause unwanted patroni failover the overall goal improve availability writable postgres instance avoid unnecessary patroni failover brief network disruption allow patroni failover network disruption last modest timeout second disable serfcheck implicitly favor failover mode serfcheck work reliable network cause failover little aggressively unreliable network background in recent month patroni failover event trigger brief network connectivity disruption when failover take significantly long complete health check failure take resolve availability high wait little long failover root cause analysis identify know way patroni failover trigger intermittent network disruption the current issue aim mitigate scenario c link note copy convenience scenario c cause serf lan health check message udp port drop direction network path patroni leader consul agent and consul agent patroni leader slow refute suspicion effect the consul agent non patroni host declare suspicion patroni leader fail patroni leader limited window refute suspicion learn gossip consul agent if patroni leader promptly refute suspicion consul server invalidate patroni leader ttl expire leave patroni cluster leaderless the patroni replicas detect begin patroni failover procedure remedy tune replace serf check dependency patroni mention ticket reduce eliminate chance scenario if serfcheck replace careful design new health check failure mode for example ensure plan maintenance implicitly break new health check trigger patroni failover thing want avoid unnecessary caveat since complete analysis learn gcp networking infrastructure service stack currently run we believe consul correctly detect brief network connectivity outage we long term datum worth note frequency network disruption detect consul serfcheck appear low week early september note serfcheck failure show reboot unplanne kernel panic unrelated interesting issue track msmiley ls log xargs sudo zcat sudo cat log egrep grep eventmemberfaile tee wc consequently patroni trigger failover recently if trend persist issue tune replace serfcheck unnecessary but reason believe gcp network reliable week
39,25551894,3.0,enable alert ci env the ci environment alert rule deploy prometheus wire alertmanager we change able alert cloud nat error ci
40,25550100,3.0,gitlab iptable append new rule final drop rule when add new source exist firewall rule attribute form firewall rule prometheus server prometheus server internal metric access port protocol tcp source gitlab iptable cookbook append iptable rule exist rule include drop match target stop evaluation newly append rule to fix directory probably need clean exist rule change apply
41,25512267,5.0,provision infrastructure new chef gitlab op project in preparation migration new upgrade chef need build infrastructure deploy chef server resiliency purpose implement ha architecture in light fact focus new service run kubernetes bar exceptional circumstance actively work migrate exist service away gce vms expend effort build manage complex configuration likely worthwhile especially consider historically experience scale issue single chef server node stand x create deploy terraform module resource chef infrastructure load balancer instance backend instance group self heal configuration monitor new chef infrastructure sense split separate documentation handbook network architecture diagram runbook reflect
42,25512062,3.0,identify update require chef repo pipeline related workflow while work chef server migration need consider impact exist pipeline related workflow chef repo specifically need syntactic change script job cookbook uploader cookbook version bump role environment push manage data bag encrypt vault item in addition need ensure course migration test new server update infrastructure push net new change chef server stay date perform final migration cutover review chef repo pipeline syntax change require chef server late issue creation review runbook hoot documentation reference workflow impact upgrade knife command datum bag vault management script etc update chef repo pipeline chef server update parallel test new infrastructure
43,25479123,1.0,remove pgio stage patroni server exhibit exhibit we cause problem it remove
44,25478058,3.0,document plan test migration process chef server as need develop process migrate current chef server digital ocean new upgrade infrastructure gcp the initial assumption center backup restore process need adjust dig detail
45,25437232,3.0,look remove non needed add need public ip gprd after blackbox exporter console public ip gprd remove ip console break kubernete access remove blackbox exporter break monitoring unknown reason possibly whiteliste apparently problem appear gstg worth check case simply notice if whitelisting oppose customer whitelisting reason need public ip use static ip exit cloud nat this decouple cloud nat ip pool scale infrastructure isolate customer concnern one prometheus instance scrape ci runner manager federate ci prometheus firewall ip whitelisting ci break remove prometheus public ip fix whiteliste nat ip ci we use static ip prometheus instance decouple whitelisting scale cloud nat ip pool
46,25427390,1.0,chef run plausibly break relate chef vs chef upgrade sep chef error load file highline sep chef error chef run process exit unsuccessfully exit code cmiskell cmiskell dpkg chef ii chef the stack chef any input right way forward i hate break
47,25386425,1.0,issue investigation error datum team this issue investigation datum sync issue replica snowflake data team sync it relate cover sync license version customer fact the datum extract run airflow gitlab analysis project gcp scheduled set job project docker file config connections connect analytic replica gcp production instance postgre dr db gprd log error slack channel analytic pipeline we add example error date time comment cc data team i try separate thread investigation this issue sync investigate sync customer license version the backend gcp project different well manange solution split investigation
48,25347064,2.0,rollout prometheus prometheus come major improvement thanos sidecar datum streaming this greatly reduce overhead remote read api access current status release candidate available
49,25346668,1.0,validate kubectl wrapper script work desire during recent maintenance event wrapper script fail notify ask confirmation end user make change production utilize issue investigate appropriate change
50,25332462,1.0,the chef secret bucket create place look like secret bucket create place call env project entry ci since env project gitlab env gprd different tf envs separate tfstate track bucket create env project import bucket if decide bucket belong meta env env project gitlab google project tf env gprd gstg since secret bucket pre populated vms provision terraform work i vote env project feel free reassign i want sure page i change cc i chat
51,25316711,5.0,plan document role vault cluster in process initial deployment vault cluster need consider organize key value store vault assign access organization mechanic process approve assign manage access request this partnership security team likely ownership access management process
52,25316624,5.0,plan document require backend vault cluster once deploy production vault cluster need determine list backend support minimally include okta likely gsuite gitlab this issue intend conduct initial research available backend document overall architecture plan need subsequent implantation issue
53,25316457,3.0,automate document unseal process vault once production vault cluster need provide documentation automation unseal process document basic process seal unseal cluster add key new engineer remove key engineer change role leave team rotate key exist engineer monitor report key expiration rotation etc
54,25279862,1.0,fix thano object store error envs thanoscompactbucketoperationsfaile fire regularly thano gprd graph high consistent failure rate testbe pre environment alert environment scatter failure op gstg dr regular failure operation gprd lead alert this need clean testbe pre eliminate noise fix error increase threshold normal recoverable
55,25268576,2.0,slo background processing job target reach upper bind min currently sidekiq sla status you day run average status available considerable work set alert base slo violation if setup background processing job a considerable work separate split job priority queue appropriately hopefully allow place monitor granularly specific category job show metric detail it raise certain maybe processing job adhere min latency average though glance possible job project export substantially long
56,25251099,5.0,practice incident rough agenda basic summary this mean simple problem solve table scenario first test incident response basic group host interaction open question could should use staging should project repo like readiness md file practice scenario scenario service stop haproxy lb current status haproxy run door closed incident start eoc execute stop lb gstg start incident handle eoc use declare incident management validate x eoc page x imoc page x cmoc page x creation incident gdoc x creation incident issue x eoc imoc cmoc join incident zoom once manager cmoc join imoc cmoc talk comment understand issue cmoc log talk create incident link test status page cmoc talk update resolution action eoc talk action load balancer restart manager talk escalate engineer verify incident resolve cmoc confirm resolution talk update follow action item create incident review issue how escalate action item infradev cc feedback
57,25241086,1.0,run rake task update usage datum entry we need execute follow rake task production instance rake the goal rake task change record non integer stat json column compliant have integer value stat column this rake task run way against record non stat column batch rake against predefine d range lessen individual execution time batch rake start d table end d table in scenario need run chunk d table print run argument pass decision run infrastructure team decide
58,25220840,2.0,investigate make haproxy metric robust use point time sample value this lead anomalous misleading graph sampling miss active session see graph drop provably request rate limited it probably bad rate limit session drop immediately compare slightly long run normal http request go result consistent sampling the anomalous graph cause rabbit hole investigation need well option include haproxy exporter metric exist mtail metric bad enhance mtail capture datum
59,25122694,4.0,stabilize cloud nat ci see original context cloud nat roll private runner machine ci there job failure attributable nat failure connection failure coincide error burst nat since increase nat port vm ratio roll order mitigate failure ensure ip support private runner fleet there report job failure attributable nat ratio set originally cloud nat default there occasional period elevated error clear result high level failure slowdown acceptable level fix high layer retrie tcp application layer
60,25119636,2.0,add gitlab exporter add recipe gitlab exporter deploy
61,25071265,1.0,i get alert upon investigation i log check console lo thano oomed below log out memory kill process thano score sacrifice child kill process thano total anon file shmem sep dashboard inf op kernel thano invoke oom killer sep dashboard inf op kernel thanos sep dashboard inf op kernel cpu pid comm thano not taint gcp ubuntu sep dashboard inf op kernel hardware google google compute engine google compute engine bios google sep dashboard inf op kernel call trace sep dashboard inf op kernel sep dashboard inf op kernel sep dashboard inf op kernel sep dashboard inf op kernel sep dashboard inf op kernel sep dashboard inf op kernel sep dashboard inf op kernel sep dashboard inf op kernel sep dashboard inf op kernel sep dashboard inf op kernel sep dashboard inf op kernel sep dashboard inf op kernel sep dashboard inf op kernel sep dashboard inf op kernel rip sep dashboard inf op kernel rsp eflags sep dashboard inf op kernel rax rbx rcx sep dashboard inf op kernel rdx rsi rdi sep dashboard inf op kernel rbp sep dashboard inf op kernel sep dashboard inf op kernel sep dashboard inf op kernel mem info sep dashboard inf op kernel sep dashboard inf op kernel sep dashboard inf op kernel sep dashboard inf op kernel sep dashboard inf op kernel sep dashboard inf op kernel sep dashboard inf op kernel node sep dashboard inf op kernel node dma sep dashboard inf op kernel sep dashboard inf op kernel node sep dashboard inf op kernel sep dashboard inf op kernel node normal sep dashboard inf op kernel sep dashboard inf op kernel node dma u u u u u u m m sep dashboard inf op kernel node ume ume ume ume ume ume ume u u h sep dashboard inf op kernel node normal ume ume ume me me m m m sep dashboard inf op kernel node sep dashboard inf op kernel node sep dashboard inf op kernel total pagecache page sep dashboard inf op kernel page swap cache sep dashboard inf op kernel swap cache stat add delete find sep dashboard inf op kernel free swap sep dashboard inf op kernel total swap sep dashboard inf op kernel page ram sep dashboard inf op kernel page highmem movableonly sep dashboard inf op kernel page reserve sep dashboard inf op kernel page cma reserve sep dashboard inf op kernel page hwpoisone sep dashboard inf op kernel pid uid tgid rss swapent sep dashboard inf op kernel lvmetad sep dashboard inf op kernel havege sep dashboard inf op kernel systemd journal sep dashboard inf op kernel systemd udevd sep dashboard inf op kernel dhclient sep dashboard inf op kernel iscsid sep dashboard inf op kernel iscsid sep dashboard inf op kernel cron sep dashboard inf op kernel lxcfs sep dashboard inf op kernel atd sep dashboard inf op kernel systemd logind sep dashboard inf op kernel account daemon sep dashboard inf op kernel runsvdir sep dashboard inf op kernel acpid sep dashboard inf op kernel dbus daemon sep dashboard inf op kernel runsv sep dashboard inf op kernel runsv sep dashboard inf op kernel runsv sep dashboard inf op kernel runsv sep dashboard inf op kernel svlogd sep dashboard inf op kernel svlogd sep dashboard inf op kernel sep dashboard inf op kernel svlogd sep dashboard inf op kernel svlogd sep dashboard inf op kernel unattende upgr sep dashboard inf op kernel polkitd sep dashboard inf op kernel mdadm sep dashboard inf op kernel ntpd sep dashboard inf op kernel agetty sep dashboard inf op kernel agetty sep dashboard inf op kernel nginx sep dashboard inf op kernel sep dashboard inf op kernel sep dashboard inf op kernel sshd sep dashboard inf op kernel master sep dashboard inf op kernel qmgr sep dashboard inf op kernel trickster sep dashboard inf op kernel consul sep dashboard inf op kernel systemd sep dashboard inf op kernel sd pam sep dashboard inf op kernel screen sep dashboard inf op kernel bash sep dashboard inf op kernel sudo sep dashboard inf op kernel su sep dashboard inf op kernel bash sep dashboard inf op kernel fluentd sep dashboard inf op kernel ruby sep dashboard inf op kernel thano sep dashboard inf op kernel chef client sep dashboard inf op kernel memcache sep dashboard inf op kernel grafana server sep dashboard inf op kernel rsyslogd sep dashboard inf op kernel pickup sep dashboard inf op kernel nginx sep dashboard inf op kernel nginx sep dashboard inf op kernel nginx sep dashboard inf op kernel nginx sep dashboard inf op kernel sleep sep dashboard inf op kernel sleep sep dashboard inf op kernel cron sep dashboard inf op kernel sh sep dashboard inf op kernel ruby sep dashboard inf op kernel chef client sep dashboard inf op kernel sshd sep dashboard inf op kernel sshd sep dashboard inf op kernel out memory kill process thano score sacrifice child sep dashboard inf op kernel kill process thano total anon file shmem sep dashboard inf op kernel reap process thano anon file shmem
62,25031726,2.0,tune puma setting there small background rate puma worker termination default memory limit combination worker thread puma memory puma memory puma worker puma worker
63,24993969,3.0,view stackdriver metric ci project in order implement need able alert stackdriver metric ci project this involve x roll sd exporter x ensure exporter scrape prometheus x be able view metric thano x be able send alert base metric alertmanager cc
64,24938011,1.0,revert manual work create db user creation gstg deploy bypass yesterday staging deploy fail statement timeout see mr this revert see mr additional context any work create dedicated user need remove this issue close user remove system
65,24921423,1.0,track cost enable serial port log stackdriver enable cause additional duplicate log stackdriver like understand cost implication we estimate upper limit day prod likely easy simply measure fact sure
66,24920914,2.0,move docker build stage gitlab services ci image repository there docker file build beginning pipeline gitlab services project this image rarely change move ci images repository it currently slow work add lot time pipeline run op good reason the docker file docker file dockerfile build stage this image base google cloud sdk alpine include git openssh curl jq kubectl helm the version docker image reflect version helm instal once image build correctly file need update correct
67,24897306,1.0,gather input engineer content grafana training video objective to identify content engineer like include grafana training video goal gitlab engineers need comprehensive tutorial video empower use grafana ensure background use robust analytical visualization tool the goal issue identify content include video
68,24866551,3.0,terraform automation blog post it feel like hit milestone terraform config like good time write initial idea brainstorming use gitlab ci run terraform leverage gitlab environments feature clean drift upgrade greenfield ephemeral environment group project service vs legacy gitlab com infrastructure be autodevops mono repo versione module tag pipeline cleanup link design doc next step future plan gitlab flow incorporate vault prioritization scheduling fyi
69,24854308,1.0,remove geo config staging the staging environment set geo primary this probably leave previous work it cause unnecessary job likely lot unnecessary database record i go ahead remove primary config since secondary hurt if need set i create issue record
70,24846543,5.0,group project request gitlab group project request project group name character gitlab project administrator email gyoung provide brief overview reason project need long this project contain environment test reference architecture build maintain quality department this project indefinitely long reference environment stay relevant support quality issue create environment security provide list datum corresponding classification project access the datum sterile copy gitlab ce there sensitive datum the datum load test simulate appropriately complex project verify i describe valid correct group project access checklist make sure follow criterion meet understand project administrator if database copy datum process pseudonymization script x regular security update apply node project x unused instance remove timely manner x the project administrator responsible user additional administrator add project x the project administrator responsible justify cloud spend project x group projects intend development test demo work everything project consider temporary infrastructure tasks x create file name copy exist file change administrator group name variable x merge change master x create branch master name group push x verify pipeline complete successfully
71,24846514,5.0,group project request gitlab group project request project group name character gitlab project administrator email gyoung provide brief overview reason project need long this project contain environment test reference architecture build maintain quality department this project indefinitely long reference environment stay relevant support quality issue create environment security provide list datum corresponding classification project access the datum sterile copy gitlab ce there sensitive datum the datum load test simulate appropriately complex project verify i describe valid correct group project access checklist make sure follow criterion meet understand project administrator if database copy datum process pseudonymization script x regular security update apply node project x unused instance remove timely manner x the project administrator responsible user additional administrator add project x the project administrator responsible justify cloud spend project x group projects intend development test demo work everything project consider temporary infrastructure tasks x create file name copy exist file change administrator group name variable x merge change master x create branch master name group push x verify pipeline complete successfully
72,24814664,1.0,take screenshot group hook gitlab org follow i try manually trigger i think work could screenshot hook editing page i properly configure test mark confidential contain token
73,24797351,4.0,fix thanos compaction due lack monitoring thanos compaction break time this cause number problem much large indexing overhead lead slow query storage overhead large number index un compact block miss downsample datum all lead high query overhead slow query
74,24783977,3.0,stor gprd reboot stor gprd reboot utc open issue track repos zeroed file currently search repos cd opt gitlab git data ionice find se tmp
75,24770817,3.0,consider use docker image contain tooling local workstation use kubernetes not engineer machine configure we need specify certain configuration limit prevent access could centrally manage docker image assist situation
76,24745540,2.0,gcp vm spontaneous reboot raise reason reboot log infrastructure log stackdriver vaguely hopeful gcp able provide sort explanation
77,24740495,3.0,investigate create kubectl wrapper script production warning decide make sense create kubectl wrapper script include check current context command line if current context production and if current command then confirm continue modify production y n we discuss sure user wrapper script call kubectl directly
78,24740391,3.0,investigate ability utilize kubectl bastion proxy node at moment allow engineer access production cluster local workstation consider block access force engineer utilize proxy bastion type connection order perform operation production cluster
79,24739441,1.0,set retention period cloudtrail bucket we enable aws cloudtrail organization wide this mean cloudtrail log bucket primary org account bucket clean how long want cloudtrail log aws i think year sufficient alternatively long use transition cold storage
80,24738075,1.0,update redirect link can update redirect this permanent url reflect change dashboard future
81,24736906,3.0,rca container registry deployment delete production kubernetes cluster summary an engineer work locally mistakenly connect production cluster the engineer send command intend local development cluster instead send production cluster result deletion kubernetes deployment object container registry this bring service endpoint pods available haproxy service start send http know the server currently unavailable request bind container registry impact metrics start following what impact incident container service who impact incident anything request image container registry customer ci job etc how incident impact customer this prevent customer upload image download image container registry how attempt access impact service feature as see chart sustain request rate container registry rate request second how customer affect todo how customer try access impact service feature todo include additional metric relevance source provide relevant graph help understand impact incident dynamic source logs stackdriver detection response start following how incident detect alert do alarming work expect yes how long start incident detection minute how long detection remediation minute be issue response incident the recreation deployment use default setting hpa minimum replica it take roughly minute container registry scale original state prior incident at point end user see performance degradation timeline utc kubectl delete gitlab registry execute production cluster xx utc alert indicate registry endpoint alert engineer xx utc incident call start utc engineer recreate container registry deployment xx utc alert clear root cause analysis an engineer perform local testing item directly relate name schema associate object match production cluster due nature testing remove bit context immediately signal engineer command send undesired kubernetes cluster the engineer perform command kubectl delete deploy gitlab registry match deployment production cluster this deletion remove deployment replicasets pods associate the service object remain long run pod place haproxy healthcheck signal failure remove gke registry backend leave haproxy return http incoming request what go start following we able diagnose issue relatively quickly with use kubernetes able utilize tooling bring container registry work state quickly what improve start follow use root cause analysis explain improve prevent happen engineer unnecessary access production cluster be improve detection time detection cluster api accessible service account act inside ci cd job cluster allow api traffic user a warning engineer act production cluster cause second guess command run locally an indicator shell engineer warn engineer cluster connect be improve response time response consider shorten alert time require trigger an alert dedicate monitoring pods replicaset deployment exist help diagnose fast be exist issue prevent incident reduce impact no do indication knowledge incident place no corrective action an epic create discuss way limit exposure production service order prevent future accident occur guideline blameless postmortems guideline s
82,24727158,3.0,stor gprd reboot stor gprd reboot utc open issue track repos zeroed file search repos cd opt gitlab git data ionice find se tmp result
83,24714776,1.0,help delete group webhook gitlab org the context we want delete webhook image the however web ui could help delete then help create i sure edit delete create new
84,24701290,3.0,potential bug gke module relate node pool instance count two recent event bring issue gke module during testing engineer unable node create node participate cluster and time engineer add additional node pool time engineers run issue the probable cause removal option utilize issue investigate test fix wrong
85,24701150,3.0,git data loss gitaly server restart when user push git repository gitaly server host repo shut reason push fail leave byte file disk when server come subsequent push repository fail manually intervene the recent infrastructure issu it discuss ee tracker some suggestion true git option set data journal mount option prevent zero file crash now datum order the ultimate solution gitaly ha need mitigate problem ready use issue discuss idea
86,24679182,1.0,change account owner current account owner john northrup in order transfer ownership need choose team member valid payment method file owner can check somebody payment info setup arrange ownership
87,24669320,2.0,stor gprd reboot the server reboot utc the follow repository byte file need delete push succeed ea these file locate opt gitlab git data
88,24660552,1.0,add new sitespeed graphite datasource grafana to finalise completely new setup live monitoring overall frontend performance sitespeed i need setup new graphite datasource grafana instance this finalise issue i add credential server address sitespeed graphite new it great new datasource sitespeed new as standard grafana member i add new dashboard problem so far i test local grafana instance
89,24643616,2.0,thano prometheus host label label currently thanos prometheus type label this mean ironically monitoring alert infrastructure work host this fix
90,24643608,1.0,discuss oncall shift make spreadsheet visualize issue discuss infra team retro
91,24641857,2.0,add prometheus alert routing issue we use gitlab prometheus integration route alert directly issue
92,24633485,1.0,decrease staging we setting gstg consistent production we align example allow issue migration surface early try production while global gstg individual user setting different root gitlab psql gitlab password user gitlab psql ssl connection protocol cipher ecdhe rsa gcm bit compression type help help row the set individually gitlab user table gitlab f f f f we reset align production since staging lot small production catch issue topic
93,24627527,1.0,temporary database testing instance pshutsin relate provision database instance restore pipeline test datum migration
94,24618359,1.0,cert renewal expire day renew sslmate need deployment
95,24574593,2.0,add detailed network monitoring postgresql patroni host as i propose we want consider have node send receive continuous stream low bandwidth traffic monitor something like permanently run iptraf this m resolution time available metric prometheus with able clearly network hiccup i like raise topic prometheus network monitoring good datum point available low resolution second resolution desire dependent traffic i propose setup system inject traffic host constant rate export metric prometheus this way performance network vs expect it consist following each host monitor talk host cluster ideally zone zone region a small traffic constant exchange think like dd zero pv limit nc etc for example export network performance metric prometheus ideally second resolution this mechanism allow detect measure precisely network hiccup disruption it help clearly diagnose recent patroni failover believe cause coordinated network disruption cc
96,24564067,2.0,connect gitlab service project environment variable the environment build gitlab services need ability associate environment variable project group gitlab application some service environment require variable connect master review app some need automatically set variable arbitrary project group some need set variable instance staging op dev instance the location project connect configurable terraform variable file folder gitlab services project the absence variable result attempt set variable project group initially variable initial need set cloudsql module possibly database variable when set variable additional attribute need configurable scope specify environment variable apply mask hide protect variable state completeness
97,24563892,3.0,connect gitlab service gke cluster gitlab project the environment build gitlab services need ability associate gke cluster project group gitlab application some service environment automatic kubernetes integration connection master review app some need automatically connect arbitrary project group some need connect instance staging op dev instance the location project connect configurable terraform variable file folder gitlab services project the absence variable result attempt connect gke project group the variable need connect the ca cert gke cluster define gke module the endpoint ip gke cluster define the service token gitlab admin account define gitlab admin service account additional setting environment scope come configuration variable environment file default base domain come configuration variable eventually dns module cluster name default branch tf variable this work ephemeral environment functional merge master assign mr review
98,24539695,3.0,stop number host naming convention lately i observe considerable amount thought attention give replace host name sequence pool count this pattern require destruction rename possible host replace new host create the extra work create thought attention give operation distract goal destroy replace broken vm cow cow cow cow cow cc i go assign i know come way randomization pool silly name
99,24529320,3.0,fix replication dr delay dr archive db postgre dr db gprd postgre dr db gprd stop replicate failover wrong timeline
100,22135693,5.0,create similar alertmanager configuration instance run alertmanager today alertmanager consist file the chef contain secret datum store inside gkms we need determine way recreate workflow modification set file pull chef desire mechanism deploy alertmanager inside kubernetes reference doc related issue
101,22135116,3.0,thanos reconfiguration at time write issue stable prometheus operator helm chart support version thano run rest infrastructure the use thano prefer ensure longevity datum time prometheus able store datum gke cluster utilize issue discuss investigate create actionable issue ensure thano enable desire configuration gke prometheus configure preferred manner x check version late version stable promethue operator chart support hopefully version thano deploy vm kubernetes infrastructure x create separate service account currently service account mangle user datum store metric datum good security practice x configure thano store metric cloud x turn data retention prometheus week single day reference doc
102,22133544,1.0,fix syntax break terraform i try plan gstg get bunch error luckily form error invalid attribute module alert line resource default an attribute require dot easy fix hopefully smooth path terraform relate
103,22126041,5.0,poc database backup base snapshot postgre basebackup currently take wal e hour this process take hour consume lot io primary we wal e basebackup replica wal e support during weekday daily backup take hour complete it increase load io wait primary extraordinarily run high traffic time additionally time time run issue wal e uploading gcs see example there effort use wal g wal e support take basebackup replicas however currently wait wal g release properly support gcs even wal g place high mttr restore backup a tb backup fetch gcs extract database host bad case replay hour worth wal basebackup catch change basebackup start mttr likely hour bad depend wal need apply proposal in order reduce mttr hour want use gce disk snapshot basebackup in detail provision dedicated replica consume wal archive run generally stay date production cluster the replica participate ha cluster at regular interval let hour tune stop postgre replica flush disk ensure snapshot consistency coordinate gce api grab snapshot datum disk start postgre let catch upstream cluster in order restore backup create new disk late snapshot instantiate database instance it configure archive replica consume wal archive recovery point then promote primary start serve connection benefit greatly reduced mttr disk create snapshot minute max need ship tb network disk snapshot incremental cost huge concern high frequent backup new database instance create easily snapshot this helpful recover cluster we create instance snapshot let catch direct replicas talk choose primary remove nightly io bottleneck take backup primary proof concept implementation tbd note it helpful data directory live disk snapshot with datum directory drop easily mount database instance
104,22098581,2.0,use pushgateway indicate pg backup start finish currently capture wal e basebackup message upload use pgbasebackup alert indicate base backup happen however alert notify backup fail process because miss base backup day alert we use pushgateway backup indicate backup start completion instead
105,22095890,2.0,convert gitlab com repo helm template helm local tiller as discuss recent meeting note capture helm tiller skarbek let utilize local tiller let convert gitlab com project utilize local tiller create issue we decide safe utilize hook provide helm to avoid security implication tiller utilize plugin provide local tiller we currently test successfully method monitoring repo need convert gitlab com repo system
106,22094961,1.0,revoke vendor access snowplow legacy bucket fishtown write datum gitlab com snowplow data bucket remove access
107,22075037,1.0,flappy flappy presence sporadic error i believe mainly m irate sample i think need evaluate i think new synthetic m useful smooth well strong consistent signal thing go wrong it probably need small number tbd smoother signal sufficient
108,22072839,2.0,deploy update prod canary host the secops team like proceed roll update version uptycs osquery production canary host this new version significant performance improvement embed rocksdb so far staging see significant performance impact like previous version for info context
109,22071747,4.0,test restore packagecloud backup new procedure packagecloud back database xbstream it restore backup new backup test documentation update
110,22071538,1.0,increase retention period packagecloud backup we currently packagecloud backup day this decide limited disk space giant database we increase retention period backup day
111,22071485,1.0,remove cron task backup packagecloud packagecloud able upload backup successfully large database it able successfully upload backup remove cron job
112,22046171,3.0,register mailroom service consul necessary able dynamic inventory consul gitlab org release
114,22040745,2.0,discuss formalise recommendation manage gke google kubernetes engine patching upgrade follow implication discuss write link relevant place gl infra expect people spin gke cluster gcp project start position discussion if spin gke cluster gcp project gl infra directly involve provision project responsible maintenance gl infra advise assist explicitly request pro actively look thing the simple way avoid issue turn automatic upgrade gke cluster master nodes detail this require consider uptime requirement thing run gke critical business gl infra look require explicit discussion handover
115,22033597,1.0,update credential salesforce hi team cred salesforce recently change integration break can update chef vault new credential specifically need update attribute the value store subscription portal share vault let know need access thank close gitlab org customer gitlab
116,22020540,8.0,automate vault restoration testing ci cd implement regular backup service restoration test vault
117,22020396,8.0,rollout infrastructure secret vault define strategy migrate secret gkms chef vault hashicorp vault move secret piece piece maybe adapt shim layer secret management chef try prevent have live world extended time period test service rollout service group service group we eventually split multiple issue clarify migration strategy
118,22019719,3.0,create runbook vault create runbook vault add vault service catalog
119,22019610,3.0,configure alerting vault configure threshold alert vault alertmanager
120,22018400,4.0,be confident metric dashboard alert relate zfs git storage node we want sufficient metric able diagnose debug issue relate filesystem storage zfs back git storage node alert condition satisfactory there side zfs specific metric check exist metric relate persistent disk i o scratch be alert from suggestion
121,22005564,2.0,add service catalog we add service catalog remind need care update example
122,22005472,1.0,register deploy service consul in order able dynamic inventory consul gitlab org release need register service deploy node able differentiate
123,22002625,1.0,investigate automate issue creation discourse upgrade notification we receive email notification op contact account new version discourse available easy ignore lose track fall victim bystander effect if possible look automate creation upgrade issue email easily consistently ensure notification receive primary workflow gitlab todo
124,21980690,5.0,failure access require bastion bastion dr environment failure examples successful bastion access production redis host fail bastion access environment dr refer success connect production bastion sh ssh libressl read configuration datum config config line apply option config line apply option read configuration datum ssh ssh line apply option connect port connection establish identity file type identity file cert type identity file type identity file cert type identity file type identity file cert type identity file type identity file cert type identity file type identity file cert type local version string remote protocol version remote software version match pat compat authenticating nelsnelson send receive kex algorithm kex host key algorithm ecdsa kex cipher mac implicit compression kex cipher mac implicit compression expect server host key ecdsa host know match ecdsa host key find key rekey block send expect receive rekey block will attempt key rsa agent will attempt key rsa akzlxbyvqrw will attempt key will attempt key will attempt key will attempt key receive server sig receive authentication continue publickey next authentication method publickey offer public key rsa agent server accept key rsa agent authentication succeed publickey authenticate channel new client session request session enter interactive session pledge network rtype send environment send env lang welcome ubuntu lts gnu linux gcp documentation management support get cloud support ubuntu advantage cloud guest package update update security update new release lts available run release upgrade upgrade system restart require nelsnelson channel rtype exit status reply channel rtype eow reply logout channel free client session nchannel connection closed transfer send receive byte second bytes second send receive exit status ssh libressl read configuration datum config config line apply option config line apply option read configuration datum ssh ssh line apply option execute proxy command exec ssh identity file type identity file cert type identity file type identity file cert type identity file type identity file cert type identity file type identity file cert type identity file type identity file cert type local version string remote protocol version remote software version match pat compat fd set fd set authenticating nelsnelson reading file find key type ecdsa file load key prefer hostkeyalgs ecdsa cert send packet type send receive packet type receive local client kexinit proposal kex algorithm host key algorithm ecdsa cert cipher cto cipher stoc macs ctos etm macs stoc etm compression cto zlib compression stoc zlib language cto language stoc reserve peer server kexinit proposal kex algorithm host key algorithm ssh rsa rsa cipher cto cipher stoc macs ctos etm macs stoc etm compression cto zlib compression stoc zlib language cto language stoc reserve kex algorithm kex host key algorithm ecdsa kex cipher mac implicit compression kex cipher mac implicit compression send packet type expect receive packet type server host key ecdsa reading file find key type ecdsa file load key host know match ecdsa host key find key send packet type mode rekey block send expect receive packet type receive mode rekey block will attempt key rsa agent will attempt key rsa akzlxbyvqrw will attempt key will attempt key will attempt key will attempt key send packet type receive packet type receive server sig receive packet type ssh userauth receive send packet type receive packet type authentication continue publickey start pass different list publickey preferred publickey publickey remain prefer publickey next authentication method publickey offer public key rsa agent send packet type send publickey packet wait reply receive packet type server accept key rsa agent rsa signing rsa send packet type receive packet type authentication succeed publickey authenticate proxy channel new client session channel send open send packet type request session send packet type enter interactive session pledge proc receive packet type rtype receive packet type channel callback start d channel request pty req confirm send packet type send environment ignored env shell ignored env lscolors ignore env less ignored env ignored env ignored env ignored env ignored env ignore env ignored env ignored env ignore env ignored env pwd ignored env logname ignored env manpath ignored env ignored env home send env lang channel request env confirm send packet type ignore env securitysessionid ignore env pythonstartup ignored env tmpdir ignored env clicolor ignored env ignored env fignore ignore env term ignore env user ignored env shlvl ignored env ignored env ignored env path ignored env ignored env gopath ignored env ignore env ignored env channel request shell confirm send packet type channel callback channel open confirm rwindow rmax receive packet type type d pty allocation request accept channel channel rcvd adjust receive packet type type d shell request accept channel welcome ubuntu lts gnu linux gcp documentation management support get cloud support ubuntu advantage cloud guest package update update security update system restart require production nelsnelson exit receive packet type channel rtype exit status reply receive packet type channel rtype eow reply channel rcvd eow channel sock wfd efd write channel input open closed receive packet type channel rcvd eof channel output open drain receive packet type channel rcvd close channel send datum close logout channel send datum close channel obuf channel sock wfd efd write channel output drain closed channel dead channel gc notify user channel gc user detach channel send close send packet type channel dead channel garbage collect channel free client session nchannel channel status the follow connection open client session fd sock cc send packet type fd connection closed transfer send receive byte second bytes second send receive exit status failure connect dr bastion sh ssh libressl read configuration datum config config line apply option config line apply option read configuration datum ssh ssh line apply option connect port ssh libressl read configuration datum config config line apply option config line apply option read configuration datum ssh ssh line apply option execute proxy command exec ssh identity file type identity file cert type identity file type identity file cert type identity file type identity file cert type identity file type identity file cert type identity file type identity file cert type local version string relevent config section sh gcp production bastion host host preferredauthentications publickey user nelsnelson gprd box host preferredauthentications publickey proxycommand ssh gcp staging bastion host host preferredauthentications publickey user nelsnelson dr box host preferredauthentications publickey proxycommand ssh all example attempt solely stanza find config snippet
125,21934822,1.0,health check fail inactive web node report slack we new node maint status yesterday haproxy health check but look like consider fact glb health check report metric go ahead stop node next week week june drive couple different effort separate api traffic scale pgbouncer depend outcome decide scale web fleet i like use node right away if proper terraform mr remove node gcp
126,21934056,1.0,discussion notification terraform module version bump when make change terraform module long gitlab com infrastructure need update we need way reliably notify primary user module update some initial idea an mr template anyone interest add line the codeowner file i sure work case update readme link place need change who responsible change the merger do open issue or submit mr someone well idea this probably effect eventually touch input well we want think similar chef
127,21927335,2.0,increase ttl package cloudfront currently default ttl packagecloud cloudfront distribution set since package unique name increase day order well cache performance
128,21926649,2.0,document cloudfront packagecloud we need document packagecloud cloudfront look like
129,21917102,5.0,configure monitoring vault configure vault monitoring prometheus
130,21916659,8.0,configure vault service configure vault service unseal key backend role acls
131,21916317,3.0,setup automate vault storage bucket backup develop automate script plan back bucket versioning
132,21916186,2.0,deploy gcs bucket vault storage deploy gcs bucket vault storage
133,21915928,3.0,create cluster vault terraform create cluster vault terraform deploy nodes we probably use
134,21915790,3.0,document vault gcp project service account document service account vault gcp project create make sure iam permission scope right restrict minimum
135,21915593,3.0,create gcp project vault create new gcp project vault resolve ip addressing routing project look cross project dns topology beta gcp gke maybe use project setup right permission etc
136,21894026,1.0,update grafana sync key per need update key chef use update key dashboard sync grafana api key item team vault
137,21889446,3.0,list know potential gotcha elasticsearch indexing gitlab projects as sre i want understand known anticipate type operational issue come upcoming addition elasticsearch indexing gitlab feature well interpret new observed behavior change workload relate subsystem background while listen today excellent elasticsearch deep dive presentation video slide note question i realize area introduction es indexing significantly change workload scale requirement relate subsystem es nod gitaly storage nod sidekiq pool postgres etc and course new technology adoption new failure mode aware es node dead shard redundancy es quorum policy write read impact es availability upstream client etc this issue aim collect list know topic aware it discussion brainstorming resolve topic raise these topic eventually add runbook actionable
138,21889206,2.0,need update version license customer restore backup currently data team pull datum restore backup version license customer db we go start pull different kube cluster get error we need update start pull new cluster just let know need know provide help cc
139,21883491,2.0,support different version terraform terraform ci cd test currently test use global terraform version specify root repository lint test for early version terraform fmt ok but new version terraform format way file fail specify old global version since environment version terraform specify update test use specific version this allow use new version have drag entire repo new version terraform
140,21863206,3.0,automate grafana datasource provisioning public dashboard since grafana support provisioning yaml file disk we add support auto provision correct datasource public dashboard server see upstream doc
141,21857648,7.0,update query source global grafana dashboard pull metric we dashboard like datum source populate per slack discussion change query source global work see before after comparison before after
142,21855579,2.0,register gitaly service consul in order able dynamic inventory consul need register gitaly service grpc health check
143,21851219,1.0,re create public dashboard node while apply terraform change node leave inconsistent state console log filesystem error relate datum log volume for simplicity node remove recreate chef converge run finish configure filesystem application setting
144,21827850,1.0,postgre restore instance os login authentication break the gce instance create use os login authenticate user ssh this currently break maybe work we fix
145,21821667,2.0,rca high rails error rate front end receive alert elevated error rate rail return error high rate git traffic impact user likely see error affect gitlab rail git api web team attribution minute downtime degradation minute impact metrics start following what impact incident portion api web git fleet register load balancer result increase error rate service who impact incident user external customer how incident impact customer error attempt api call perform git operation https how attempt access impact service feature how customer affect how customer try access impact service feature include additional metric relevance provide relevant graph help understand impact incident dynamic detection response start following how incident detect engineer receive pagerduty do alarming work expect yes alert ignore initially concurrence ongoing deployment how long start incident detection how long detection remediation be issue response incident bastion host access service available relevant team memeber page able timeline utc deployment start hotpatch utc deployment fail engineer experience issue add remove backend server haproxy utc deployment restart utc evidence grafana imbalance distribution connection server utc instance alert fire increase error rate utc engineer alert increase error rate utc alert acknowledge engineer utc alert auto resolve utc alert recur utc alert auto resolve utc alert recur utc alert acknowledge engineer utc investigation start utc inconsistent load distribution identify probable cause utc imoc notify slack utc imoc page slack utc impact backend identify utc team begin work register affect backend server haproxy load balancer utc git service fully restore utc api web service restore root cause analysis why the end fleet experience elevated error rate disruption service why portion api web git fleet register load balancer why register properly deployment why hot patch deployment pipeline cancel why pipeline appear hang why script incorrectly detect nodes status what go start follow identify thing work expect any additional out go particularly what improve start follow use root cause analysis explain improve prevent happen be improve detection time detection be improve response time response be exist issue prevent incident reduce impact do indication knowledge incident place corrective action list issue create corrective action incident for issue include follow bare issue link issue label action include estimate date completion corrective action incldue name individual own delivery corrective action guideline blameless rca guideline s
146,21813808,2.0,add generic linux observability tool host as sre dbre i want linux host include tool ad hoc observation i collect short term metric investigate behavior impractical scope general purpose monitoring background prometheus provide variety metric collect periodically this serve purpose however need granular narrowly scope instrumentation ad hoc investigation example polling disk i o statistic second interval helpful analyze suspect burst i o contention smooth frequent polling interval measure variation memory access latency vm run memory datum store redis reveal opaque root cause transient query response time spike measure trend tcp connection open close event help inform tune kernel tcp stack gracefully handle traffic spike it lead improve general purpose diagnostic monitoring alert approach saturation certain finite resource tcp connection table pool available client port etc in issue let build wish list tool like available linux host to start list slack discussion iostat per block device i o statistic include queue depth busy mean read write latency etc sysstat provide sar utility give wide variety usage statistic system wide specific pid linux tool lightweight tracing facility perf suite tool perf perf perf mem perf trace perf ftrace etc ebpf support recent kernel iftop network flow show remote ip currently network throughput ifstat vmstat network interface polling network throughput interface
147,21809882,3.0,investigate git storage node disk space expect some gitaly storage node significant gap actual disk space versus table report try find dominate discrepancy time box discovery effort soon plan significantly change git storage architecture this investigation aim understand nature magnitude measurement error matter cause systematic accounting error improve example host stor gprd nfs actual disk space tb report disk space tb for background discrepancy
148,21803369,2.0,time offline upgrade we offline upgrade we know long go figure downtime acceptable
149,21801140,1.0,update build runner setting lose support version gitlab runner we need update build runner cache setting new format roll runner specifically need change vault secret the build runner omnibus package throw deprecation warning startup use runner cache parameter the configuration deprecate remove an mr update visible setting but access secret store vault need update they need move nest level key
150,21768290,5.0,connect monitoring kubernetes exist infrastructure determine connect exist monitoring infrastructure new kubernete cluster monitor solution pick conversation leave
151,21768260,5.0,deploy monitor infrastructure inside kubernetes without kubernete cluster need way monitor utilize issue perform deploy necessary service application monitor cluster service run pick conversation following location
152,21768064,1.0,update consul our current version allow multi tag querying need example select canary api node tag cny api that feature introduce impediment get late version break change affect see
153,21658381,6.0,cleanup chef acl permissions in work grant read access chef resource discover default permission model incredibly wide user chef server add user delete node update role update environment etc this need clean lock default user access gitlab chef organization grant
154,21653164,3.0,camo proxy secure configuration from we discuss rate limiting concurrency request timeout camo proxie request and decide apply limit camo ingress load balancer camo instance egress i form threat model cuff want avoid make camo attractive anonymize proxy attacker abuse victim domain prevent attacker easily saturate pool camo instance dos
155,21652190,4.0,packagecloud upgrade required our version packagecloud stop work june this aws deprecate support api packagecloud utilize an upgrade available we upgrade date the email i receive aws deprecate aws api june packagecloud enterprise early currently use new version packagecloud enterprise version high use aw cloudfront serve package object instead aws directly this upgrade significantly improve package download speed avoid api deprecation to continue packagecloud enterprise june need upgrade version include support cloudfront on packagecloud enterprise version optionally enable disable cloudfront upgrading packagecloud enterprise allow create cloudfront distribution test aws deprecate api week note release packagecloud enterprise version require cloudfront create new cloudfront distribution minute long we strongly urge customer upgrade create cloudfront distribution serve package aws deprecation take place this upgrade allow seamless switch downtime wait upgrade packagecloud enterprise june certainly result downtime need wait creation cloudfront distribution enable downloading package object user follow upgrade instruction once upgrade complete user follow cloudfront setup instruction contact support question problem happy packaging
156,21647603,2.0,evaluate viability magnetic hdd instead ssd on slack mention possibility save money cheap slow traditional hdd we evaluate possible production load rely arc ram nvme local ssd
157,21647032,1.0,upgrade ruby
158,21639495,5.0,remove ci infrastructure digitalocean do serve year have autoscale shared runners but transition gcp connect transition ci infrastructure backup environment after migrate ci infrastructure gcp add update increase efficiency environment configuration simple so moment configuration ci infrastructure do gcp during year transition do environment twice time problem gcp environment and time unsuccessful the big load get migrate gcp different configuration infrastructure do environment current shape usable ci infrastructure at moment lot alert noise relate cache server infrastructure totally we backup strategy but think quickly ci infrastructure region gcp idea like with say i think time send ci infrastructure do deserve retirement what digitalocean ci infrastructure termination x cleanup prometheus configuration x prometheus server role node job list x remove x prometheus server role prometheus job list x remove x role ci node job list x remove gitlab runner consul x remove runner cache server x role ci node job list x remove x role blackbox job list x remove x remove x remove x remove x remote x role share runner job list x remove x remove x role share runner gitlab org job list x remove x remove x role private runner job list x remove x remove x role prometheus job list x remove x role ci prometheus fleet job list x remove x role x remove job x remove job x remove job x remove job x remove runner cache registry job x remove runner cache server job x remove runner cache minio job x op infra prometheus server role node job list x remove x remove x remove x remove x remove x remove x remove x remove x remove x remove x remove x op infra prometheus server role prometheus job list x remove x op infra prometheus server role thano job list x remove x chef cleanup x remove node x x x x x x x x x x x x remove role x gitlab runner srm x gitlab runner gsrm x gitlab runner prm x runner gitlab com x runner cache server x prometheus blackbox runner cache x gitlab runner prometheus x gitlab runner consul x gitlab runner consul firewall x gitlab runner consul x remove chef vault x gitlab runner srm ci prd x gitlab runner gsrm ci prd x gitlab runner prm ci prd x runner gitlab com ci prd x gitlab runner prometheus ci prd x gitlab runners consul client x gitlab runners consul cluster x unregister runner x x shared runners x id x id x id x id x x shared runners x id x id x id x id x gitlab org group runners x id x id x gitlab com group runners x id x id x chart group runners x id x id x digitalocean resource cleanup x remove node x gitlab prod team x x x x x x x volume x volume x x x x cleanup runner base image x gitlab team x gitlab ci team x update base image building configuration stop do x x grafana cleanup x remove panel x remove panel x remove panel x remove panel x remove cache server row x remove hang droplet clean row x remove droplet zero machine clean row x remove cache server variable x remove hang doplet cleaner variable x remove droplet zero machine clean variable x remove digitalocean row
159,21614481,3.0,give read access chef we access request open read access chef gitlab com access gather issue look like appear read group need knife acl command finish make group test
160,21613757,2.0,allow user staging group ssh staging nodes role include stage allow ssh access user staging group nessus staging user allow authenticated scan
161,21610917,2.0,centralize include terraform module pipeline config i create new project initial intent centralize common element pipeline job run principle easily extend aspect code infrastructure manage x create base template terraform module pipeline x validate template ci configuration terraform module pipeline x document pattern x propagate change remain module ops mr get pipeline template working pipeline pipeline pipeline pipeline pipeline roll template usage terraform module project x x cloud x cloud x database backup x generic x generic stor x generic stor x generic sv x generic sv x x x monitoring x monitor x x x x static object x storage x tcp x x web cloud cloud sql cloud database backup generic generic stor generic stor generic sv generic sv monitoring monitoring static object storage tcp web
162,21609028,3.0,new bucket greenhouse datum extract we go bi connector greenhouse datum they nightly csv dump i need bucket read write access datateam greenhouse extract thumbsup i need access key secret key help
163,21603010,1.0,update secrets management section production architecture handbook as result stand vault production architecture change please update relevant documentation diagram include
164,21602748,3.0,secrets management vault design document review require change design document vault add engineering infrastructure section handbook you require ask question document that exploration research consider issue please close task design agree link public work ready begin
165,21602281,1.0,benchmark syslog vs config echo syslog echo stderr echo stderr echo csvlog echo csvlog echo stderr
166,21600891,3.0,allow user op group ssh op base runner host op base runner role set ci production ci op production allow ssh access user op group nessus op user allow authenticated scan
168,21590640,2.0,benchmark relevant candidate config benchmark following configuration all ubuntu xenial gcp kernel zol ssd zpool local ssd hdd hdd zpool local ssd single ssd local ssd ssd
169,21582719,2.0,camo proxy monitoring alerting runbook camo proxy need basic monitoring vm health start point require think expansion service responsive load balancer pageable service responsive node alert page may require pre prepared encoded url request ensure end end functionality suggest static like logo image wo change url expect case need camo proxy work wo alert the internet completely wo notice alert specifically storm alert ideally need sort graph throughput usage unclear stage implement mtail custom exporter in addition alert response runbook need include debug step verify correctness determine request fail likely end respond request ask url respond
170,21578062,5.0,create gcp project groups there request recently create gcp project group work currently project create ad hoc standardization set the project create standardize process group isolated place work have infra team request advantage include cost tracking great accountability team it reduce workload infrastructure team non production work by empower team manage isolated sandbox safe way benefit the follow list update issue evolve include project create initially project admin geo rachel nienaber rnienaber release darby frey dfrey verify elliot ruston erushton customer success joel krooswyk jkrooswyk professional daniel peric dperic
171,21576304,3.0,whitelist atlasssian ip address space api calls atlasssian get rate limit end ha proxy node api access this whitelist request infrastructure integration atlassian provide link ip addresses atlassian cloud use derive content whitelist http jq
172,21572815,2.0,alert zpool run low disk space this issue placeholder need work sre receive non paging alert threshold determine zpool space reach be mindful reservation filesystem interact the threshold choose low unlikely run disk space awake pick non paging alert consider add predictive alert prometheus send alert predict run zpool space week consider add page alert high emergency threshold discuss team make sure grafana dashboard place work inspect zpool utilization
173,21568319,1.0,figure deal registry service memory issue kubernete currently job name registry restarter run day vm fleet i wonder work exactly kubernete i assume service crash restart reason restarter job prevent error it carefully drain node haproxy restart service daily restart job registry memory
174,21565835,1.0,document step go cluster traffic direct container registry utilize issue track work necessary implement cluster application configuration move traffic cluster this necessary future documentation mean place consolidate note drive infrastructure component mesh
175,21565766,1.0,modify pre registry lb point new gke registry now new cluster stand container registry run inside let point lb nod registry step configure haproxy enable
176,21565111,1.0,workload use direct download party build image if party site reason fail job unrelated change test pipeline we instead mirror fork project create image utilize prevent susecptable failure reference conversation
177,21565004,1.0,replicas miss stage patroni cluster currently instance patroni cluster gstg root gitlab patronictl list cluster member host role state lag mb pg ha cluster leader run pg ha cluster run on node postgre run currently this relate network issue
178,21564842,2.0,decide initial zfs configuration after complete need propose initial set config option zfs git storage node current question need answer add list mean complete amount memory dedicate arc gb total size reservation filesystem zpool zfs recordsize use native compression if algorithm investigate disk space saving real git repository use native encryption require new zol should use quota repository filesystem tb this help sense node get exclusive snapshot bulk please use discussion thread comment digestable output issue need code instead comment reference come write infra since exist let send mr issue this configuration intend use single canary production refine see
179,21564603,4.0,new machine bootstrappe zfs mount filesystem git storage node stateful node class instantiate terraform generic stor module all machine stateful run version bootstrap script find among thing script detect gcp pd format mount configurable location our new git storage node use zfs instead possibly pd raidz configuration depend output write chef cookbook contain recipe idempotently format mount pds configurable way variable pass terraform module instruct recipe filesystem disk config use a zfs config support initially with regard zfs fine support vdev config agree single disk write new iteration bootstrap script format mount persistent disk add recipe beginning stateful node chef run list it interact pathologically exist bootstrap script format mount disk after issue close i able dial count new storage node declare usable zfs filesystem mount opt gitlab the recipe config inject behave today format single pd disk id google persistent mount for exist non git stateful role redi i able follow step sequentially cause data loss downtime add new recipe role runlist bump bootstrap script version new mount roll new node try thing staging production the main benefit move functionality chef improve testability speed iteration the particularly important roll change stateful node note bootstrap script detect format mount log disk move functionality chef essential worth move mount formatting relate appear simple place
180,21558990,2.0,forum functioning healthcheck endpoint despite forum today blackbox scraper think alive request return status code curl server nginx we need well healthcheck detect problem soon cc
181,21542819,3.0,camo proxy gprd deploy build nodes enable production optional security test
182,21542800,5.0,camo proxy gstg deploy build nodes enable staging security test
183,21542750,2.0,camo proxy chef implementation assume vm implement chef recipe deploy if ready way instead disregard
184,21542693,2.0,camo proxy deployment terraform implement terraform vm nodes external load balancer
185,21542678,5.0,camo proxy performance testing estimate expect throughput req s peak exist log datum available do ad hoc performance testing find limit choose camo proxy implementation look cpu ram mainly determine production node scale requirement
186,21542664,1.0,camo proxy security review design hello could review design work epic particularly interesting background info i think cover big concern security appreciate input if look ok let know proceed implementation
187,21542624,3.0,camo proxy whitelist configuration control on implementation issue discussion whiteliste object storage determine what exactly mean value need if need whitelisting how manage value nb strictly camo proxy config gitlab application setting whitelist domain pass camo proxy
188,21542595,3.0,camo proxy network design decide deploy precisely gcp assume vm detail network design implication important consideration include public ip presence consider nat gateway vs node public ip network control firewall et al prevent proxye internal network internal system camo ilb something
189,21542558,1.0,camo proxy deployment platform vm document choice reason
190,21542492,1.0,choose camo implementation there option original cactus camo arachnys camo fork cactus feature ensure need list quick evaluation criterion feature deployability maintenance etc do require run active testing consider paper base exercise
191,21542221,3.0,organize project folders gcp currently appear obvious organization scheme project folder gcp some level folder contain project nest folder lot there obvious schema thing i update diagram thing clear i especially interesting history location analytic gitlab analysis karu gitlab analytic customer success empty development frontend project distribution project infrastructure environment primary environments prod staging pre dr ephemeral review app environment temporary project security products gemnasium projects project op poc tool review app it operations empty market gitlab public migration testing projects obsolete monitor monitoring development sandbox user sandbox demo labs security project system gsuite app script project project location
192,21540178,4.0,rca june gcp related incident okay own rca i add question cc summary google major networking outage us east region affect entire infrastructure include affect all service team attribution external minutes downtime degradation minute downtime total degradation impact metrics start following all service completely minute postgres fail service restart huped the application degradation hour all user affect incident during minute entirely inaccessible user for remainder incident elevated rate error the main reason go completely patroni cluster instability failover application unable follow new primary the long period elevated rate error relate network instability many alert flap monitoring server unable reach server question there lot flapping redi cache primary secondary this affect availability application patroni network patroni network error rate error rate detection response start following the incident detect pagerduty alert alarming work expect alert overwhelming hard quickly determine because deluge alert important relevant alert alert indicate postgres fail lose noise it take minute beginning downtime alert begin response it take minute recover postgres failover site remain unstable provider outage our dashboard partially break known issue early week make difficult start response timeline utc patroni fail utc most grafana dashboard inconsistently work thanos issue utc pingdom return error utc diagnosis postgres failover utc services utc operational pingdom reporting service utc watching utc watch utc continue monitor google incident utc failover utc postgre fail utc error rate return normal utc restore tuple statistic run cluster wide analyze utc google post rca root cause analysis go minute instability course total hour why the application long reach database why postgres fail unexpectedly why there network instability cause cluster try fail multiple time why gcp major networking outage east region locate what go start following we find quickly problem multiple people jump help diagnose repair issue delegation duty expectation clear effective you update status page i restart sidekiq etc patroni failover successful if repmgr true disaster what improve start following we well automate tune database failover process application gracefully handle failover we try execute failover staging eventually production confident similar incident future cause complete outage case we automate run analyze table populate statistic we try prune curate alert massive deluge alert obscure problem hard relevant alert while directly affect production notice staging fall apart result it important fix asap sunday evening create follow issue immediately issue like follow monday corrective action some issue create reaction specifically incident correct action automate failover testing client connect old primary failover graceful patroni failover production incident relate thanos problem use virtual ip failover guideline blameless rca guideline s
193,21535659,4.0,find place cache repository archive andrew suggest look option cache object storage
194,21535555,3.0,ensure live trace work properly production relate
195,21535292,3.0,allow temporary personal snippet upload upload object storage this sort meta issue main work gitlab ce i issue update progress
196,21531642,1.0,workload iam service account need high level permission cluster as note job iam service account workload need ability create cluster role binding we create necessary role role bind user restrict namespace operate currently only gitlab namespace
197,21489189,1.0,split terraform environment our current repository structure terraform leverage symlink reference share code multiple environment this great keep code dry frequently occasion need deploy code individual environment follow deployment environment point future test new change staging example with environment reference file way change single environment require change apply environment hold deployment environment require target applie bring host issue allow configuration drift time as initial step resolve enable work automate deployment duplicate current file environment reference share asset symlink remove symlink likewise share content copy merge environment file symlink remove clean
198,21489151,1.0,automate tf plan environment as step automate terraform deployment need update automatically run tf plan environment the job conditional base change relevant portion repository job respective environment
199,21482307,1.0,own gcp project geo team the geo team like gcp project use instead gitlab internal this help manage member permission help track resource would infrastructure team able create
200,17457017,2.0,inventory catalogue file task fill gsic gitlab service inventory catalogue template service the task fill need reach team member order capture information service file sample template location question comments template reach
201,17457008,2.0,inventory catalogue elk task fill gsic gitlab service inventory catalogue template service the task fill need reach team member order capture information service elk sample template location question comments template reach
202,17456998,2.0,inventory catalogue contributors task fill gsic gitlab service inventory catalogue template service the task fill need reach team member order capture information service contributor sample template location question comments template reach
203,17456983,2.0,inventory catalogue consul task fill gsic gitlab service inventory catalogue template service the task fill need reach team member order capture information service consul sample template location question comments template reach
204,17456971,2.0,inventory catalogue console task fill gsic gitlab service inventory catalogue template service the task fill need reach team member order capture information service console sample template location question comments template reach
205,17456959,2.0,inventory catalogue blackbox task fill gsic gitlab service inventory catalogue template service the task fill need reach team member order capture information service blackbox sample template location question comments template reach
206,17456938,2.0,inventory catalogue api task fill gsic gitlab service inventory catalogue template service the task fill need reach team member order capture information service api sample template location question comments template reach
207,17443838,1.0,add gitter vpc terraform the current gitter vpcs manually create manage terraform add terraform import current resource state file
208,17443767,2.0,add terraform state bucket dynamodb table gitter aws account as step automate gitter terraform deployment need setup aws bucket remote state dynamodb table state locking
209,17441944,1.0,chatbot show production override run oncall prod return escalation manager primary override
210,17439306,2.0,setup network level access database replica elt load this second provide network level access vpc firewall rule archive replica gprd elt job runner see discussion detail network access archive replica from gitlab analysis project network limit access resource gitlab production say replica
211,17431133,1.0,review candidate questionnaire ps review complete
212,17410845,3.0,sslmate action require cert renewal jan we action require email check need new cert renew x exp jan x jan x jan x feb x jan x jan x jan x jan x jan
213,17407249,1.0,rackspace access kathy wang access review billing gcp contract like access rackspace she need rackspace portal account billing overview privilege
214,17396320,1.0,many node chef server connect there node chef server connect server pull datum these server remove list know host connect pull datum i put list discussion case state purpose here list jjn knife status min hour ago ubuntu hour ago ubuntu hour ago ubuntu hour ago redi chart win ubuntu hour ago redi chart win ubuntu hour ago ubuntu hour ago ubuntu hour ago ubuntu hour ago ubuntu hour ago ubuntu hour ago ubuntu hour ago ubuntu hour ago ubuntu hour ago ubuntu hour ago ubuntu hour ago ubuntu jjn
215,17374791,1.0,download late cookbook download late cookbook pin chef server since node rename dns cdn ssl configuration i think hostname node chef server need update follow process sanity check
216,17373801,1.0,chef service stale gprd patroni servers test run knife status min nodes effect hour ago ubuntu hour ago ubuntu hour ago ubuntu hour ago ubuntu hour ago ubuntu hour ago ubuntu hour ago ubuntu hour ago ubuntu hour ago ubuntu hour ago ubuntu hour ago ubuntu hour ago ubuntu hour ago ubuntu hour ago ubuntu hour ago ubuntu hour ago ubuntu hour ago ubuntu hour ago ubuntu hour ago ubuntu
217,17370685,1.0,stale chef runs production chef run stale production server
218,17347657,1.0,fix payment issue service see notification email payment fail link fix x dead man snitch scaleway x digital ocean
219,17347202,2.0,share learning terraform change dec rca part discussion rca december incident document idea sure conflict terraform force gcp console clean thing rca dec track propose note process create issue point discussion mr
220,17343226,2.0,review gitter security group rule assignment as follow need perform detailed review gitter network security rule vpc acls instance rule vpc security groups this issue track discussion review vpc security group rule assignment instance necessary ingress egress filtering relate mr implement change
221,17343190,2.0,review gitter vpc acls as follow need perform detailed review gitter network security rule vpc acls instance rule vpc security groups this issue track discussion review vpc acls boundary ingress egress filtering relate mr implement change
222,17328415,5.0,database reviews carry fixture new x x x x x x x x x ready approval x ready approval add composite primary key diff file x x x ready approval forgotten approve
223,17275391,1.0,new gcp project gitlab qa gke clusters maybe team we multiple use create kubernetes cluster gke right most people company include automate test gitlab internal project gcp company grow reach gcp limit regularly challenging resolve ip range limit multiple cluster we configure team use test gitlab kubernetes integration locally we create cluster automated qa test this regularly result failure exhausted resource eg break master require ask people clean eg manual process there way solve quota ip range exhaustion problem perhaps increase gcp quota help i understand need reconfigure gcp allow ip address fit range another challenge have company gcp instance difficult encourage good habit clean thing everybody realise clean thing actually create blocker team break master build if separate project qa break build configure team end block run gitlab internal configure team rely heavily work possibly need gcp project
224,17268993,1.0,version go like weekend for past couple weekend go offline bit we think reach installation customer ping app weekend bring currently server sit nearly available ram there swap available machine this single instance stack machine the current unicorn worker count set what short term alleviate problem sidekiq unicorn chef memory use offender i afraid try bump worker run system memory reference
225,17268522,8.0,design document service production inventory please fill idea follow design doc
226,17267568,3.0,create db replica zfs please create zfs replica staging production not patroni cluster first staging stable proceed prod we need create replica production dataset equal production future test environment similar scramble confidential datum process
227,17267510,2.0,design document postgresql bloat maintenance propose solution reduce bloat far input
228,17255905,3.0,discussion programming scripting language accept team objective the objective issue derive discussion programming scripting language accept want accept usage team this come feedback recent automation work python proposal suggest ruby go forward hence i want create issue gather feedback understand reasoning ruby go ultimately team datapoint start obvious our product ruby rail product the sre role job description list ruby go may fit we ruby script help infrastructure work discussion be answer inquiry simple we ruby go infrastructure relate work while data point present i know use bash script specifically call it possible inherent requirement know able use however go think adopt language python java replacement ruby go addition long job we want avoid get depth discussion compare language underlying implementation learning curve syntax performance community support adoption functionality feature etc i think well focus team want language well choice base experience infrastructure need some indicator i think possibly add python list we gcp google cloud officially support python library like ruby go kubernetes item radar officially support python library similar go but ruby list community support library if end ansible write python within gcp want write google cloud function currently support node js python note talk language performance syntax configuration implementation feature etc i sure degree ruby go experience rest team but transparency personal experience ruby go python java therefore decision stick group language mean i ramp however i happy end stick ruby go references
229,17246637,1.0,enable flag production there new feature async deletion upload feature flag disabled to sure feature work properly i like follow step production run rake gitlab cleanup prod current number orphan file enable feature wait day run rake gitlab cleanup compare number orphan file significant increase orhpane file sign upload deletion work expect run rake task cause dramatic io load prod server checking object store expect run probably hour i set date base slack discussion week prefer instead
230,17241002,2.0,grafana dashboard git sync break the grafana dashboard repo sync gitlab recipe the account secret config currently break sync fail
231,17220699,1.0,chat op access support hi the support team love able use chat op command the follow people active op account ready add appropriate group allow access would possible go x x x x x x x x x x thank
232,17219445,1.0,consider additional alerting add storage node recently storage node surpass usage new node spin so spin new node now need alert prevent surprise future let set overall alert file server breach warn these server subject standard alert policy file storage process remediation slow potential muck lot community we consider dedicated alert system this consequence alert today rebalance git repos node
233,17219364,1.0,sentry dsn op gitlab instance point incorrect fqdn the sentry dsn fqdn op instance point please remediate
235,17177255,5.0,github import starves pgbouncer connections sidekiq queue start grow observe github import directly correlate db lock starvation pgbouncer connection resource
236,17176496,2.0,bootstrap kernel update require successful chef run successful chef run require successful bootstrap kernel update when bootstrappe new system upgrade kernel desire version during bootstrap process kernel upgrade chef run if chef run fail reason include miss vault key kernel update fail system reboot leave system lock unbootable state this make difficult determine chef run fail fix chef problem cause recover try the machine need destroy create the kernel upgrade depend success chef run it robust succeed chef it end powered repeatedly try chef run that leave state fix chef credential problem result chef attempt succeed here relevant line bootstrap log feb geo db dr startup script info startup script gcp feb geo db dr startup script info startup script apt install linux gcp linux module gcp linux gcp linux gcp feb geo db dr startup script info startup script read package list feb geo db dr startup script info startup script build dependency tree feb geo db dr startup script info startup script read state information feb geo db dr startup script info startup script e unable locate package linux gcp feb geo db dr startup script info startup script e could find package glob linux gcp feb geo db dr startup script info startup script e could find package regex linux gcp feb geo db dr startup script info startup script e unable locate package linux module gcp feb geo db dr startup script info startup script e could find package glob linux module gcp feb geo db dr startup script info startup script e could find package regex linux module gcp feb geo db dr startup script info startup script e unable locate package linux gcp feb geo db dr startup script info startup script e could find package glob linux gcp feb geo db dr startup script info startup script e could find package regex linux gcp feb geo db dr startup script info startup script e unable locate package linux gcp feb geo db dr startup script info startup script e could find package glob linux gcp feb geo db dr startup script info startup script e could find package regex linux gcp feb geo db dr startup script info startup script dpkg query linux image linux header feb geo db dr startup script info startup script grep feb geo db dr startup script info startup script apt purge linux header linux linux gcp linux header gcp linux image linux gcp linux image gcp feb geo db dr startup script info startup script read package list feb geo db dr startup script info startup script build dependency tree feb geo db dr startup script info startup script read state information feb geo db dr startup script info startup script the follow package automatically instal long require feb geo db dr startup script info startup script grub common grub gfxpayload list grub pc grub pc bin common feb geo db dr startup script info startup script linux gcp os prober feb geo db dr startup script info startup script use apt autoremove remove feb geo db dr startup script info startup script the follow package removed feb geo db dr startup script info startup script linux gcp linux gcp linux header gcp feb geo db dr startup script info startup script linux gcp linux image gcp feb geo db dr startup script info startup script upgrade newly instal remove upgrade feb geo db dr startup script info startup script after operation mb disk space free feb geo db dr startup script info startup script read database database database database database database database database database database database database database database database database database database database database database database file directory currently instal feb geo db dr startup script info startup script remove linux gcp feb geo db dr startup script info startup script remove linux header gcp feb geo db dr startup script info startup script remove linux gcp feb geo db dr startup script info startup script remove linux image gcp feb geo db dr startup script info startup script remove linux gcp feb geo db dr startup script info startup script warn proceed remove run kernel image feb geo db dr startup script info startup script examine kernel feb geo db dr startup script info startup script run part execute kernel initramfs tool gcp gcp feb geo db dr startup script info startup script update initramfs delete gcp feb geo db dr startup script info startup script run part execute kernel zz update grub gcp gcp feb geo db dr startup script info startup script generate grub configuration file feb geo db dr startup script info startup script feb geo db dr startup script info startup script the link damage link feb geo db dr startup script info startup script remove symbolic link vmlinuz feb geo db dr startup script info startup script need run boot feb geo db dr startup script info startup script the link damage link feb geo db dr startup script info startup script remove symbolic link feb geo db dr startup script info startup script need run boot feb geo db dr startup script info startup script purge configuration file linux gcp feb geo db dr startup script info startup script examine kernel feb geo db dr startup script info startup script run part execute kernel initramfs tool gcp gcp feb geo db dr startup script info startup script run part execute kernel zz update grub gcp gcp feb geo db dr startup script info startup script update grub feb geo db dr startup script info startup script generate grub configuration file feb geo db dr startup script info startup script
237,17175318,1.0,investigate consul etcd service discovery the okr include operationalize service discovery current document refer consul service discovery mechanism in geo group conversation morning good point etcd start look like industry standard since move cloud native eventually eventually etcd kubernetes point solution support consul easy use currently surely value support single solution the geo team currently research issue the team look work start document consul well solution long term lie path convert exist consul use etcd
238,17174259,1.0,chatops sql explain unable connect pgbouncer the chatops sql explain work moment the error vendor bundle gem lib initialize error pgbouncer connect server this reproduce run explain select query as point slack ci variable control host connect currently ci job connect we want change setting stable hostname preferrably point replica
239,17368221,5.0,runner windows gitlab runner team description create new window environment late version gitlab runner instal proposal we need window machine follow software instal docker gitlab runner run shell executor powershell start build container run test gcp provide window machine follow pricing cost need calculate this machine provision manually second class beginning linux environment automate we use mixture terraform chef provision configure machine as point need windows server core windows server version late version window the follow software need instal machine openssh docker gitlab runner git link relate issue merge request reference
240,17158979,1.0,replication lag dashboard dr replica datum we separate dashboard replication lag dr replica archive delay all datum
241,17158754,1.0,workaround github importer bug apply database level fix affect project this time apply manual fix change issue work way
242,17158717,1.0,require script perform maintenance dry run output review this come multiple time incident i think worth make bit formal maintenance automate dry run mode display exactly script ssh command api call etc review corrective action
243,17155221,1.0,unresolvable service discovery config spamme sentry see invalid address initialize gitlab database new gitlab database gitlab database resolver gitlab database gitlab database gitlab database block level start gitlab database loop gitlab database block start resolve localhost gitlab database rescue gitlab database gitlab database resolver gitlab database gitlab database gitlab database block level start gitlab database loop gitlab database block start the config the error suggest local nameserver able resolve record
244,17152180,4.0,install configure gitlab application geo now close work environment the step application set way automatically update production diverge before enable geo work need
245,17152036,2.0,create group reduce gl infra spam currently folk draw attention infrastructure issue this large group lot alert not everybody need i propose split type tag small group it easy maintain focus tag issue interest we start split new sre group create functional area
246,17151849,1.0,chef automation break delete file when delete file chef repo knife try push delete file chef server this cause pipeline fail if change file upload repo commit wo work only change file push subsequent commit file long change we need filter delete file list change file upload chef server this mr address issue
247,17149085,3.0,fluentd repos break gitter host while update package cache gitter host today return follow error read package list w an error occur signature verification the repository update previous index file gpg error trusty inrelease the follow signature verify public key available w gpg error trusty inrelease the follow signature verify public key available w fail fetch w some index file fail download they ignore old one instead this update gpg key fluentd package repository we need download import new key reference manual process some discussion
248,17092334,5.0,ensure integrity terraform change apply merge corrective action use atlantis automate queue terraform deployment we implement atlantis distribute lock queue merge request incorporate work the description issue originally setup atlantis terraform deployment op instance more fundamentally issue intend ensure integrity terraform change run apply merge master utilize atlantis terraform enterprise way implement fifo queue merge request able leverage merge train gitlab product
249,17071059,1.0,clean orphan resource gitter environment the follow dns entry zone reference non existent resource ensure associate infrastructure remove terraform configuration clean ansible remove corresponding dns record resource beta environment
250,17070459,1.0,remove unused ebs volume gitter aws account a recent review ebs volume show follow volume unused unattached if long need delete volume
251,17070164,2.0,setup nat gateways gitter azs as follow gitlab com gl infra introduce nat gateway egress internal host gitter environment instead legacy vpn act nat instance the new gateway currently serve traffic subnet susceptible disruption amazon suffer outage availability zone nat gateway provision traffic disrupt azs now issue gitlab com gl infra mitigate need shore infrastructure provision additional nat gateway availability zone associate local private subnet azs
252,17069985,5.0,setup autoscale gitter bastion as follow gitlab com gl infra introduce bastion host access gitter environment instead legacy vpn the new bastion single instance redundancy fault tolerance ha quick mitigation gitlab com gl infra need shore infrastructure setup network load balancer ssh traffic bastion node setup autoscaling group bastion node setup launch configuration launch template bastion node provision deterministic host key bootstrap bastion node allow client rely strict host key checking
253,17061569,1.0,create file node as customary need create storage node we usually capacity way far usual point thus i pretty urgent
254,17044047,3.0,fix statement timeout spike relate github importer job this infra issue track work
255,17025630,2.0,transition temporary index release this track work able remove temporary index add production once ce mr release execute remove temporary index
256,17018615,3.0,postgre backup restore fail gitlab restore postgre gprd check deadmanssnitch anymore dec see notification slack
258,17009034,2.0,set db replica analytic pull edw organization in issue want setup access archive replica configure elt load in related issue go provide network level access say replica elt runner job appropriate firewall rule discussion summary the elt load need read permission replica fine no strict sla guarantee uptime availability performance query elt load expect finish minute statement timeout enforce accordingly our current method get update production datum warehouse follow update pseudonymizer config possible ping production team run pseudonymizer usually stan hope fail bomb write csv gcs import generate csv object storage snowflake this present number challenge precious time member production team time want update datum read like able set isolated replica update regular basis production datum we regularly version customer license dbs we need minute update delay week fine if follow model project runner whiteliste access i think simplify security footprint risk we explicit list field pull security approve update increase datum pull my time request stand end january we get okr relate modeling dotcom datum reasonable i know save time thought be setup fyi i try close time update production datum cc
259,17001284,2.0,investigate solution vacuum database prior failover during recent failover elect leader upon leader receive alert exceed threshold dead tuples this node start high percentage dead tuple use issue investigate resolve following question why secondary datum analyze table information dead tuple primary should alert consider high priority if problem prior failover ensure potentially elect secondary high percentage dead tuple
260,17001129,1.0,improve wal e alert walebackupdelaye during recent failover alert walebackupdelaye fire this technically false alarm wal e backup continue work datum prometheus look correct fire server begin process upload improve alert reduce alert fatigue incident this alert introduce noise stressful situation
261,16960452,3.0,do graceful db failover new patroni setup right patroni failover result error short period demote db promote we leverage fact client connection pgbouncer pgbouncer pause accept new connection however current setup utilize pause feature consider scenario master want failover we pause pgbouncer exist client connection complete close client try establish new connection forward ilb pgbouncer accept we failover we resume un pause pgbouncer now connection try execute read write query fail we need pause connection level ilb forward master possible i try mark patroni ilb backend unhealthy new connection fail immediately wait backend healthy ilb forward connection replicas backend mark unhealthy so i think need dedicated pgbouncer node ilb control connection flow high level pgbouncer node configure ilb fqdn run issue consul watcher update database configuration we step drop ilb altogether favor consul dns record master node assume production
262,16957200,1.0,large file generic sv group terraform module there m file generic sv group terraform module plugins terraform provider on fast connection difference slow internet provider problem i have prevent run tf init i suspect slow people notice fail i delete i sure reason do need and delete repo as note repo probably need file
263,16954549,1.0,prometheus gprd server get low space nodes space data disk mount prometheus
264,16940106,1.0,unable run terraform gcp unavailable i strange issue today i work dr environment run terraform command configure resource gcp reason i lose connectivity aws location i ping amazon address connect aws console all connectivity gcp unaffected this problem i work gcp file life i use terraform i like propose move file google storage it nice reduce dependency critical component like if happen high pressure incident extremely frustrating do know reason
265,16932917,1.0,prometheus staging environment crash on night december early morning prometheus keep crash staging environment we alert twice restart frequently source both server appear suffer damage wal file level warn caller component tsdb page wal tear fill zero prometheus prometheus data level warn caller component tsdb series reference both server stack trace log history goroutine select local src create local src goroutine select local src create local src goroutine select local src create local src goroutine select local src create local src goroutine select local src create local src snip goroutine io wait internal local src runtime internal local src internal poll internal local src internal poll internal local src internal poll local src net local src net local src local src bufio snip as time writing throw despite server have prometheus ready receive web request due currently page staging environment wake person
267,16912805,3.0,breach free space the run low tb volume use issue find problem resolve
268,16888035,3.0,allocate storage the filesystem fill weekend prevent user update website i manage free mb old unnecessary file need assistance familiar system file gitlab runner delete free space b extend volume azure c
269,16866440,2.0,restart minute this look like chef restart reason root grep master process ready i info master process ready i info master process ready i info master process ready i info master process ready i info master process ready i info master process ready i info master process ready i info master process ready i info master process ready i info master process ready i info master process ready i info master process ready i info master process ready i info master process ready i info master process ready i info master process ready i info master process ready i info master process ready i info master process ready i info master process ready i info master process ready i info master process ready i info master process ready i info master process ready i info master process ready i info master process ready i info master process ready i info master process ready i info master process ready
270,16861061,2.0,move terraform module ops instance rca corrective action fail inaccessible terraform module host op instance we module op instance source truth push pull like repository setup push mirror mirror
271,16860285,2.0,enable deletion protection flag gitaly page share server single point failure cause disruption delete
272,16855627,3.0,rca gitaly outage please note incident relate sensitive datum security related consider label issue mark confidential summary a brief summary happen try executive friendly possible affect gitaly storage nodes team attribution infrastructure minute downtime degradation minute impact metrics start following what impact incident all customer internal request git datum node unable service minute who impact incident external customers ci job how incident impact customer see impact how attempt access impact service feature how customer affect all how customer try access impact service feature include additional metric relevance provide relevant graph help understand impact incident dynamic detection response start following how incident detect pagerduty slack alerts do alarming work expect yes how long start incident detection approximately minute how long detection remediation minute be issue response incident bastion host access service available relevant team memeber page able yes terraform module repos delay run terraform stand compute node local copy timeline on production issue root cause analysis while work disaster recovery project region sre team unable use terraform properly remove node dr project region they choose gcp console perform delete terraform state good while search node illustration project switch gitlab dr gitlab production it clear project switch proceed remove gitaly compute instance gitlab production project at point monitoring start alert problem team start restore delete compute node illustration when attempt search gcp search bar partial match expect press enter execute search partial ip address case return result press enter result which appear search find however search line dropdown highlight project change at point go delete dr file node result gitlab production node delete gitlab dr nodes what go start follow identify thing work expect any additional out go particularly quicknes team jump zoom start mitigate issue we able restore affect infrastructure data loss what improve start follow use root cause analysis explain improve prevent happen be improve detection time detection be improve response time response be exist issue prevent incident reduce impact do indication knowledge incident place look different node name disaster recovery compute storage node name vs production mirror terraform repos environment module prevent issue access look enhance procedure delete production require set eye way prevent need interaction cloud console further automation ask double check perform delete corrective action add deletion protection gitaly servers move terraform module ops instance start practice incident response create list incident response scenario setup atlantis terraform deployment op instance change tf process prevent conflict send cloud console guideline blameless rca guideline s
273,16855222,5.0,put cdn discuss fastly but maybe consider cloudfront in case disabled transfer acceleration semi urgent change clear effect download speed
274,16841326,1.0,beefier gitlab share runner in note increase power runner build save lot time build happen the machine change test gcp switch dramatic change spec confirm in say we pool gcp use pay make sense low form box take time i think combine waste time wait ci employee pay pay spin high cpu count box and i agree can beef even have core machine website master pipeline great extend gitlab project probably help lot
275,16819225,2.0,update handbook db arch diagram patroni info i look reference repmgr like update patroni info
276,16800610,1.0,slow sentry error reporting slow i barely load issue i get error frig something go horribly wrong render page we use decent error reporting service probably fix soon unless error reporting service break that awkward anyway apologize inconvenience typeerror can read property d undefined be dashboard sentry cpu etc sentry error report
277,16795683,1.0,registry node scale recently spin extra node combat memory leak node leak memory create job daily restart service since problem systematic dependent scale let rid extra node reference
278,16787443,1.0,shutdown postgre pgbouncer node gstg gprd now patroni environment week need machine run just remove terraform module we snapshot production disk case
279,16784839,3.0,elasticsearch disk watermark monitoring we observe storage capacity usage elastic search cluster addition email notification elastic add alert reach low watermark node
280,16784424,2.0,add runbook instruction recover elastic search cluster add runbook instruction recover elastic search cluster case go storage
281,16784239,3.0,evaluate log retention need aggregate log cloud service provider like elastic bind cost eventual storage bandwidth capacity limit to cost reasonable research bandwidth storage limit elastic what current rate log send elastic what expect future type log need elastic need debug log how long need log type which log type store currently how reduce log send elastic do need look alternative elastic cloud relate
282,16783625,2.0,increase elastic cloud storage watermark storage fill elastic cloud when reach storage low watermark node shard move node node reach low watermark cluster stop store datum as suggestion elastic increase low watermark leave gb free well use storage capacity
283,16782710,1.0,canary instance report prometheus stage main lb lb lb lb lb lb lb lb lb lb cc
284,16774215,1.0,terraform report change instance gstg a recent unidentified change terraform repository result instance fleet register taint plan add change destroy
285,16772659,1.0,gitter os patching beta summary as interim step update gitter ami image follow support need perform os patching gitter instance establish timing validate step list patch process the patch process script possibly implement ansible the broad stroke require run apt update apt upgrade on instance preferably instance service group need script validate rollback process proceed production finally need consider service impact orchestrate change drain connection detach instance load possible
286,16772426,3.0,rca pipeline slow queue job summary multiple user report slowness launch pipeline job private runner timing correlate spike sidekiq queue sidekiq stat which correlate patch deployment cny later gprd yellow annotation sidekiq queue deployment affect ci runner sidekiq team attribution minute downtime degradation tbd impact metrics start following what impact incident service outage sub service brown exposure sensitive datum who impact incident external customer internal customer specific team how incident impact customer prevent x incorrect display y how attempt access impact service feature how customer affect how customer try access impact service feature include additional metric relevance provide relevant graph help understand impact incident dynamic detection response start following how incident detect do alarming work expect how long start incident detection how long detection remediation be issue response incident bastion host access service available relevant team memeber page able timeline utc report issue share runner slack utc user report slow slack notification email stalled pipeline utc user report hung pipeline idle runner utc user report latency pipeline job utc slowness report production team slack root cause analysis the purpose document understand reason cause incident create mechanism prevent recur future a root cause person way writing refer system context specific actor follow s blameless manner core root cause analysis for necessary start incident question happen keep iterate ask time while hard rule time help question deep find actual root cause keep min come answer consider follow different branch example usage s the vehicle start problem why the battery dead why the alternator function why the alternator belt break why the alternator belt useful service life replace why the vehicle maintain accord recommend service schedule fifth root cause what go start follow identify thing work expect any additional out go particularly what improve start follow use root cause analysis explain improve prevent happen be improve detection time detection be improve response time response be exist issue prevent incident reduce impact do indication knowledge incident place corrective action list issue create corrective action incident for issue include follow bare issue link issue label action include estimate date completion corrective action incldue name individual own delivery corrective action guideline blameless rca guideline s
288,16750974,2.0,create periodic restart registry haproxy drain this need address memory issue the restart need coordinate carefully my current leverage deploy tooling haproxy drain logc cc
289,16736493,3.0,convert environment associate branch deployment instead tag the step environment project correct design decision use tag track deployment it make sense track deploy environment branch use tag track version code the original design tag overload meta information tag intuitive cool way information i go convert boring solution easy understand work way rest world work
290,16736470,3.0,change environment project use single pipeline there design decision i like original project they work limitation ci pipeline run multiple pipeline deploy i expand variable pipeline there well way work i convert use pipeline deploy mean automate rollback
291,16735357,1.0,rotate hi team change store chef vault ref
292,16718673,3.0,database reviews x x x x x x open review move
293,16671827,8.0,upgrade linux kernel production environment this issue track planning review execution cr upgrade linux kernel production environment for past reference follow complete staging lessons learn upgrade kernel staging revert kernel redi sentinel postgre prometheus alertmanager staging do upgrade kernel redi sentinel postgre prometheus alertmanager include dry run functionality print list host upgrade link script production cr create review cr
295,16635745,1.0,enable elasticsearch i suggest enable elasticsearch number reason we test use elasticsearch real environment this provide opportunity basic testing global search helpful look content cookbook it expose limitation geo support etc thought
296,16608848,2.0,fix link postgres grafana dashboards since patroni migration postgres dashboards undergo adjustment lead link point non existent dashboards grafana for example link top qeries dashboard query drilldown dashboard query id they fix
297,16602897,2.0,chatop command oncall override it look like override oncall show
298,16593367,1.0,make pre check verify ulimit proper setup service launch production make pre check maintenance template verify ulimit proper setup service launch production
299,16590390,4.0,aws rate limiting our access aws project rate limit aws excessive usage api resource call it point effect gui console throw rate exceeded error after investigation determined automation create excessive usage there dns record domain to remove throttling break thing i remove role policy review app ee dns user review app ee dns remove access
300,18327550,2.0,create blueprint process exporter monitor pg io create blueprint process exporter monitor pg io
301,18303242,4.0,consider move gitlab production commit use discount in investigate able apply commit use discount gcp compute spend two idea emerge apply cud ci runner fleet move gitlab production sustain use discount commit use discount issue since committed use discount cuds offer significantly deep discounting sustained use discount suds issue investigate possible saving action plan proposal identify gitlab production system compute cuds currently benefit suds estimate additional discounting possible saving evaluate worth pursue base risk commit spend link resource google cuds
302,18250633,3.0,chef complete break td agent td agent start dr database server possibly server this cause chef fail the problem multiple version googleauth gem delete allow td agent start chef run put start fail td agent embed lib ruby rubygem activate activate this issue relate issue they result chef complete run the fail bootstrappe this fail later
303,18247646,3.0,in case patroni demotion event want notification the patroni primary db demote rca notice we need sure notification patroni event like
304,18246426,1.0,add welcome page public dashboard we security report publicly visible it slightly confusing drop directly triage dashboard we add welcome page public grafana instance
305,18235635,2.0,bump client output buffer limit redi nodes from set hard limit gb have hard soft limit defy purpose note chef client run gitlab ctl reconfigure redi nodes run restart service change so update file manually run config set new config redis console good approach experiment stage
306,18232490,2.0,rca primary db failover summary postgres primary restart failover affect postgres team attribution infrastructure minutes downtime degradation impact metric what impact incident increase error rate failover maybe slightly decrease db performance incident miss table stat who impact incident external customer how incident impact customer customer see error response failover how attempt access impact service feature how customer affect how customer try access impact service feature include additional metric relevance gitlab triage dashboard incident detection response start following how incident detect we get short increasederrorrates alert associate db problem the incident mistaken miss table stat start toomanydeadtuple alert a close look db log day reveal primary db restart failover do alarming work expect we get alert high error rate dead tuple know go notification db failover miss detect real cause investigation day how long start incident detection we detect high error rate immediately detect cause take how long detection remediation the db cluster autonomously minute be issue response incident bastion host access service available relevant team member page able timeline utc fail connect consul utc postgres restart utc increase error rates alert utc increase error rates alert resolve utc alert receive utc alert receive utc alert receive utc alert receive similar alert utc notice postgre restart take place utc yesterday spike memory graph trolleybus utc kill ongoing vacuum analyze process table utc start analyze verbose database utc analyze verbose finish statistic utc while investigation go i ask ongres look root cause analysis patroni demote primary postgre db because connect dcs consul because get exception max retrie exceed url kv service pg ha db gprd error error communicate dcs info demote self dcs accessible leader info promote self leader acquire session lock because network issue maybe relate dmesg log show tcp relate kernel stack trace what go patroni failover work second get alert high error rate dead tuple work restore table stat notice demotion what improve alert patroni demotion event miss table stat lead conclusion db failover happen db analyze procedure case db failover well know automate monitor kernel stack dump help detect issue network corrective action automatically run analyze failover assignee tbd tbd make sure notification patroni restart failover role change like assignee tbd tbd investigate cause primary demotion assignee tbd tbd consider monitoring kernel stack trace message dmesg log assignee tbd tbd
307,18221698,3.0,chef runs broken bootstrappe chef run break dr database server machine wo bootstrap it appear file miss prometheus metric chef the error feb db dr startup script info startup script error run exception handler feb db dr startup script info startup script error prometheushandler no file directory prometheus metric chef feb db dr startup script info startup script error exception handler complete feb db dr startup script info startup script fatal stacktrace dump chef cache chef feb db dr startup script info startup script fatal please provide content file file bug report feb db dr startup script info startup script error undefined method nil nilclass this difficult debug happen user ssh key set chef cache chef unavailable log gcp console work serial port present login prompt useless credential this work machine initially set chef module version now create instance i combination version result successful initial chef run
308,18219976,1.0,gather rough estimation non http traffic cloudflare estimate make issue note collection estimate thank response i apologize email clear specific spectrum non http s question pm what look obtain how mo route non standard http s port how concurrent ssh connection today plan get average query like offset plan
309,18215745,3.0,create version source control packer image template determine correct place repository new existing add template configuration google compute builder
310,18215555,3.0,design document process restore database gce disk snapshot rather wait zfs database snapshot rely snapshot quality level necessary disaster recovery customer face functionality go automate extraction gce disk snapshot production environment use testing the design outline process technical detail it limit pay attention way protect customer datum treat production datum scrub datum introduce personally identifiable information pii intellectual property ip testing environment ensure proper safeguard place potentially move process scrub datum isolate environment avoid accidentally manipulate production datum scale datum functional use
311,18202713,3.0,monitor packagecloud db backup run disk space db backup take space some backup fail leave temporary datum disk gb backup we monitor backup failure automatically cleanup datum fail backup if trim local retention interval longterm solution migrate mysql rds
312,18180568,3.0,configure test cookbook license gitlab com as pre requisite step need enable basic chefspec inspec test configuration validate change deploy update cookbook x configure chefspec exist test x configure test kitchen validate kitchen converge exist default inspec test
313,18154582,2.0,ci cd handoff switch alert sre rotation tbd base feedback the main action change alert template alert relate ci cd cicd op we post ci cd alert mention second follow exist format go change channel alert ci cd alert channel prefix way cc note change
314,18154514,2.0,cd d readiness review part performance monitoring alerting part ci cd sre performance explain validation follow gitlab performance guidline explain link result query performer sherlock request profiling be potential performance impact database feature enable scale be throttling limit impose feature if manage if throttle limit customer experience hit limit for dependency external internal application retry strategy do feature account brief spike traffic expected tps monitoring alerts be service log json format log forward logstash be service report metric prometheus how end end customer experience measure do target sla place service do know indicator sli map target sla do alert trigger sli sla meet do troubleshooting runbook link alert what threshold tweet issue official customer notification outage relate feature responsibility which individual subject matter expert know feature which team set individual responsibility reliability feature production be team build feature launch if testing describe load test plan feature what break point validate for component failure theorize feature test if include result failure test give brief overview test run automatically gitlab ci cd pipeline feature exist doc use reference note recent summary meeting diagrams etc acceptance criteria perform monitoring alert analysis gather note link service catalog child issue in case change actual alert infra prep design mr new routing alert update sre team proper information link runbook etc child issue
315,18154479,2.0,ci cd readiness review part db backup restore security part ci cd sre database x if use database data structure verify vet database team x do approximate growth rate store datum capacity planning x can age datum delete datum certain age security be gitlab security development guideline follow feature if feature require new infrastructure update regularly os update have effort obscure elide sensitive customer datum log be potentially sensitive user provide datum persist if datum encrypt rest backup restore x outside exist backup customer datum need back product feature x be backup monitor x be restore backup test exist doc use reference note recent summary meeting diagrams etc acceptance criteria perform summary architecture analysis gather note link service catalog child issue
316,18154434,2.0,ci cd readiness review part operational risk assessment part ci cd sre handoff operational risk assessment what potential scalability performance issue result change list external internal dependency application ex redi postgre etc feature impact failure dependency be feature cut compromise feature launch list operational risk feature go live what operational concern present launch concern later can new product feature safely roll live disable feature flag document way customer interact new feature customer impact failure interaction as thought experiment think bad case failure scenario product feature blast radius failure isolate exist doc use reference note recent summary meeting diagrams etc acceptance criteria perform risk assessment gather note add service catalog child issue
317,18154389,6.0,ci cd readiness review part summary architecture part ci cd operational work summary provide high level summary ci cd feature set what metric include business metric monitor ensure feature launch success architecture add architecture diagram issue feature component interact exist gitlab component include internal dependency port security policy etc for component dependency blast radius failure be feature design reduce risk where applicable explain scale potential single point failure design exist doc use reference note recent summary meeting diagrams etc acceptance criteria perform summary architecture analysis gather note link service catalog child issue
318,18139872,2.0,database reviews x x x x x x x nik check restore box x x
319,18123368,4.0,service catalog dashboard track work build dashboard service catalog issue
320,18083211,5.0,add gke cluster gitlab org why what we want deploy auto devops in order need kubernetes cluster associate project probably gke cluster live production gcp project live separate gcp project preferable during setup probably regular maintenance troubleshooting need provide level access gke cluster member configure team dogfoode auto devops long term objective also assume production team want fully responsible keep system run production critical infrastructure how decide gcp project use somebody permission create gke cluster gcp project need add cluster project operation kubernete add kubernetes cluster sign google name cluster design system leave default setting click create
321,18056896,3.0,tflint configure property ci cd duplicate issue open op instance currently stage sh d echo check d validate d tflint issue env evaluate variable false tflint option far i tell this translate practice run tflint issue repo root directory if try local copy pass but try environment gstg error we adjust actually check
322,18050312,1.0,slack access core team member while work slack access issue general core team member provide new core team member ben bodenmiller access follow slack channel ben sign nda core development gdk mr coach release post security please let know need i send ben email address dm slack
323,18047015,8.0,execute maintenance execute repacke maintenance index select table for select table want execute table maintenance those table outlier base analysis those table service snippet the maintenance roll iteratively increase risk impact repack normal index repack primary key repack table user creation create specific user repacke set
324,18046840,5.0,implement automation execute require automation clean event failure we wrap small command line tool ruby later move gitlab rake task ship product
325,18046752,1.0,include omnibus in order ship product include omnibus this track change available standard install
326,18046726,2.0,deploy this instal deploy since currently base omnibus install chef
328,18020114,3.0,set geo database replication dr work database replication prerequisite enable geo the dr site use omnibus bundle postgres need replication set manually chef currently dr node run unconfigured database shell ssh devin db starting console wait psql fatal entry host user gitlab database ssl fatal entry host user gitlab database ssl secondary check db host fail connection closed the dr database node shell knife node list grep patroni grep gitlab dr the step load datum dump production dr cluster we want actually make connection production this reduce risk ideally able load database certain time switch replication catch difference turn replication have pull entire database production directly currently dr database minimally configure terraform chef if go wrong trivial delete node create scratch
329,18017513,2.0,automate ansible run gitter environment currently manually initiate ansible run gitter environment effect change deploy ssh key com gl infra we minimum setup service account perform automate run ansible playbook pipeline gitlab com gl infra gitter infrastructure other option possibly worth future discussion include schedule ansible run schedule auto scaling reap old node new one migration architecture base immutable infrastructure docker
330,18015299,1.0,deploy ssh key michal cameron apply change gitlab com gl infra gitter gitlab com gl infra gitter
331,18006307,3.0,familiar omnibus setup postgresql repmgr please familiar setup postgresql rep mgr look forward integration patroni product omnibus pg marin docs ha
332,17980799,2.0,transfer gitlab domain i register early day gitlab need domain point solution architect aws account i realize personal account i transfer ownership company domain gitlab let know information need start
333,17979155,1.0,consider well alert during we receive alert page wrong re evaluate current rule set ensure problem page
334,17973844,1.0,increase support request there recently influx support request project show read user gitlab use issue figure overall impact investigate way figure flip project read state relate other note initial cause point work however work encompass project mention issue i log project i touch work the project log locate server maintenance not perform
335,17949980,2.0,gemnasium service backup restore test leave db follow find clean restore db work the initial clean destroy function look remaining production db db current date this script update list production db iterate destroy
336,17942420,3.0,rca summary a brief summary happen try executive friendly possible affect team attribution infrastructure minute downtime degradation hour minute as maintenance task script run slowly old large repos different file server job schedule fail timeout retry this lead excessive work schedule file server target time maintenance slowly ramp io load file server this high io translate gitaly timeout request go lead customer impact metrics start following what impact incident slow performance http response gitlab who impact incident everyone include additional metric relevance another fun set chart detection response how incident detect customer report beta monitoring do alarming work expect not entirely while alert high rate alert come late root cause able strengthen leverage we receive page how long start incident detection hour how long detection remediation hour timeline utc maintenance fileserver begin command execute gitlab rail runner file server nfs file server nfs run false tee utc page high count utc alert gitaly error count fileserver utc oncall engineer reach engineer perform maintenance utc maintenance operation halt proceed utc estimate project flight maintenance process relate maintenance work lower io impact file server utc fileserver start sign recovery git process complete utc fileserver reach okay slo degradation utc fileserver remain slo root cause analysis a seemingly routine maintenance item work past entire week incident result failure a script routinely query project move chug take account active job failure retrie exist job these job responsible tell file server datum server some job take long time mark failure timeout despite underlie mechanism run file server when job retry spin process file server one project move result total operation project during time maintenance stop discover project schedule file server continue process datum repository complete this process move datum invoke git upload pack command io intensive for large repository awhile the script create maintenance build schedule work allow sidekiq schedule work it visible maintenance script operator extent progression failure rate occur specifically repo migration what go the maintenance operator able quickly gather list process tie project move renice help server we choose kill running process order prevent datum corruption the script design prevent overflow queue need manual intervention future work queue question be potential network bottleneck can develop well understanding gitaly slo impact overall webserver performance what improve no notify this situation treat incident our support team user lack status information sre need well understanding impact project migration new failure scenario run previously documentation project move work discuss mechanism utilize perform move learn git upload pack run multiple time file server new information visibility process overall highly limited this encompass log view sidekiq metric we need discuss well way garner attention correct party if member think incident need ensure mark correct urgency potential issue corrective action improvements script perform project repo migration well handling failure scenario inside gitlab improve process visibility gitlab improve gitaly process monitor niceness alert improvements communication improvements guideline blameless rca guideline s
337,17907786,2.0,miss log kibana it problem ingest particular log api node here graphql graphql locally knife ssh role gprd base fe api sudo zcat log gitlab gitlab rail jq time c snip knife ssh role gprd base fe api sudo zcat log gitlab gitlab rail jq time c wc
338,17889787,2.0,repository inconsistent size in issue move datum nfs nfs two project datum size inconsistency server for repos datum wiki size disk large location migrate i curious figure potential corrupted datum potentially coalesce necessary at moment time database point storage repos old server nfs these repos currently mark read the goal issue remove move nfs safe mark repository writable project question reference material
339,17871540,1.0,most request stage time staging throw error web request unicorn process kill workhor throw error the problem database relate query time and transaction high
340,17835293,1.0,increase quota gitlab restore gcp project this issue track quota increase cpu memory disk resource gitlab restore gcp project space
341,17822007,3.0,design document productionize consul write technical underpinning represent guidance design propose implementation method run robust instance consul production the work mr
342,17817891,4.0,blueprint zfs file system gitlab write technical underpinning represent guidance design propose implementation method bring zfs
343,17814420,3.0,design document move pgbouncer dedicated cluster we need design doc propse
344,17809742,3.0,investigate design execution plan migrating chef server gcp as outcome want chef server digital ocean gcp we test restore chef backup switch ubuntu consider want upgrade chef new version chef run different region gitlab op project able bootstrap case region document plan test backup restore process production change migration identify plan update require chef repo pipeline related workflow process cookbook role environment datum bag vault support new chef server x provision infrastructure new chef gitlab op project document perform test migration staging infrastructure available
345,17789492,3.0,blueprint ci cd handoff oncall operation sre placeholder issue blueprint ci cd ownership sre team
346,17786862,4.0,start blueprint vault placeholder issue attach mr blueprint define plan implement hashicorp vault secret management
347,17779219,3.0,auto fetch populate certs we craft need glue advantage sslmate api fetch cert auto renew place appropriate vault
348,17775589,1.0,functional onboarding buddy currently new hire assign onboarding buddy usually functional role new hire purpose however have somebody functional role dedicate onboarding help speed get start we lot onboarding material have functional onboarding buddy mean replace we want stay async onboarde however certain topic help somebody dedicate point right direction provide short talk certain topic at minimum functional onboarding buddy aim convey topic important understand provide starting point this outcome dbre sync meeting today suggestion
349,17769852,1.0,certificate expire fix
350,17735529,3.0,update slack access core team member currently core team member access gitlab slack channel employee we want restrict access channel customer confidential information expose core team member could implement script slack api
351,17727344,2.0,database reviews x x x x contribution x contribution x x x inherit previous week x x x x x x x x x x x x x x x x x x
352,17726319,1.0,use consistent naming pgbouncer terraform there bunch inconsistent reference pgbouncer terraform config pgbouncer pg bouncer pgb it good consolidate pgbouncer find related code easy
353,17695289,1.0,show datum show datum if correct decom
354,17657121,2.0,increase web worker pool base peak monitoring datum hit limit web request handle give time web worker web worker for reference peak time cpu utilization memory utilization web fleet web fleet web fleet web fleet we currently deploy worker cpu node i think increase node worker stay acceptable cpu memory utilization
355,17649529,1.0,centralize gcp bootstrap module as step break terraform module separate repos start data source reference bootstrap script https file this issue primary source bootstrap file provision keep need host second permission ops instance long available anonymous url x create new bootstrap module include module provide bootstrap teardown script content version x configure push mirror new module x replace resource current module reference new bootstrap submodule x clean bootstrap script gitlab com infrastructure repository
356,17645996,5.0,terraform runs clean environment dr this prevent able confidently run terraform all member team able run terraform give environment change relate issue work right wide variety change lead member team target work slowly build list thing terraform want change at point go bring bout destruction avoid utilize issue track work necessary environment clean run
357,17632840,1.0,setup mailgun account meltano relate we process add authentication meltano need smtp service send confirmation recovery email
358,17630848,5.0,create runbook stand chef server the chef server need bootstrap but loose chef gitlab stand site backup we write runbook instruction bootstrap chef backup disaster recovery move cloud provider
359,17617820,1.0,prometheusunreachable dr environment now log prometheus server dr environment long get prometheusunreachable alert alert fire we create silence server run run see alert
360,17617676,1.0,access error connect monitor dr environment we get error connect
361,17612726,3.0,setup aw service account terraform ci overview while set ci pipeline gitlab com gl infra gitter start receive permission error job the pipeline previously setup use packer service account permission temporarily add enable access remote state resource plan go forward need configure service account account role remote state create terraform remote state iam account add credential add ci variable update credential job attach terraform remote state iam policy terraform plan copy privilege dynamodb kms service packer policy remove dynamodb kms privilege packer iam policy beta create terraform beta iam account add credential add ci variable update job credential create terraform beta ro iam role read terraform plan pass role arn variable create terraform beta priv iam role admin privs terraform apply pass role arn variable prod setup terraform prod iam account add credential add ci variable update job credential create terraform prod ro iam role read terraform plan pass role arn variable create terraform prod priv iam role admin privs terraform apply pass role arn variable
362,17604839,2.0,database review process documentation in order ramp database review process go improve documentation line current workflow database review document expectation database review document workflow database review
363,17598570,1.0,sidekiq json log parse mtail some dashboard go blank sidekiq cluster log prod json format
364,17596910,3.0,consider move chef server gcp chef server currently run digital ocean we consider well gcp instead this issue discuss pro con a followup issue open decide chef gcp summarize discussion pro network permission setup easy service care we upgrade chef ubuntu keep old chef fallback con if gcp go need standup chef bootstrap but instal different region test backup mitigate risk conclusion we chef gcp track pre requirement work backup restore procedure chef
365,17596223,5.0,regular backup chef server we need regular backup chef while postgre back wal g need backup datum locally persist chef tool like chef server ctl backup knife ec backup a runbook restore chef backup help
366,17595873,2.0,monitor chef redi monitor redi chef server
367,17595834,2.0,monitor chef rabbitmq monitor rabbitmq chef server
368,17595118,2.0,monitor chef nginx monitor nginx chef server
369,17595026,5.0,monitor chef erlang components we need find way monitor erlang base component chef server there probably way interact beam vm metric we alert component opscode chef mover opscode erchef opscode expander opscode pushy server opscode
370,17583384,1.0,when server fleet go multiple alert fire the alert increasedserverconnectionerrors increasedbackendconnectionerrors problem description go one server fleet barfed do care one server go sure we totally should i wake midnight cuz server barfed no we provision service server ability slack time server this server reboot google the underlie host run fail json insertid teusfpfndyiui jsonpayload info instance automatically restart compute engine code version actor user system resource zone d d web sv gprd type instance operation type operation zone d d resource type label zone d gitlab production timestamp severity info label d web sv gprd instance logname project gitlab production log receivetimestamp protopayload servicename methodname insertid resource type label zone d gitlab production timestamp severity info logname project gitlab production log operation d producer true true receivetimestamp insertid jsonpayload version actor user system resource web sv gprd type instance zone d d operation type operation zone d d info code instance terminate compute engine resource type label zone d gitlab production timestamp severity info label d web sv gprd instance logname project gitlab production log receivetimestamp protopayload servicename methodname insertid resource type label zone d gitlab production timestamp severity info logname project gitlab production log operation d producer true true receivetimestamp use issue means discuss remove page wake engineer need the server come online fine it healthy chef job haproxy know
371,17580128,2.0,ensure firewall rule place properly scrape dr metric part
372,17576637,1.0,performance review app slow over couple week review app extremely slow the deploy job finish expect actual use app slow point unusable i leave tab open minute order navigate app i notice multiple project current example pst jan slack
373,17572112,1.0,chef client fail gstg bastion look like chef client fail info client key chef present register warn fail read private key chef no file directory chef chef encounter error attempt create client private key not find your private key load if key file exist ensure readable chef client relevant config settings chef platform linux realize figure able access staging host
374,17568836,3.0,right size sidekiq nod base day metric grossly provision sidekiq server size priority mem cpu trace asap page pipeline pullmirror besteffort import realtime we consider reduce instance size node possibly add instance queue need we able safely utilization active queue problem this save lot production resource the napkin math say engineer salary
375,17564335,2.0,console session log miss history tag extension log stackdriver easily filter miss tag i think easily fix include tag tag filter
376,17559102,2.0,registry fleet memory starvation from oncall handover meeting decide create issue registry server get reboot memory consumption between week incident the thing talk possibly profiling registry issue happen frequently identify source memory leak
377,17553237,2.0,remove persona authentication persona authentication provider mozilla discontinue like year ago i think get rid use forum these plugin use action x follow step sure persona plugin absent
378,17542428,3.0,move chef repo ops instance the chef repository currently mirror ops instance this create latency merge apply it mean easily push change need bring it set like optimize want submit mr repo have access ops instance we need optimize majority request for small number outside request receive repository manually apply patch the step audit adjust permission chef repo project op instance ensure repository change direction sync op instance instance change ci job automation point chef server op instance notify user change remote local copy
379,17520126,2.0,design okr gitlab implementation write design implement okr gitlab
380,17519947,5.0,develop framework automate infrastructure change take example i want script look roughly like ruby step chef client db patroni end step end step end step consul end verify entry entry end end step consul end verify role master replica entry entry end end end step end step end verify count put count end end note approach use dsl make script easy read review each action ruby class separate method dry run run dry run allow check prerequisite if step execute knife ssh check knife properly configure actually ssh machine in run prompt step progression skip retry halt etc environment mention explicitly script over time grow inventory reusable action log the framework gem dependency i rough structure framework push thing work
381,17508483,2.0,inventory catalogue version task fill gsic gitlab service inventory catalogue template service the task fill need reach team member order capture information service version sample template location question comments template reach
382,17508478,2.0,inventory catalogue web task fill gsic gitlab service inventory catalogue template service the task fill need reach team member order capture information service web sample template location question comments template reach
383,17508471,2.0,inventory catalogue sidekiq task fill gsic gitlab service inventory catalogue template service the task fill need reach team member order capture information service sidekiq sample template location question comments template reach
384,17508469,2.0,inventory catalogue share task fill gsic gitlab service inventory catalogue template service the task fill need reach team member order capture information service share sample template location question comments template reach
385,17508463,2.0,inventory catalogue runner task fill gsic gitlab service inventory catalogue template service the task fill need reach team member order capture information service runner sample template location question comments template reach
386,17508400,2.0,inventory catalogue registry task fill gsic gitlab service inventory catalogue template service the task fill need reach team member order capture information service registry sample template location question comments template reach
387,17508225,2.0,inventory catalogue psql timing task fill gsic gitlab service inventory catalogue template service the task fill need reach team member order capture information service psql time sample template location question comments template reach
388,17508223,2.0,inventory catalogue prometheus app task fill gsic gitlab service inventory catalogue template service the task fill need reach team member order capture information service prometheus app sample template location question comments template reach
389,17508217,2.0,inventory catalogue prometheus task fill gsic gitlab service inventory catalogue template service the task fill need reach team member order capture information service prometheus sample template location question comments template reach
390,17508212,2.0,inventory catalogue postgres dr delayed task fill gsic gitlab service inventory catalogue template service the task fill need reach team member order capture information service postgres dr delay sample template location question comments template reach
391,17508208,2.0,inventory catalogue postgres dr archive task fill gsic gitlab service inventory catalogue template service the task fill need reach team member order capture information service postgres dr archive sample template location question comments template reach
392,17508199,2.0,inventory catalogue postgres task fill gsic gitlab service inventory catalogue template service the task fill need reach team member order capture information service postgre sample template location question comments template reach
393,17508194,1.0,inventory catalogue patroni task fill gsic gitlab service inventory catalogue template service the task fill need reach team member order capture information service patroni sample template location question comments template reach
394,17465567,2.0,fix postgre wal archive run disk space wal log pile opt opscode data wal e archive command fail it take gb disk disk usage need fix run space log archive command fail exit code detail the fail archive command bin envdir wal env wal e bin wal e wal push error msg storage prefix define hint either set prefix prefix prefix prefix option define environment variable we need monitor successful db backups chef monitoring track
395,17465341,3.0,problem statement define goal pg repack epic the epic pg repack define problem statement define goal let start define problem set goal epic ultimately beginning design document description epic i like epic specific measurable know aim smart
396,17457094,2.0,inventory catalogue pages task fill gsic gitlab service inventory catalogue template service the task fill need reach team member order capture information service pages sample template location question comments template reach
397,17457081,2.0,inventory catalogue mailroom task fill gsic gitlab service inventory catalogue template service the task fill need reach team member order capture information service mailroom sample template location question comments template reach
398,17457059,2.0,inventory catalogue load balancer fe task fill gsic gitlab service inventory catalogue template service the task fill need reach team member order capture information service load balancer fe sample template location question comments template reach
399,17457042,2.0,inventory catalogue influxdb task fill gsic gitlab service inventory catalogue template service the task fill need reach team member order capture information service influxdb sample template location question comments template reach
400,32556319,3.0,create new gitaly storage shard node replace nfs gitaly storage shard nfs currently usage a new gitaly node create add list shard configure include consideration store new project repository that way nfs remove rotation concern node removal configuration additional burden remain node it important avoid acceleration usage growth remain node accept new repository
401,32556303,2.0,create new gitaly storage shard node replace nfs gitaly storage shard nfs currently usage a new gitaly node create add list shard configure include consideration store new project repository that way nfs remove rotation concern node removal configuration additional burden remain node it important avoid acceleration usage growth remain node accept new repository
402,32552241,1.0,resolve redis config discrepancy redi cache gstg gprd apart credential discrepancy production stage config redis cache node disk persistence option the redi cache node staging environment enable periodically write rdb dump production since dataset mean ephemeral get persist disk clean shutdown reasonable setting we enable later decide expensive repopulate cache anecdotally people mention think tolerable for let staging config match production for reference discrepancy go resolve shell msmiley ssh sudo cat opt gitlab redi egrep perl msmiley ssh sudo cat opt gitlab redi egrep perl msmiley diff gprd
403,32538978,3.0,elastic field expansion match field request detail version true size sort order desc boolean source exclude aggs field utc script source lang painless field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format query bool filter type query externaldiffuploader lenient true query range format gte lte highlight field response detail take false shard total successful skip fail failure shard index pubsub rail inf node reason type reason fail create query index pubsub rail inf type reason field expansion match field limit get shard index pubsub rail inf node reason type reason fail create query index pubsub rail inf type reason field expansion match field limit get hit total hit
404,32503229,8.0,configure wal g wal push backup push test postgres backup creation wal g production in start test wal g creation use backup staging after week successful testing daily staging restore wal g wal g staging archive time start test production replica x decide replica choose configure chef note master gprd right such replica mark unavailable failover x in addition wal e install wal g postgres instance it okay wal e wal g instal x configure wal use new gcs bucket different exist one choose replica gprd wal g wal push set ensure wal push log error show successful wal pushing new gcs bucket folder fill wal configure daily backup push replica cronjob similar wal e backup push cronjob master backup daily additional todo adjust runbook
405,32496839,1.0,incorrect runbook link alert m link runbook troubleshooting service ci m alert incorrect correct link
406,32481109,3.0,terraform artifact large ci ci job terraform fail request entity too large error environment find match file error uploading artifact coordinator large archive request entity too large request entity too large token fatal large error job fail exit code
407,32478381,3.0,queue execution time slo violation we regularly get alert violate queue execution time slo we investigate reason improve execution time adjust slo
408,32344201,3.0,experimentally consider reduce gitaly storage shard node instance gcp machine type experimentally consider conduct data base evaluation consequence reduce gitaly storage shard node instance gcp machine type measure reduce operating expenditures propose
409,32341678,2.0,execute postgresql upgrade staging use template we like execute test migration postgresql upgrade the date march utc the maintenance time staging minute please upgrade node secondary host rollback
410,32249101,1.0,redis pcap file there pcap file large directory this filespace free root filesystem if need file posterity delete and need consider delete asap thank
411,32239542,4.0,create rollback script postgresql upgrade we need script rollback postgresql upgrade need use we consider gcp snapshot also database node read execute upgrade with node restore environment quickly please consider scenario upgrade script describe file i like share step rollback script at begining rollback a snapshot consistent database upgrade exclude instance group node upgrade if successful upgrade upgrade reduce cluster number nodes at point ready run rollback upgrade run chef rollback file stop service fail instance like chef client pgbouncer patroni postgresql check manually on fail instance change config false value avoid start service accidentally stop pgbouncer exclude instance avoid connection start chef client service exclude instance resume patroni service check patroni elect new leader start pgbouncer continue
412,32155027,2.0,new storage set terraform state in configure system group create new feature store terraform state gitlab this involve add new storage setting terraform state correlate setting introduce uploader use new storage setting it add default value setting add storage set omnibus add storage set gitlab helm chart x add bucket feature module x setup bucket gstg x enable feature gstg x setup bucket gprd x enable feature gprd
413,32151883,1.0,dns record please add follow a record dns record ttl relate gitlab com gl security assignment
414,32145921,2.0,incident review intermittent cpu saturation gitaly node please note incident relate sensitive datum security related consider label issue mark confidential incident review relate incident root cause these incident confidential protect privacy project involve summary affect gitaly minutes downtime degradation several minute period slowness sum minute spread day one gitaly shard stor gprd reach cpu saturation lead increase slowness rate error timeout other gitaly shard affect the cpu saturation trigger client concurrently run git fetch git clone repo large graph commit object traverse to generate inventory object client need gitaly run git pack object repo require significant cpu time memory enough helper process run host starve cpu time result observe increase latency error rate timeout particular gitaly shard for reference incident tie similar regression particular gitaly shard start each regression event cause roughly minute degraded performance gitaly shard now understand pathology link following regression event event minute event minute event minute event minute event minute mitigation the owner repo distribute build trigger regression kindly offer adjust build reduce impact reuse fresher git clone build server help reduce concurrency definitely help on gitlab server gitaly need avoid saturation limit resource usage helper process both cpu time memory relevant problem one option limit concurrency grpc postuploadpack call spawn git pack object helper process for context git client fetch pull clone etc subcommand send http post request repo upload pack endpoint cause rail send gitaly postuploadpack grpc use instance wide limit combination small scoper limit namespace project allow project continue function busy peer rate limited another possibility cgroup impose hard limit resource usage particular type helper process however certain workload pattern result starve tenant gitaly shard this well suit second line defense application base concurrency limit act primary defense graceful degradation behavior impact metric what impact incident slowness intermittent error operation involve git repos store gitaly shard who impact incident all customer have git repos store gitaly shard roughly repos how incident impact customer git operation slow timeout how attempt access impact service feature routine traffic minute timespan how customer affect approximately repos slow access timing unknown customer actively experience slowness how customer try access impact service feature see dashboard gitaly overview latency apdex gitaly service degraded shard show drop regression correspond spike gitaly error rate timeout jump high a similar pattern find early date later backtrack pathology latency error rate dashboard host stats the host level resource usage metric affect gitaly node clearly cpu usage saturation memory usage increase network egress throughput spike result send client git object need network throughput near saturation point disk i o show significantly affected disk read throughput moderately increase disk i o satisfied filesystem cache physical disk i o increase result cache erode anonymous memory allocation numerous git pack object process cpu usage memory usage network usage command line list process show cpu usage predominantly git pack object process spawn gitaly thread gitaly note process hold significant resident memory share memory also git pack object process accumulate second cpu time minute cpu time follow analysis show process tend wall clock time spend cpu regression suspect process entirely cpu bind cpu time wait time slice cpu starvation these git pack object process operate git repo directory show way trace regression trigger condition top process cpu time shell day user load average task total running sleeping stop zombie sy ni d wa hi si st kib mem free buff cache kib swap total free avail mem pid user pr ni virt res shr s cpu mem command git g g s gitlab embed bin gitaly opt gitlab gitaly git g r gitlab embed libexec git core git pack object base offset git g r gitlab embed libexec git core git pack object base offset git g r gitlab embed libexec git core git pack object base offset git g r gitlab embed libexec git core git pack object base offset git g r gitlab embed libexec git core git pack object base offset git g g r gitlab embed libexec git core git pack object base offset git g g r gitlab embed libexec git core git pack object base offset git g g r gitlab embed libexec git core git pack object base offset git g g r gitlab embed libexec git core git pack object base offset git g g r gitlab embed libexec git core git pack object base offset git g g r gitlab embed libexec git core git pack object base offset git g r gitlab embed libexec git core git pack object base offset git g g r gitlab embed libexec git core git pack object base offset git g g r gitlab embed libexec git core git pack object base offset git g r gitlab embed libexec git core git pack object base offset git g r gitlab embed libexec git core git pack object base offset git g g r gitlab embed libexec git core git pack object base offset git g r gitlab embed libexec git core git pack object base offset git g r gitlab embed libexec git core git pack object base offset git g g r gitlab embed libexec git core git pack object base offset git g m r gitlab embed libexec git core git pack object base offset git g r gitlab embed libexec git core git pack object base offset root s td agent embed bin ruby td agent embed bin fluentd log td agent td git g g r gitlab embed libexec git core git pack object base offset git g r gitlab embed libexec git core git pack object base offset git g r gitlab embed libexec git core git pack object base offset git g r gitlab embed libexec git core git pack object base offset git g g r gitlab embed libexec git core git pack object base offset git g r gitlab embed libexec git core git pack object base offset git g r gitlab embed libexec git core git pack object base offset git g r gitlab embed libexec git core git unpack object input git g r gitlab embed libexec git core git pack object base offset git g g r gitlab embed libexec git core git pack object base offset git g r gitlab embed libexec git core git pack object base offset git g r gitlab embed libexec git core git pack object base offset git g g r gitlab embed libexec git core git pack object base offset git g r gitlab embed libexec git core git pack object base offset git g r gitlab embed libexec git core git pack object base offset git g g r gitlab embed libexec git core git pack object base offset git g r gitlab embed libexec git core git pack object base offset git g r gitlab embed libexec git core git pack object base offset git g g r gitlab embed libexec git core git pack object base offset git g m r gitlab embed libexec git core git pack object base offset git g m r gitlab embed libexec git core git pack object base offset git r gitlab embed libexec git core git pack object base offset tag git g g r gitlab embed libexec git core git pack object base offset git g g r gitlab embed libexec git core git pack object base offset cpu profile gitaly host regression a cpu profile cpu process show git pack object dominate cpu time collectively spend time calculate delta git object client request say versus server bare repo available offer this suggest pathology likely affect git repos large complex graph object tree compare perf profile cpu process stack regression shell msmiley sudo perf record sleep perf record wake time write datum perf record capture write mb sample msmiley sudo perf script msmiley cat perf complete svg flame graph flame pack object saturating cpu gitaly node flame pack object saturating cpu gitaly node detection response start following how incident detect pagerduty alert gitaly error rate high do alarming work expect yes how long start incident detection minute how long detection remediation the regression self resolve roughly minute alert trigger analysis regression continue incident resolve discover pathology triggering condition reach owner trigger repo we temporarily block behavior repo protect customer effect work customer understand use case mitigate impact be issue response incident no we necessary access people technology root cause analysis a gitaly node saturate cpu significantly change memory usage profile why more cpu bind memory hungry process spawn cpus available host why gitaly receive numerous postuploadpack grpc call client run git fetch similar gitaly process concurrently why most repos prone pathology focus attention gitaly tune why to trigger pathology repo access large object graph traverse compose response client git fetch specifically git client http post upload pack even repos meet condition cause regression gitaly node client request repo concurrently run why the gitaly server cpu memory capacity handle workload specification once reach saturation point gitaly operation begin slow cpu starvation degree effect proportional number cpu bind process run what go cross team collaboration fantastic imoc cmoc wonderful job usual facilitate investigation get help need regular pulse check guide balance info ok release publicly unsure precursor malicious attack accidental outcome legitimate use case alerting work appropriately gitaly rail event log elasticsearch give excellent observability app behavior client request profile the cpu profiling tool perf bcc give clear picture cpu time spend numerous git repos associate bulk activity what improve to prevent happen need prevent gitaly spawn git process saturate machine resource cpu memory gitaly support form concurrency limiting tune sufficient additional layer protection cgroup implement consider effect saturation case choose desirable failure mode health service saturation reach for example prefer slowness downtime downtime possible corruption automatic manual recovery etc each incident day series low severity time root cause analysis notice recur pattern our engineer field alert question interruption rarely time analyze self recover regression like this worth discuss iterative process improvement incident triage or maybe acceptable cost exist prioritization scheme corrective action prevent gitaly run git pack object process issue estimate date completion tbd owner tbd for short term mitigation add support remove user access ci cd have fully block user issue estimate date completion tbd owner tbd guideline blameless rca guideline s
415,32131630,4.0,execute database benchmarke postgresql the goal check load postgresql verify performance similar well we find query performance change new planner like reduce case we execute benchmark concept plan test primary database the main step list query execute jmeter different host reproduce query database generate similar traffic production
416,32012958,2.0,recur cpu bind git fetch process file gitaly node while look intermittent alert high gitaly latency node praefect i notice host run git fetch process these git fetch process cpu bind because tend run minute easy find profiling exit repeat time different git fetch process consistently show pattern particular git fetch process spend lot time read directory getdent syscall these slow cpu intensive git fetch process gitaly latency high alert plausible prove i think worth documenting mainly investigation lead tune git config handful gitlab own repos store host these repos repo gitaly tend ref relate git fetch expensive dentry fetch observation profile this example give consistent result git fetch run different git repos shell msmiley pgrep git gitlab embed bin git xargs ps uwf user pid cpu mem vsz rss tty stat start time command git r gitlab embed bin git fetch tmp msmiley sudo perf record sleep perf record wake time write datum perf record capture write mb sample terminate complete flamegraph svg format git git screenshot tall thin tower size dentry cache time the kernel slab cache dentrie roughly mb time writing shell msmiley date sudo slabtop sun mar utc active total object active total slabs active total caches active total size k k minimum average maximum object k k k objs active use obj size slabs obj slab cache size name k k k k dentry k k k k k k msmiley free total free shared buff cache available mem swap confirm getdent syscall frequent time consume profile syscall count duration example git fetch process shell msmiley pgrep git gitlab embed bin git xargs ps uwf user pid cpu mem vsz rss tty stat start time command git sl gitlab embed bin git fetch tmp git r gitlab embed bin git fetch tmp msmiley sudo trace syscall print quit syscall count time getdent open write lstat fstat close brk stat detaching
417,31980837,1.0,certificate expire the certificate expire
418,31931941,1.0,update cheat sheet prevent outage originate incident incident review update page dotcom documentation reflect run console gitlab customer user root the location need update a source show proper command
419,31915588,2.0,change request improve checkpoint setup as mention issue we like reconsider checkpoint relate postgresql setting gb gb frequent forced checkpoint frequent plan checkpoint smoother checkpointer behavior and need create follow change day observation step perform change staging production change change apply gstg apply gprd change gb gb apply gstg apply gprd change gb gb apply gstg apply gprd change gb gb apply gstg apply gprd change gb gb apply gstg apply gprd for step staging create
420,31906779,5.0,create ingress gke alertmanager to simplify add silence examine alert port forwarding this restrict access currently allow access we likely use gcp iap
421,31881534,3.0,implement elasticsearch operational logging utility script like implement elasticsearch operational logging utility script like got idea just wonder aware semanticlogger delivery use preference i prompt it lot handy thing include log elasticsearch desire not critical nice
422,31840173,2.0,implement actual proper opt parse bash script implement actual proper opt parse bash script at storage management utility
423,31838746,1.0,re factor information runbook utility command bash function support wiki cli argument flag re factor information command bash function support wiki cli argument flag this mean function invoke main repo directory directory wiki flag give this dependent implementation proper option parsing track issue
424,31809361,2.0,update disaster recovery page handbook reflect current state while answer customer question notice piece information datum handbook page describe dr posture this task review document update need regard current state recoverability render source
425,31804085,2.0,semantic release work terraform module the publish step pipeline mr merge work example pipeline simply i personally able reproduce precisely though member team from appear work expect person workstation alejandro module google pubsubbeat master semantic release pm semantic release ℹ run semantic release version pm semantic release loaded plugin verifycondition pm semantic release loaded plugin analyzecommit pm semantic release loaded plugin generatenote pm semantic release loaded plugin publish pm semantic release this run trigger know ci environment run dry run mode pm semantic release run automate release branch master dry run mode pm semantic release allow push git repository pm semantic release ℹ start step verifycondition plugin pm semantic release ℹ verify gitlab authentication pm semantic release complete step verifycondition plugin pm semantic release ℹ find git tag associate version pm semantic release ℹ find commit release pm semantic release ℹ start step analyzecommit plugin pm semantic release ℹ analyzing commit merge branch jarv add gke sink master feat add default sink gke exclusion application log see merge request gitlab com gl infra terraform module google pm semantic release ℹ the commit trigger release pm semantic release ℹ analyzing commit feat add default sink gke exclusion application log pm semantic release ℹ the release type commit minor pm semantic release ℹ analysis commit complete minor release pm semantic release complete step analyzecommit plugin pm semantic release ℹ the release version pm semantic release ℹ start step generatenote plugin pm semantic release complete step generatenote plugin pm semantic release skip tag creation dry run mode pm semantic release skip step publish plugin dry run mode pm semantic release publish release pm semantic release ℹ release note version in order unblock work perform push tag this investigate ensure terraform module impact
426,31802130,2.0,update create runbook increase reduce disk patroni cluster after increase disk patroni cluster observe restart postgresql node to increase security platform like update create runbook stop traffic node i suggest add tag node drain traffic noloadbalance true
427,31797992,2.0,license db extraction data warehouse the command owner acl se on extension g
428,31706514,2.0,create runbook resync delay replicas i like describe step execute restore delay replicas i think positive document possible engineer execute procedure need
429,31704706,2.0,resync dr delayed replica moment replication lag day we need hour primary cluster
430,31607481,4.0,regular predictable database latency spike lead web latency slowdown spike spin within minute follow time daily basis experience slowdown web possibly service db duration hour web duration minute spike cc
434,31501130,4.0,postgresql migration test staging in issue i like plan list step execute generate restore staging environment nowadays cluster postgresql we need execute following step install postgresql chef setup new instance chef stop traffic pg execute migration migration plan include script step ansible backup dump database cluster before the upgrade extra test qa stop traffic pg after finish migration test remove restore database backup instance pg restore traffic pg
435,31500788,8.0,create process feature stop control impact background migration database we face background migration generate load database cluster we like investigate possibility manage well situation future the idea issue find improve generate documentation background migration manage would goal following feature be possible stop migration setup interval migration execute change chunk datum affect pause migration
436,31398755,3.0,fix terraform warning deprecation we update target terraform version environment different reason leave cumulus warning output terraform some simple address like remove quote datum type variable deffinition string string require refactoring x x x x x x x x x x x x x
437,31370583,4.0,evaluate database lab delay replica we like test possibility database lab delay replica scenario actually scenario hour delay apply log main advantage we snapshot hour restore time easy the restore time image fast
438,31333694,5.0,change delay replica wal e wal change backup tool delay replica use wal g instead wal e
439,31303472,1.0,bump ruby version chef cookbook goal update pin ruby version chef cookbook reliably run rspec test locally bundle exec rspec change for chef cookbook pin ruby version update version file update file refer ruby container image version step for reference gitlab server cookbook definitely affect follow additional cookbook affect shell cookbook gitlab cookbook license gitlab cookbook omnibus gitlab gitlab gitlab gitlab omnibus gitlab gitlab gitlab gitlab background yesterday i discover know bug rubygems i run bundler install cookbook prerequisite goal time run cookbook rspec test locally bundle exec rspec the bug work around describe this bug require bundler ruby gem exactly match version bundler write indicate file bundled with line if version ruby defective rubygems allow new version bundler bundler command fail chef cookbook currently pin ruby version version file rbenv switch active ruby version update pin ruby version version avoid bug rubygems our chef repo currently pin ruby i like propose update pin cookbook use ruby version note cookbook ci pipeline test stage affect bug presumably run container start pristine ruby environment question team can think reason apart run cookbook rspec test let ci pipeline run integration test testing appropriate some cookbook lack rspec test recipe example problem shell cat version rbenv install rbenv version set msmiley src git gitlab gitlab cookbooks gitlab version gem install bundler fetching successfully instal parse documentation instal ri documentation do instal documentation bundler second gem instal bundle install traceback recent bin main src git public rbenv lib src git public rbenv lib find gem bundler executable bundle grep bundled with bundled with gem install bundler grep bundled with tail bundle install fetch gem metadata
440,31292481,2.0,create database user sidekiq create new database user sidekiq it good practice life easy identify statement log database specific application we like know percent statement primary database sidekiq redirect read statement secondary node
441,31279527,1.0,new dedicated gitlab qa mirror runner runner manager as like setup dedicated gitlab qa mirror runner runner manager project the set specification similar omnibus gitlab mirror runner runner manager i originally open mr chef repo point infrastructure involve step i anticipate the runner manager gitlab qa projects gcp folder probably new dedicated quality runner project any help appreciate thank
442,31239899,4.0,analyze query sidekiq the intention recognize select statement come read write pgbouncer pool primary database have number suggest change application use read database
443,31219344,2.0,bad gateway error testing purchase stage customer portal amanda rueda encounter bad gateway try complete simple purchase test staging customer portal occur browser clearing cache effect step reproduce stage gitlab page then select bronze plan purchase then nginx return url edit customer id test customer portal
444,31219311,5.0,migrate gitlab production project to effort dogfood incident management product feature remove dependency installation support export production project import a prerequisite move op assess production readiness installation while intent publish public issue page intent op installation responsibility operational workflow status page additionally adjustment require substantial change handbook instruct engineer on call open production incident op we begin campaign inform company switch encourage engineer business stakeholder verify request access op this necessary view contribute ongoing incident lastly redirect incident management automation workflow automation use op api endpoint interaction incident issue there number situation lose convenience cross referencing issue project we manage handle project primarily require participation infrastructure department it problematic need increase visibility issue impact business unit cc
445,31217526,2.0,simplify standardize path base haproxy blocking use file base block list simplifie common form acl addition make urgent change safe quick apply write new acls scratch useful list standardize include regexp match request path regexp match beginning request path implement today substre match request path computationally cheap flexible equivalent regexp matcher cidr match client ip cloudflare compatibility transformation regexp match request path rate limit configurable x request second client ip cloudflare compatibility transformation
446,31200227,2.0,document gcp escalation path runbooks this escalation policy need document runbook accessible incident manager
447,31192506,4.0,add extra box testing pg upgrade need add extra server test replication upgrade the box configuration primary box upgrade postgresql i cookbook recipe install version postgresql primary
448,31141840,1.0,verify sre access azure the follow sres need verify access azure portal x aamarsanaa x ahmad x cbarrett x cfurman x cmiskell x dsylva x hmeyer x hphilipps x msmiley x nnelson please reachout access portal cc
449,31137645,1.0,configure thanos scrape airflow the datum team prometheus metric endpoint ready scrape thanos need configure scrape endpoint store metric gather data team start monitoring alerting
450,31134970,8.0,create config setup chef new postgresql cluster we need able install package create folder structure configuration file postgresql host chef currently postgresql also need adapt monitoring backup patroni setup at moment recipe package available we need add chef install package pg create setup cluster folder structure render config file chef config chef proper patroni file change monitoring function postgresql change function point backup data folder postgresql version change patroni setup work data folder pg
451,31134677,4.0,create chef recipe cookbook install extension postgresql we need available new pg cluster need install extension we add config new cluster start require new restart future it add cookbook recipe pg
452,31134470,4.0,create script postgresql upgrade need script step database migration ansible consider step execute cancel total execution case failure execute bastion share session screen step pre check verify file ownership check version upgrade verify check delete verify monitoring function delete verify collation version upgrade step migration script stop connection verify stop cluster take snapshot database execute node apply pg configuration change parallel query parameter parallel autovacuum maybe node mr execute separate start cluster vacuumdb analyze primary vacummdb freeze primary run long start qa test enable traffic change node post check migration script check version upgrade create view function taht need monitor verify create properly change function monitor exporter
453,31113051,5.0,add elasticsearch key sli metric catalog we add key sli elasticsearch log cluster metric catalog order define slo alert dashboard base
454,31078638,1.0,rollout the prometheus x rollout release candidate subset node testing x rollout final release release candidate
455,31031271,1.0,dns certificate update bouncer i currently process move bouncer internal antispam tool gcp actually access i like point dns entry ip address gcp machine i provision and i need update https certificate rare instance serendipitous timing old cert expire thank let know need end
456,30994839,1.0,canary web saturate deploy please note incident relate sensitive datum security related consider label issue mark confidential incident summary during deploy canary sluggish behavior report slack metric high spike latency saturation canary web fleet once deploy finish recover affect web canary team attribution minute downtime degradation minute for calculate duration event use platform metrics dashboard look appdex slo violation impact metrics start following what impact incident service outage sub service brown exposure sensitive datum who impact incident external customer internal customer specific team how incident impact customer prevent x incorrect display y how attempt access impact service feature how customer affect how customer try access impact service feature include additional metric relevance web overview dashboard canary time rail controller metric saturation request second detection response start following how incident detect do alarming work expect how long start incident detection how long detection remediation be issue response incident bastion host access service available relevant team memeber page able root cause analysis the purpose document understand reason cause incident create mechanism prevent recur future a root cause person way writing refer system context specific actor follow s blameless manner core root cause analysis for necessary start incident question happen keep iterate ask time while hard rule time help question deep find actual root cause keep min come answer consider follow different branch example usage s the vehicle start problem why the battery dead why the alternator function why the alternator belt break why the alternator belt useful service life replace why the vehicle maintain accord recommend service schedule fifth root cause what go start follow identify thing work expect any additional out go particularly what improve start follow use root cause analysis explain improve prevent happen be improve detection time detection be improve response time response be exist issue prevent incident reduce impact do indication knowledge incident place corrective action list issue create corrective action incident for issue include follow bare issue link issue label action include estimate date completion corrective action incldue name individual own delivery corrective action guideline blameless rca guideline s
457,30945722,6.0,design database infrastructure praefect in order praefect support replication coordination failover rely postgresql database we define cost effective performant infrastructure support replication mind more info praefect datum model x x opt cloudsql maintenance notification address x configure prefer maintenance window day week time thought
458,30917235,2.0,create change request reduce number read replica as discuss issue the goal reduce replica production cluster we like node read receive traffic node standby traffic please create change request proceed
459,30731697,1.0,clarify duration deployer annotation text clarify duration annotation text old text print duration adjacent result confusing phrasing hour hour deployer finish deployer pipeline gprd take hour hour wall time this change aim separate clearly define duration deployer finish deployer pipeline gprd end end wall clock duration hour sum pipeline stage duration hour
460,30730945,1.0,ci job stall trace output chef repo job investigate ci job chef repo ci pipeline stall indefinitely detail the follow manually trigger job normal chef repo ci pipeline stall show job trace output web ui the purpose job push merge request change chef server subsequent chef client run apply chef server change initially suggest job run lose output however discover job run twice run succeeding second run stalling so successful run push change chef server that put suspecting job run stall report output
461,30723483,3.0,postgres replication lag hour archive recovery replica postgres replication lag frequently hour archive recovery replica especially high tuesday some initial diagnosis work undertake ongre gitlab from time time wal e wal fetching get stick please link incident issue detail
462,30722090,1.0,rca the sidekiq queue main stage meet latency slo please note incident relate sensitive datum security related consider label issue mark confidential incident gitlab com gl infra summary our default setting sidekiq db connection pool size equal make likely worker thread compete db connection node low as result see job time get db connection queue affect team attribution minute downtime degradation for calculate duration event use platform metrics dashboard look appdex slo violation impact metrics start following what impact incident service outage sub service brown exposure sensitive datum who impact incident external customer internal customer specific team how incident impact customer prevent x incorrect display y how attempt access impact service feature how customer affect how customer try access impact service feature include additional metric relevance provide relevant graph help understand impact incident dynamic detection response start following how incident detect do alarming work expect how long start incident detection how long detection remediation be issue response incident bastion host access service available relevant team memeber page able root cause analysis the purpose document understand reason cause incident create mechanism prevent recur future a root cause person way writing refer system context specific actor follow s blameless manner core root cause analysis for necessary start incident question happen keep iterate ask time while hard rule time help question deep find actual root cause keep min come answer consider follow different branch example usage s the vehicle start problem why the battery dead why the alternator function why the alternator belt break why the alternator belt useful service life replace why the vehicle maintain accord recommend service schedule fifth root cause what go start follow identify thing work expect any additional out go particularly what improve start follow use root cause analysis explain improve prevent happen be improve detection time detection be improve response time response be exist issue prevent incident reduce impact do indication knowledge incident place corrective action from we override minimum connection pool size elasticsearch export priority resolve issue the database connection pool minimum number pool size great value cc for high concurrency worth consider add extra connection pool concurrency insurance deadlock cc we report prometheus connection pool utilisation process interval eg second gauge metric we record maximum pool size add connection pool saturation saturation metric cc track cause exception rescue elastic indexer worker cc do report old sidekiq error future invocation do mask error exception reindex lose es index job cc guideline blameless rca guideline s
463,30682623,3.0,it appear disk soon alert the filesystem predict hour bash bundle exec knife ssh role gprd base fe api df grep g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g and
464,30681897,4.0,it appear component pgbouncer service consistently exceed slo this alert pop alert general slack channel today the pgbouncer service main stage component saturation exceed slo close capacity limit this mean resource run close capacity risk exceed current capacity limit so i click dashboard see that look pretty consistently good i imagine pool need adjust upward expand capacity i knowledge exist system resource support expansion
465,69129504,4.0,check performance bloat check we check offline get context deadline exceed seem reach timeout we investigate find performance improvement possible the query come external codebase recommendation
466,30647134,1.0,set redirect followup
467,30622920,2.0,create test environment stage capacity planning consider issue we need necessary environment proceed gpt x database server setup similar production x create box deploy performance tool configure box point gitlab instance database box
469,30600879,1.0,add auto scale policy snowplow collector aws there trouble collector snowplow morning there auto scaling group define collector policy advantage we add policy
470,30593946,1.0,remove availability zone snowplow collector auto scaling group the machine size specify available availability zone possibly replace az
471,30488450,1.0,regenerate docker machine cert gitlab runner upgrade task opportunistically regenerate ca client certificate docker machine ci runner manager host provision vms this avoid race condition corrupt cert expire naturally host service we deploy gitlab runner service cert expire plan downtime provide opportunity docker machine process run avoid race condition background currently ci runner manager host use docker machine provision vms run ci job docker container the manager host act ca sign client cert vm server cert the ca client cert year lifespan expire invocation docker machine create generate fresh pair cert however high throughput environment like likely multiple docker machine process concurrently try generate new cert overwrite work this lead mismatched corrupt ca client cert when occur host run docker machine provision new vm vms reject attempt authenticate dockerd daemon damaged untrusted client cert consequently manager host fail run job pull
472,30481273,2.0,terraform error create forwarding rule load balancer when delete load balancer create forwarding rule error specify subnet error error create forwardingrule googleapi error invalid value field network interface specify subnet network resource custom subnet mode invalid module gcp tcp lb internal line resource internal resource internal
473,30440115,1.0,assist support marquee customer help support follow task customer build share ci runner platform metric miss grafana dashboard help troubleshoot metric miss
475,30285613,4.0,create chronogram postgresql upgrade create chronogram well visibility blocker postgresql upgrade this chronogram task overview owner deadline date field possible blocker
476,30285198,8.0,define step postgresql upgrade restore backup production cluster replica read the idea task create test scenario postgresql upgrade pg database size like production we similar hardware setup production box box we idea estimative time need execute task upgrade the cluster node step step standby the follow step execute upgrade pg test server preliminary version upgrade before execute issue resolve relate server extension instal solution install pg some view function query postgre exporter incompatible pg solution like workaround rewrite view function add wrapper function query make copy config file to execute gitlab server follow step remove following view function pg view function make sure connection checkpoint master check replica status add pg line host reject connect postgre database select database checkpoint select stop pg run server install stop service ignore instal stop apt install main stop create path want new pg cluster correct permission mkdir opt gitlab postgresql chown gitlab psql gitlab psql opt gitlab postgresql chmod opt gitlab postgresql create new cluster database user gitlab psql master su gitlab psql lib opt gitlab postgresql enable connect locally trust opt gitlab postgresql data add follow line beginning file local trust install extension pg apt install repack compile source code wget unzip cd lib bin root install lib bin install postgresql server standby server extension apt install repack compile source code wget unzip cd lib bin root install lib bin check fine execute master su gitlab psql lib bin bindir lib bin bindir lib bin datadir opt gitlab postgresql datum datadir opt gitlab postgresql opt gitlab postgresql opt gitlab postgresql if result cluster compatible execute upgrade approximately sec su gitlab psql lib bin bindir lib bin bindir lib bin datadir opt gitlab postgresql datum datadir opt gitlab postgresql opt gitlab postgresql opt gitlab postgresql if result upgrade complete test start pg sure config correct value need test start stop pg service su gitlab psql cd lib opt gitlab postgresql opt gitlab postgresql stop server upgrade standby replicas create structure datum replicas mkdir opt gitlab postgresql chown gitlab psql gitlab psql opt gitlab postgresql chmod opt gitlab postgresql copy standby server datum pg delta pg configure connecto master pg cd opt gitlab postgresql rsync link inc recursive data opt gitlab postgresql config replica set necessary confing standby start master standby server late user gitlab replicator password password host ip create replication slot master select connect postgre master server create function view require postgre exporter sudo gitlab psql execute code create or replace function returns setof language sql security definer as select create view as select from create or replace function returns setof language sql security definer as select create view as select from grant all privileges on to grant all privileges on to create or replace function returns as select language sql stable create or replace function returns as select language sql stable create or replace function returns as select language sql stable create or replace function returns text as select language sql stable to collect statistic add temporaly line opt gitlab postgresql data reload conf select host gitlab superuser trust execute vaccumdb binarie follow parameter approximately min lib bin vacuumdb gitlab superuser remove comment line opt gitlab postgresql data reload conf select master server host reject local trust host gitlab superuser trust integrate pg ha solution patroni
477,30284876,4.0,create image test postgresql upgrade i suggest create image execute i suggest stop postgresql instance database consistent state the suggestion cluster node read write read only with postgresql actual version production version target migration also consider have restoration backup pg with image restore testing scenario easier test able iterate fast
480,30283479,8.0,verify optimize parameter pg upgrade our goal upgrade database cluster postgresql version need verify optimize setup parameter postgresql patroni consul database cluster please consider parallelism parameter optimize add thought new parameter value discuss well also check optimize parameter command execute this task priority need execute migration april
481,30249778,8.0,write design doc postgresql upgrade in document describe step upgrade mr
482,30231907,4.0,create test box simulate postgresql upgrade we need staging box following different version postgresql instal pg production pg target migration please create box spec production for need create new cookbook need complete task execute postgresql migration to execute initial test restore backup staging execute restore dump database with step able execute
483,30169532,1.0,document tf chef provisioning bootstraping process runbooks soc type i control remediation step vm provisioning the security compliance team prepare soc external audit and ask simple documentation build process provisioning bootstraping server production environment we perform workflow terraform chef i unable locate simple explanation process perhaps i overlook i find runbook directory outline workflow definition do in markdown file runbook directory identify machine image get vm creation preferably point code include workflow diagram outline human machine interaction file tool provision bootstrap image
484,30168629,4.0,blueprint postgresql upgrade please create blueprint postgresql upgrade
485,30152120,1.0,prometheus config reload rule change from slack last some change runbook get merge master night pick prometheus on investigation change roll rule directory look like chef tell prometheus reload configuration i huppe process right world we ensure chef perform config reload change
486,30126521,2.0,pagerduty services escalation policies users terraform pagerduty recently publish article advocate iac pagerduty object terraform this great idea or this issue seek collect feedback pro con commit put put resource version control
487,30121574,3.0,configure replication praefect for need create new storage node add exist virtual storage all setting require available omnibus x terraform change x chef repo change
488,30120782,2.0,error check query replica i try debug extract certain table fail replica we try query table like kill job i tell timeout postgre the late query time utc info root select d action reference from info b info event gitlab com db event type fail maybe related like able provide guidance insight happy provide information i need end
489,30027952,8.0,monitor alert elastic log cluster slo we define slo key sli implement monitoring alerting query latency availability query error rate delay log visibility completeness log
490,30027585,5.0,consider backup elastic visualization dashboard we keep kibana visualization dashboard version control mainly create ui we consider make sense export visualization dashboard configuration regularly able restore need
491,30025393,3.0,add alert fluentd we alert fluentd fail forward message possible metric watch fqdn type relate
492,30004127,5.0,allow rate limit specific url currently straightforward way rate limit access specific url we allow url block it useful able set limit time minute single client request url time minute url request total a flexible way specify add significantly site resilience wishlist it nice able specify url regex example rate limit file it nice able rate limit group user differently include unauthenticated user it nice able rate limit ip cidr range separately user it nice able limit criterion user agent thing consider some large organization nat user appear ip cloudflare appear lot request ip need able differentiate proxy server end user a ddos distribute appear come ip we way set high threshold time specific url request this high single ip threshold we careful limit internal component sidekiq inadvertently relate thing here incident prevent here related work happen product here related corrective action api rate limiting
493,29895201,1.0,replace stage certificate cloudflare the certificate update gkms vault server the key update there copy cert key cloudflare old version the new certificate console openssl null null openssl stdin db cf the cert key gitlab omnibus secret geo gstg gkms vault they retrieve like vault gitlab omnibus secret geo gstg jq gse g vault gitlab omnibus secret geo gstg jq gse g the cloudflare cert key need update new version cert key
494,29863428,1.0,scale praefect node cpu core on establish periodical cpu spike see praefect node actually cause chef client run suggest increase cpu core count mitigate problem let change
495,29826208,3.0,split secret geo secondary staging the geo secondary node staging currently use gitlab omnibus secret file rest environment in case fine case like redis server want configure secret use local omnibus instance this work secret configure chef assume want use the proposal change chef configuration node use different secret file then delete secret new file want apply node cange secret this probably scope issue
496,29824453,3.0,write runbook document write runbook document
497,29772770,1.0,add fluentd pubsub graph log dashboard on log dashboard graph fluentd bandwidth datum log stackdriver pubsub
498,29756649,4.0,create read replica license customer database analytic query the datum team need read replica license customer database purpose report snowflake as cloudsql pretty quick spin new the thing need similar vpc peering like production replica connect directly
499,29715262,3.0,configure external access in order work geo secondary instance need able admin interface i try different approach use exist haproxy dangerous require major change template instance include production order haproxy proxy request geo staging instance use gcp load balancer little heavy handed backend i try i discover change module type use provision geo staging instance generic store generic sv group write new module this addition add individual resource health check backend thing write new module right way work development work provisioning external ip address create firewall rule allow access directly node this i go moment simple need complicated this unblocked revisit future to provision external address need a static external ip associate node a firewall rule allow access node a dns entry a certificate san ssl connection interface
500,14073141,3.0,number postgresql database gprd change past minute alert the alert number postgresql database gprd change past minute number postgresql database gprd change past minute alert receive twice sep the alert sep the second sep notice appear change database configuration wrong way alert work todo check code alert reliable
501,14072755,1.0,add service log kibana we move project service relate log different file log it nice add log kibana like describe
502,14060305,3.0,add gcs storage thanos service in order reduce offload local storage requirement prometheus thanos upload tsdb datum gcs also similar todo x add gcs bucket op project thanos read write x configure sidecar upload datum x add store gateway provide read access x reduce prometheus local retention day x add compactor add downsample datum
503,14029269,3.0,setup data alert we need come strategy create data alert recently see alert define refactore alert long valid this problematic current configuration alert rule environment cause problem hardcode environment specific label
504,14011795,3.0,postmortem page incident this issue postmortem issue track get milestone
505,13996456,1.0,marvin access tristan jerome could marvin access permission thank
506,13995832,5.0,bigquery sink gitlab production log haproxy haproxy log long forward elasticcloud size continue run case basic query helpful investigate traffic i explore log sink directly bigquery support query in particular bulk query pattern path user agent come time month while user agent log elk sure storage stability impact my primary concern look haproxy tag log stackdriver appear different format jsonpayload i think try see result bigquery fast way iterate determine feasability x infra team set sink staging haproxy log staging collect day worth log x security examine result determine change need filter request change x iterate x infra create filter sink production collect hour x both team evaluate efficacy size
507,13994382,1.0,update pagerduty api key use cog got email pagerduty api deprecate in pagerduty key pm patricio am gitlab we probably check support remove zendesk likely need update gitlab cog key whereever email pagerduty rest api decommission long operational start october pdt your pagerduty account rest api key use day this message send administrator pagerduty account please forward message team ask migrate rest api click learn rest api decommissioning faq page please note pagerduty events api deprecate read faq step identify active rest api key feel free contact support unanswered question need assistance migrating
508,13964150,1.0,destroy report no impact unused machine deprovisione
509,13953961,1.0,email user list end early adopter program we end early adopter program discuss problem solve we like email user group impact make switchover october what need csv user group early adopter plan column email state username
510,13949790,5.0,setup delay postgre replica archive replica for dr purpose let setup postgre replica configurable replication lag feed wal archive
511,13939955,5.0,rename postgre archive be production replica differ replicas the replication lag replica start grow significantly today point repmgr run differ replicas streaming replication enable use wal shipping leave time migration node intentionally use sr sudo cat opt gitlab postgresql data recovery file create standby server use fetch wal chunk transition secondary possible specifie start postgresql server standby if parameter server stop recovery end archived wal reach try continue recovery fetch new wal segment connect primary server specify setting by default recovery recover end wal log so need recovery option if option unspecified string corresponding environment variable section check export pgpassword xxx user password xxx repmgr sslmode prefer lastly restore command run switch bin envdir wal env wal e bin wal e wal fetch f p question wal shipping instead stream replication symmetric setup lag high lag possible wal shipping aws way long reliable way sr time lag high need replica yes reason
512,13937039,1.0,product link resolve pricing page resolve
513,13935219,1.0,create io dashboard postgres disk we like gain insight io behavior postgre disk let add dashboard monitor place io wait iops throughput read throughput write as point metric some exist
514,13913576,1.0,asset canary work when canary asset work this appear production cdn canary asset production from staging i guess cdn host suppose however work check backend long exist should bring fix canary config cc want reason post canary
515,13908410,2.0,create well howto troubleshoot rail alert context
516,13907887,1.0,set canary environment drop recreate canary need add
517,13903751,3.0,automation infrastructure deployment in bid increase reliability consistency transparency asynchronous workflow begin automate current infrastructure deployment process we start simple scripted automation ci pipeline start consider change advanced tool necessary base result work since atlantis project sufficient security control public repository unable use time terraform enterprise provide similar advanced feature need evaluate cost well understanding pain point gitlab ci objectives master branch current state consistent live environment terraform plan master zero change terraform apply run merge all change environment code automation all change gprd apply validate gstg we obviously need define monitor report next step update ci pipeline run terraform plan environment add automatic apply stage gstg add optional manual apply stage op gprd setup require block manual apply stage op gprd
518,13902622,1.0,new user new project mau august we track user project internal use report interested party i like obtain count august for i output select from where between and thank lot smile
519,13901770,1.0,enable support able stackdriver log from namho andrew work come useful support access stackdriver research log fall elastic cloud need review security decide okay determine group create list user lyle
520,13900795,5.0,week begin migration project hashed storage relate migrate project hashed storage
521,13900771,5.0,week hashed storage begin migration gitlab com gitlab org project relate close begin migration process gitlab com gitlab org project this go well script x gather list project id migrate x perform migration x validate project complete migration
522,13900748,1.0,week enable hashed storage permanently new project relate in week temporarily enable hashed storage new project no issue find in issue enabling hashed storage new project leave feature enable we continue monitor problem x enable hash storage new project automatic migration rename move easy story complete joy x monitor issue rename move x if pop rename move migration flip feature toggle hashed storage new project enable
523,13900730,1.0,week temporarily enable hashed storage new project relate x flip feature toggle disable migration rename move project step list parent ticket x enable hashed storage option hour monitor x after hour disable hashed storage option new project x flip feature toggle initial state step list parent ticket
524,13890067,1.0,sometimes gcs return server error status code gitlab rail object storage integration feature it kind area lfs avatar job artifact etc it gcs return status code server error gitlab rail try access you bunch error happen server production staging from i see happen transiently at moment clue cause we know issue fall infrastructure gitlab ce for create infrastrcture project
525,13868516,3.0,setup prometheus pushgateway have prometheus pushgateway help solve observability issue allow ci job post information relate ci run as example setup ci job routinely perform restore postgre backup end job post time take push gatewat we decide well let exec business decide acceptable restore time postgres backup this time limit codify alert alertmanager if ci job take long x hour raise alert additionally trend datum predict plan ahead cc scenario come discussion configure takeoff send metric relate deployment time host latency histogram deployment progress gauge gitlab give metric able annotate grafana dashboard detail deploy we able compare error rate deployment great ease present allow easy integration non prometheus component for example redis sentinel allow cli command run failover occur we use increment counter failover push gateway setup robust alert present before start build estimate effort chef vs update team estimate cc
526,13868201,2.0,use cicd environment role update chef repo we start op instance role environment update at point role update gprd gstg merge automatically we want pipeline apply change gstg check alert apply gprd manual step gprd apply work
527,13867132,3.0,use cicd upload cookbook chef individual cookbook upload chef merge master for work probably add pipeline check ensure version bump
528,13865657,2.0,design document canary environment use backend server weight start send small percentage production traffic canary web node
529,13749361,1.0,disable transparent huge pages thp redis machine i think i warning redis log redis strongly recommend disable thp avoid latency it disable simply sh echo kernel mm enable right set following sh cat kernel mm enable madvise
530,13718658,1.0,arihant production access as move gcp state create issue production access
531,13701151,2.0,unmount nfs gitaly the tripswitch path access rails sidekiq enable step take remove nfs architecture now use nfs good knowledge test i think forward unmounte nfs drive staging manual testing and production
532,13696150,1.0,configure snowplow integration production we add snowplow configuration stage explode we event successfully populate snowflake dw configure turn integration production in admin setting need enable snowplow set following value collector uri site id gitlab cookie domain snowplow
533,13728932,1.0,environment run ee master refresh nightly problem we currently run ce master refresh nightly in addition serve production environment gitlab security relate merge request great place test feature master live environment real datum early rcs ee production datum run master so want test feature master feature rc oftentime mean week later test feature master super helpful pretty gitlab for pm review feature help qa fa this especially helpful new feature integrate late great code branch new functionality currently alternative set gdk locally super easy gitlabber proposal have environment run ee master have refresh nightly great weekly have type real datum great have datum refresh awesome but crucial second iteration just have environment master running mean user create test datum manually
534,13653802,2.0,some user have trouble perform deploy stopping chef client nfs sidekiq mailroom web api git registry deploy node connection close remote host connection close remote host connection close remote host connection close remote host connection close remote host error broken pipe jivanvl gitlab takeoff lib step abort fail execute command bundle exec knife ssh fqdn role gprd base stor nfs or role gprd base sidekiq or role gprd base mailroom or role gprd base fe web or role gprd base fe api or role gprd base fe git or role gprd base fe registry or role gprd base deploy node sudo service chef client stop scripterror jivanvl gitlab takeoff lib steps jivanvl gitlab takeoff lib step run jivanvl gitlab takeoff lib block run jivanvl gitlab takeoff lib jivanvl gitlab takeoff lib run bin takeoff block main lib ruby gem lib lib ruby gem lib bin takeoff main at member report nearly error message
535,13631669,5.0,pgbouncer authentication incorrectly configure vault pgbouncer incorrect credential store vault the time gitlab ctl reconfigure run db server time cause outage pgbouncer able properly authenticate master postgre node in order close issue correct password vault ensure chef place proper credential run ensure gitlab ctl reconfigure complete database pgbouncer server validate pgbouncer successful connect master node the staging production
536,13631324,1.0,run low disk space disk space node take docker container running discourse soon wo available space reference this node this node ought rebuild
537,13613595,1.0,update infrastructure section handbook iteration need refresh bit work serve great starting point update diagram i think probably scaffolding big update relevant gcp cc on page other related pages current architecture need update propose cloud native architecture need update production environment consolidate current architecture high level components view borrow diagram readiness review pod definition this probably stay maybe use update database architecture need update monitoring architecture need update logging architecture either reference link log page infrastructure services their slx i think remove solid plan cc host naming standards update gcp hostnames update gcp service tiers this pretty environment need update location update gcp tld zones need update internal address need update internal networking scheme replace pointer terraform production remove canary remove stage remove gitlabgeoprd remove gitlabgeostg remove gitlabops remove remote access replace pointer howtos bastion access azure remove load balancers remove cover high level diagram service nodes remove digital ocean need update aws need update google cloud remove gcp monitor reference monitor section technology gitlab
538,13575681,1.0,create blackbox scraper alert this public dashboard service we want hit api health endpoint port blackbox scraper internal ip hit endpoint public
539,13502571,1.0,fix mailgun email login apparently log mailgun enable email mailing list code
540,13487380,1.0,gcp permission cleanup add user group remove specific permission see issue
541,13483938,1.0,week test limit repos hashed storage week issue follow step i discuss additional step initial rollout gitlab dot com first week let enable new project short period time hour monitor coordinate geo team repos time window
542,13483439,1.0,change enabled endpoint staging reason support prioritize static resource ought reachable problem staging use gl staging cdn endpoint support fix change environment variable gl staging as add benefit enable see fastly free tls documentation
543,13480437,1.0,ssl expiration the certificate expire page serve application
544,14000505,2.0,define database reliability engineer role this track work define dbre role currently discuss
545,13460797,2.0,investigate deploy ebpf metric exporter the latency histogram disk io this useful debug view impact slow throttle storage
546,13460848,1.0,database access for dylan griffith the configure team regularly find need query database certain thing like debug production incident understand row certain table affect migration analytic usage product query db understand impact introduce new constraint db we anybody team execute query usually rely like case analytic product usage need slightly exploratory query know look it hard prod db access count query tell kind query want understand datum well could provide access configure team well setup manage kind problem future external support
547,13460332,8.0,hashed storage rollout gitlab dot com follow step i discuss additional step initial rollout gitlab dot com x first week start migrate project gitlab com gitlab org x second week flip feature toggle disable migration rename move project enable hashed storage new project short period time hour disable hashed storage new project flip feature toggle initial state x third week enable hashed storage new project automatic migration rename move project monitor issue rename move if pop rename move migration flip feature toggle hashed storage new project enable x fourth week migrate gitlab com gitlab org project x fifth week start migrate user repository batch k day repository storage etc x resolve failure x resolve failure because introduce feature migrate hashed storage rename move project want reduce risk introduce feature toggle disable behavior separate issue i discuss after finalize migration gitlab dot com list learn corner case find when precondition fail visibility fail that require extra debugging allow improve code the failure corner case bug part codebase rely the rake task include pagination param sre want order throttle execution initially sre custom script trigger throttle project comply gitlab dot com load we improve thing rake task add extra param lack initial batch trigger custom bash script call rake task paginate files object storage hash list candidate the code corrupt mess object storage get noise effect the query fix remove list candidate trigger we add extra bit ignore attempt schedule migration precondition initial goal build fast mean optimistic environment stop prevent data loss confidence this necessary scale gitlab dot com the conservative approach help achieve data loss because conservative approach require multiple iteration find fix corner case we find previous attempt leave folder carrierwave folder structure file inside we consider ok try migrate for gitlab dot com find case early attempt tmp folder case require manually intervention allow retry we lack instrumentation log good beginning we improve information log that allow follow migration watch specific worker class logstash instance because speed concern migration happen sidekiq schedule trigger sidekiq this hard debug ideally background foreground mode i create proposal build internal framework lesson learn help future data migration permission problem incorrect permission disk remain project this previous sre support work hashed storage impact script permissions fix manually sql timeout attempt database performance degrade dot com grow reduce timeout limit we add partial index help speed the index track remain legacy storage project migrate index zero cost repository reference counter inconsistent repository probably early bug we manually reset counter project there code available add mr we find project legacy storage pende delete remove create the solution find permanent fix schedule removal manually there documentation
548,13452769,2.0,alert fail basebackup we discuss couple way s alert fail restore edit i prefer hold run ops instance alert cronjob fail
549,13435488,1.0,documentation gkms secret storage gitlab servers document architecture page usage gkms methodology use gkms automate chef run secret management
550,13432006,1.0,adrielsantiago need account join recently need staging account help test change rcs thank create issue recommend
551,13429069,3.0,page oncall redi fail there specific alert type failover redis redi cache postgre although failover happen service interruption usually indicate problem help shorten troubleshooting time alert immediately
552,13410376,2.0,reconfigure gitaly cgroup cookbook match new file server node gitaly currently use default cgroup setting gb memory cpu share etc gitaly cgroup cookbook however machine gitaly run big memory ensure optimal setting root free total free shared buff cache available mem swap gb unused memory disappoint root cat fs cgroup memory gitlab gitaly cgroup constrain gb memory maximum available ram dedicated gitaly machine sup cc
553,13382075,1.0,few request go vms load balance pool it appear gprd respond fast instance graph the reason few request route result liveness check skew latency result from couple open question should exclude liveness latency measurement why receive few request relative liveness out request liveness liveness
554,13380775,4.0,move chatop runner gitlab op in order chatop network access gstg gprd gitlab op gcp project register instance
555,13363801,2.0,create wildcard certificate we currently use self sign certificate ee review app work i following visit firefox the owner configure website improperly to protect information steal firefox connect website this site use http strict transport security hsts specify firefox connect securely as result possible add exception certificate use invalid security certificate the certificate trust self sign the certificate valid error code we proper wildcard certificate
556,13359158,2.0,infrastructure op environment need monitoring we currently alert manager op environment mean poor optic important operational infrastructure this include nginx proxy public grafana we deploy dedicated prometheus alertmanager
557,13347037,5.0,create vm cloud inventory iteration after get quetion tool produce kind inventory host ip help track thing cloud provider this help new people find thing before come plan exist tool use do want output spreadsheet get set eye review plan
558,13324527,1.0,decommission sidekiq trace server with object storage migration complete decommission sidekiq trace server reintegrate queue besteffort
559,13297246,1.0,new storage servers we need create new storage server soon we approach mark this obviously post migration x build new storage server x add storage server chef repo mount nfs x ensure server set storage option x attempt migrate test project new server x update default repository location gitlab admin interface
560,13226598,1.0,fix chef client issue rc incident we lot chef client stop stale errore this issue tracking point sure good state aug
561,13217341,3.0,create alert dashboard sidekiq exception we currently little visibility nice add sidekiq stat dashboard azure gcp during failover rehearsal see large number exception alert
562,14000516,1.0,wal e wal push overwrite exist datum i wonder wal e overwrite exist wal segment sure happen docs fail situation the archive command generally design refuse overwrite pre existing archive file this important safety feature preserve integrity archive case administrator error send output different server archive directory
563,13305504,8.0,registry api web backend unhealthy entire site go we load balancer serve gcp git these load balancer serve traffic haproxy vms follow health check validate health corresponding backend frontend bind mode http option splice auto acl lt monitor uri ssh monitor fail frontend bind mode http option splice auto acl lt monitor uri http monitor fail frontend check bind mode http option splice auto acl lt acl lt acl lt monitor uri monitor fail check mean web api registry zero healthy backend host cease serve traffic this configuration put unnecessary coupling registry rest site registry serve dedicated load balancer git traffic route git vms probably git traffic different fleet use web fleet
564,12876064,1.0,canary create kubernetes cluster fetch google project billing status summary when create new kubernetes cluster i select project google cloud platform project field request google block csp step reproduce turn canary set cookie go operations kubernetes select create new cluster gke select project google cloud platform project field example project what current bug behavior the google cloud platform project field stick validate project billing status select anymore what expect correct behavior we successfully validate project billing status field show gcp project select user relevant log screenshot console show follow error refuse frame violate follow content security policy directive frame src self with i receive follow header content security policy object src worker src blob script src self unsafe inline unsafe eval style src self unsafe inline img src datum blob frame src self frame ancestor self connect src self report uri without content security policy object src worker src blob script src self unsafe inline unsafe eval style src self unsafe inline img src datum blob frame src self frame ancestor self connect src self report uri so canary server miss csp header output check this bug happen possible fix add src frame src csp directive
565,13305476,4.0,migrate registry bucket gcs as standardize registry bucket this likely work however bucket add
566,12598698,1.0,access request aws key billing info meltano in order help meltano billing analysis look set billing aws iam profile see turn help cc
567,13304968,1.0,architecture diagram aid systematic debugging per failover the debugging unnecessarily difficult have puzzle exact cdn setup we ensure date architecture diagram exist gprd document route take variation present tcp service debug thing systematically come
568,12530971,1.0,chef access andrewn relate while i production access i ideally like chef access while i plan change have access allow run command like knife node host
569,12511484,1.0,set meltano domain gitlab pages we like use meltano domain list gitlab pages the project link issue set page to set need set dns record ssl certificate
570,13304976,1.0,decide endpoint use haproxy health check in azure use go way rail serve in gcp use hit database there discussion good idea maybe use health check depend database
571,13304997,1.0,re enable archiving mode postgre node gcp they disable why quoting slack my concern single bucket azure gcp failover demos push wal segment different timeline i think confuse wal e failback constantly look new timeline fetch segment after failover permanently enable
572,13305245,1.0,fix reverse dns gcp ip note failover various ip gcp miss appropriate reverse dns for instance resolve reverse dns since post failover want point ip ensure reverse dns set we send email ip super important i think desirable other ip matter include one exist resolve perhaps i miss we consider set appropriate rdns ip gstg
573,12301013,1.0,improve download speed globally currently take hour morning hour afternoon install gitlab ce ee shanghai china official instruction connect would need improve significantly well experience user customer china
574,14000523,3.0,alert database backup restore we database backup restore automation let add alert process let know restore fail reason
575,13305493,2.0,repmgr add centralized logging we currently include probably example info connect database host user dbname error connection database fail connect server connection refuse be server run host accept tcp ip connection port info connect database host user dbname error connection database fail connect server connection refuse be server run host accept tcp ip connection port
576,13305489,2.0,consul client consul cluster add centralized logging we currently include probably i think simply add exist postgre index consul client unfortunately log unstructured log gitlab consul current example info serf eventmemberfaile err agent fail invoke watch handler opt gitlab consul script exit status info serf eventmemberjoin err agent fail invoke watch handler opt gitlab consul script failover consul server log systemd journal currently fluentd plugin need add
577,12112496,1.0,monitor consul member we instance postgre secondary miss consul it useful prometheus check alert member count postgre detect situation automatically there consul exporter available
578,12069327,5.0,add centralize logging prometheus grafana host when troubleshoot difficult determine go logging rotate host quickly we add prometheus thanos grafana host centralized logging we add troubleshooting step runbook project repository see x prometheus x thanos component compact store query x grafana x alertmanager x pushgateway
579,13498116,8.0,create public dashboard service to replace old need new sync dashboard
580,13305512,2.0,migrate sidekiq worker dashboard prometheus currently sidekiq worker dashboard rely influxdb since strategically move influxdb prometheus current plan leave influxdb azure need rebuild dashboard metric prometheus point important dashboard frequently migration references issue
581,13305231,2.0,migrate rail controller dashboard prometheus currently rail controller dashboard rely influxdb since strategically move influxdb prometheus current plan leave influxdb azure need rebuild dashboard metric prometheus point important dashboard frequently migration references issue
582,13305468,2.0,migrate grape endpoint dashboard prometheus currently grape endpoints dashboard rely influxdb since strategically move influxdb prometheus current plan leave influxdb azure need rebuild dashboard metric prometheus point important dashboard frequently migration references issue
583,11619150,1.0,make sure gcp sizing update change azure week we item sure scale gcp host similar azure need api nodes sidekiq worker worker chef role sidekiq queue any tuning nfs nodes pgbouncer change prometheus cc
584,11302970,1.0,bastion access gprd reference reference in order conduct migration production failover rehearsal grant follow gitlab team member bastion access gprd how access bastions
585,11302840,1.0,bastion access gstg reference reference in order conduct migration production failover rehearsal grant follow gitlab team member bastion access gstg how access bastions
586,11302497,1.0,ssh shell access production reference reference in order conduct migration production failover rehearsal grant follow gitlab team member ssh shell access production
587,11302385,1.0,ssh shell access staging reference reference in order conduct migration production failover rehearsal grant follow gitlab team member ssh shell access staging
588,11302358,1.0,rail access production reference reference in order conduct migration production failover rehearsal grant follow gitlab team member gitlab rails access production please note require complete
589,11302305,1.0,gitlab admin access production reference reference in order conduct migration production failover rehearsal grant follow gitlab team member gitlab admin access x x
590,11302265,1.0,gitlab admin access staging reference reference in order conduct migration production failover rehearsal grant follow gitlab team member gitlab admin access x x x
591,11283994,1.0,allow rail console db console nfs user access bastion host propose fix currently user rail console db console nfs access allow bastion host able access host previously it good previous access restore
592,13676071,1.0,gcp security review followup firewall rule by role subnet review communication vpc subnet currently organize role well catalog active connection production environment recommend additional configuration firewall rule etc
593,13305352,1.0,update documentation runner configuration gcp summary the current documentation production runner configuration date update reflect addition change gcp environment link
594,13305367,1.0,haproxy we long need ssl configuration remove haproxy cookbook original description accord haproxy configuration include section domain this counterpart gprd entry dns do require action assume obsolete remove haproxy configuration currently
595,13305360,1.0,set imap prometheus exporter monitor service desk right blind service desk reply email a quick thing improve run imap prometheus exporter we need fork export unseen message hard at minimum i run local exporter watch gcp failover
596,13362078,1.0,clean delete repository similar lot delete repository disk clean migration issue we want build product
597,10843587,3.0,gemnasium service backup restore test in bring attention gemnasium service utilize gcp sql product backup it determine acceptable backup solution need test restore
598,13305319,2.0,investigate postgresql service shutdown db gprd restart about hour ago go come reason sudo gitlab ctl status run consul pid run log pid run logrotate pid run log pid postgresql run log pid run remote syslog pid run log pid repmgrd normally want run log pid manually start work i log suggest come automatically
599,10641750,2.0,sidekiq run note thank this block attempt plan failover
601,14961158,4.0,upgrade haproxy we run haproxy go allll way in order support probably thing consider upgrade suggest role staging canary i believe problem know well put milestone scheduling consideration
602,14960184,1.0,aws do access our new security operations engineer need read access aws do production account the engineer handle
603,14959799,1.0,access create project google api i currently work deploy tool link sfdc instance google sheets this easy rep mass update opportunity one step installation setup create custom google api project page install instruction i need access create project edit project instance i twice installation production installation i sure affect permission set i need i update project create
604,14957229,8.0,reboot api git sidekiq web fleet in order pick late kernel coordinated reboot fleet this rolling fashion node time make sure drain lb this followup
605,14947093,1.0,upgrade grafana there nice feature interestingly build support stackdriver put milestone scheduling consideration cc
606,14936975,1.0,redirect comparison pages we like tool this require redirect order route existing page new url here spreadsheet require redirect source url desire blocking work before redirect place warning note not url swap s comparison devop tool see redirect spreadsheet correct url note any additional comparison page create go live need added redirect spreadsheet note this timing merge need coordinate correspond merge if redirect merge early web visitor send page exist if redirect merge late exist link google juice page cc
608,14904892,1.0,enable cloudtrail aws we ensure global trail enable cloudtrail aws api audit log this require create configure new bucket cloudtrail log appropriate bucket exist create default trail default trail want ensure apply region step
609,14893756,5.0,http target miss tls upgrade there instance http endpoint redirect x x x x x x x x x
610,14884201,5.0,design document git workflow ci cd this document describe git workflow facilitate ci cd deploy infrastructure application component easy manage to summarize standard gitlab flow workflow desire model manage related resource this document discuss use common git workflow gitlab ci cd tool set manage resource terraform chef kubernetes monitoring deployments etc the document review the initial merge request it consider parallel dogfoode ci cd
611,14865964,1.0,renewal tls certificate on november tls certificate expire these certificate gitlab charts review apps development envs distribution team the current certificate store cloud native vault great replace ac manage sslmate automate renewal
612,14860691,1.0,tls certificate please register tls certificate we go run web application host like secure
613,14857043,2.0,transition backups cloud storage plan action the discourse forum engine ability use aw backup target instead local disk nightly backup we x create aws bucket backups x configure admin interface use bucket x verify backup write bucket original issue description the forum service backup process ownership service lack ability restore give point document backup store disk host we want probably ship offsite cold storage case disaster recovery
614,14856898,5.0,runbook monitor with underway issue gain specificity ultimate input corresponding production readiness review gitlab com gl infra readiness migrating launch service gke work incur corresponding change requirement specific monitor service metric available norm establish manage log etc this issue focused author initial set runbook update relevant architecture page handbook properly comprehensively document infrastructure forum service original description we appropriate monitoring forum service the alert notify backup fail or pingdom tell we standard set prometheus metric collect box we unable proactively handle potential situation arise use issue discuss well manage box
615,14841496,3.0,review setup iteration firewall change security team staging this comment propose set rule review after review create production issue rule change change staging
616,14834812,1.0,fastly potentially serve incorrect content user discovery an example url provide test a slack conversation unthreaded thread some team member report different version site multiple hour successful deploy one user report deploy mobile old version show workstation unamused use fastly check cache feature i discover endpoint differ version page example those question cache cache i open support ticket fastly i hesitate purge cache case able perform troubleshooting as fyi backup time cache configure hour be deploy complete hour prior fastly ought late version site support fastly support ticket
617,14828768,5.0,blog post dr replicas see this issue track work milestone
618,14816677,4.0,onboarde architectural program to improve onboarding new sre facelift there good pointer lot detail lack this issue collect thought handbook mr i like expand onboarde include general architecture overviews this include trace different type request architecture since page serve documentation index gear new hire section flesh bit i replacement onboarding template way augment information useful non sre typical day day team oncall overview where find thing fleet information log store find monitoring alert rule dashboard rail database access terraform environment state file share configuration overview variable etc chef update cookbook automate cookbook version update node attribute update secret architecture overview different type request request public api git ssh git repository overview page request registry download project zip file download project archive request raw file cicd job release distilled version release work deployment work chatop
619,14816145,2.0,evaluate stickiness setting haproxy backend currently haproxy backend configuration pool server round robin exception websocket ssh page websocket balance roundrobin cookie prefix nocache ssh balance source hash type consistent page page balance source hash type consistent the backend api api xx nodes git xx nodes web web xx nodes web cny xx nodes registry registry xx node use roundrobin load balancing as start production traffic canary probably consider make request sticky client potentially multiple version application single page load for web mean modify load balance algorithm use session cookie balance roundrobin cookie prefix nocache for api mean modify load balance algorithm use source ip balance source hash type consistent i wonder smart traffic come cicd until able separate traffic probably leave roundrobin for registry i sure matter use source ip balance source hash type consistent
620,14792324,1.0,user receive email we receive couple request relate service desk send email user receive email ticket few discussion internal can check investigate
621,14788640,2.0,request update the forum need manual update ssh node sh cd discourse git pull origin master rebuild app i create runbook
622,14786753,2.0,enable git ssh now land production soon rc work git http client request git ssh in order enable production need set following acceptenv now security consideration and man page sshd be warn environment variable bypass restrict user environment for reason care take use directive the default accept environment variable so i think need security evaluation enable give restrict i big concern particular situation ping evaluate security also enable directly omnibus setting possible i find docker relate a fact git protocol it opt client gitlab user support git onwards need explicitily pass configuration enable not git command execute evaluate string contain ignore wo pass git default mean enable ssh config people git able use ssh use instead use http in doc point user enable premise decide configure omnibus directly git protocol man page sshd cc
623,14786669,4.0,setup geo instance right regular deployment geo could simple set single geo instance ops instance deploy regularly that way ensure database migration work ensure exist functionality work test new functionality
624,14780650,1.0,rail console access wvandenberg hello team i need rail console access allow ownership gitlab page alike can assist grant access thank request
625,14768974,1.0,unable unarchive project a user unable unarchive project i believe gcp migration i unable reproduce project
626,14760740,5.0,database reviews new issue current milestone contain pende review x x x x x x thursday x thursday x x x x x x x x x
627,14746994,1.0,create critical pd alert haproxy connection error corrective action alert currently good way determine malfunction backend server accept ssh pass rail healthcheck discussion improve healthcheck generally
628,14715420,1.0,iam policy gitlab internal for configure team similar i like request permission follow people mcabrera tkuah taurie mgreile dgruesso jerasmus they require iam permission gitlab project role role
629,14710648,8.0,use correspond backend api git traffic our route logic canary currently which mean request web request cookie set web fleet ideally public api git ssh use new canary git api server
630,14688316,3.0,convert environment label server in order improve metric alert routing maintenance use prometheus external label external label allow prometheus assign label base view system we currently populate label provider gcp azure etc region east etc monitor default app etc we currently overload monitor thing like gprd default i propose change add new env label contain environment prometheus server this allow transition alert route new label
631,14668026,2.0,evaluate time incremental rollout host server we go ship delay job feature autodevops time incremental rollout we go evaluate work host server autodevops time incremental rollout mode evaluation plan enable term evaluation date x wait daily sync x create sample project new autodevops deployment strategy sure fully functional x check health metric log crash report enable term it depend rm plan evaluation date x rc new code deploy x create sample project new autodevops deployment strategy sure fully functional x check health metric log crash report enable term it depend rm plan evaluation date x rc new code deploy x create sample project new autodevops deployment strategy sure fully functional x check health metric log crash report note this feature feature flag enable default see this feature run user manually change deployment strategy autodevops presumably publish release post user try use usage new sidekiq worker gradually increase time eye server health check health metric log crash report do new worker run properly this use namespace priority high do new worker pressurize worker namespace be stale delay job zero be crash report relate feature sentry stuckcijobworker use index scan properly ref cc feature flag the feature flag the new autodevops deployment strategy time incremental rollout base delay job feature by disable effectively revert time incremental rollout manual incremental rollout also stop new creation delayed job check enable enable feature disable feature
632,14663527,8.0,out disk space error inode artifact tmp directory sentry error all api server complain similar error no space leave device opt gitlab gitlab rail share artifact tmp the disk i suspect inode parent directory
633,14655002,1.0,move project com group what move gitlab com group why the handbook say project complete gitlab employment gitlab namespace i like project move announce gitlab page url why infrastructure change group need owner project creator maintainer target group i think i able maintainer owner project see
634,14653331,1.0,rollout sidekiq reliable fetcher we issue it implement expect review merge by default disabled enable feature flag the state flag consider run sidekiq process so need restart sidekiq process node enable gradually go to monitor state sidekiq check grafana dashboard new working tab admin area monitor background job if go wrong disable feature flag restart sidekiq process i need assistance production team
635,14638385,1.0,request access aciciu hi guy i try account work do mind create account staging environment
636,14638204,1.0,request access tpresa my account work log i like request access
637,14632494,8.0,database reviews i track pende database review list mr check i as discuss today idea create issue database reviews milestone maintain list pende database review the issue assign the idea react ping mr link mr list the goal distribute review work communicate review i tend duplicate list i get ping repeatedly every ping cycle mean distraction worth track this current list let use issue extend list milestone create new issue milestone x x kamil x x x x gitlab restore x x x x x x x x x x x x x x x x
638,14630448,1.0,create new wildcard certificate domain ce from meeting today address cleanup scalability team like create new cluster ce review app project meeting note currently create ce ee this prove challenge debug implement cleanup scale we like use cluster degree separation the load ce lot ee the benefit have cluster separation stability clean ce unstable ee affect ease debug phase implementation possibility have aggressive clean ce short term help assign appreciated pray i set weight similar request link
639,14621925,3.0,redis cache failover lead datum corruption currently save disabled redis cache instance this change place january when sentinel detect reboot decide wait original master come online fail secondary we experience saturday plan failover it mention redis issue reasoning behaviour clear when couple current setup follow scenario occur redis snapshot file dir unspecified date we reboot redis master since save redis shutdown save snapshot design redis restart on start begin load rdb snapshot file this file week old load complete slave resync master overwrite current cache old invalidated cache the application request cache item return previously invalidate datum this datum write persistent store eg postgre lead corruption see cc
640,14615910,3.0,tune threshold sidekiq exception alert we recently create new alert sidekiq exception count quickly notice see radically different volume high controller particular repositoryupdatemirrorwork after look look like majority legitimate albeit misconfigured job for easy fix alert useful cut noise implement separate threshold repositoryupdatemirrorworker base set repositoryupdatemirrorwork appropriate
641,14610224,1.0,access i permission i project activity assume i i permission
642,14604190,1.0,ship new file elasticsearch in add new json file log gitlab gitlab rail log gitlab gitlab rail we ship elasticsearch stackdriver etc
643,14599350,1.0,chef server certificate expire soon sslab provide certificate ready current certificate expire day replace server
644,14564889,2.0,when canary enable customer report broken asset from thread split small percentage production traffic canary appear root page load css correctly
646,14529363,2.0,redirect app sec page please redirect thank
647,14513053,1.0,error page include link when user present error page link
648,14511922,2.0,separate gitlab page rest web worker fleet currently gitlab page run alongside unicorn web worker node the traffic profile gitlab pages growth relate profile rest application have service run alongside web couple scaling rails web service pages service this disadvantage we instance page service run alongside web worker this vastly need each page daemon load index memory compete unicorn fleet memory consumption traffic volume instance low page deployment restart notoriously slow have unnecessary additional worker exasperate problem page require additional nfs mount web worker require split allow drop nfs mount cc
649,14502779,1.0,pullmirrorsoverduequeuetoolarge stage trigger pager duty the pullmirrorsoverduequeuetoolarge gstg trigger pagerduty this mr remove filtering alert manager decide page since alert manager make wrong decision i add mr remediate immediate problem need well solution the pagerduty alert look like label alertname pullmirrorsoverduequeuetoolarge channel backend monitor gstg app pager pagerduty provider gcp region east replica severity critical annotation description on average overdue pull mirror job minute check runbook troubleshoot large pull mirror title large number overdue pull mirror job source label alertname pullmirrorsoverduequeuetoolarge channel backend monitor gstg app pager pagerduty provider gcp region east replica severity critical annotation description on average overdue pull mirror job minute check runbook troubleshoot large pull mirror title large number overdue pull mirror job source
650,14502182,1.0,dashboardsgitlabcomdown alert grafana server process crash run chef manually bring reboot server the process come server the alert clear minute later i sure determine process crash help figure happen
651,14500572,5.0,add ssl version ha proxy logging the ssl version handshake good information track especially look continue improvement ssl handshake remove weak deprecate method thing do x add sslv template ft x update fluentd config add field ha proxy logs x ensure stack driver ingestion issue
652,14498208,1.0,request specific permission gcp gitlab demos project for partner demonstration work i need able test install gitlab google marketplace gcp i request account dgordon give proper privilege i i cluster i create gitlab demos project when i try install marketplace give error i need kubernetes engine admin permission thank
653,14491301,1.0,redirect couple whitepaper we midst move content email gate i like redirect old url new landing page here detail old url redirect url resource gitlab move git whitepaper move resource gitlab scale ci cd whitepaper scale ci thanks
654,14483845,3.0,plan make source truth operation repos chef terraform automation give automate infrastructure repos cicd instance i think probably time consider switch source truth reason the deployment submit mr deployment run cicd job important approve reviewing we want encourage contribution repository ensure little friction team member contribute we currently setup email address login view protect repository all repository mirror the downside plan way wide community contribute wo easy way enable enable wide community create account option later consider this propose high level plan sane way plan chef repo x prevent push branch setting x export import project x notify member update remote x setup repository push private project x update description clear source gitlab com infrastructure x prevent push branch setting x export import project x notify member update remote x setup repository push public project x update description clear source gitlab change group membership gitlab cookbook op gitlab net push update repository setting gitlab cookbook repository configure push op gitlab net user notify member update update description clear source cc
655,14483319,2.0,design document first iteration kubernete migration it bring handover careful opportunity cost spend time effort internal network harden relevant start migrate service kubernete personally i find cost difficult reason give roadmap plan iteration kubernete service migration this issue track design document accomplish iteration kubernete horizon complete work priority currently i believe likely candidate registry service give set standalone service the service sidekiq i propose document transition plan registry sidekiq service well fit i milestone consideration decide want start think cc
656,14479585,1.0,jose access gcp console access gcp console
657,14479549,2.0,restore pipeline disk snapshot break take snapshot work restore pipeline break x fix pipeline stage fail fail pipeline step x use deadman snitch pipeline well monitor x fix restore pipeline i think extremely urgent snapshot pipeline work restore test break i milestone capacity cc
658,14468768,3.0,investigate way tls endpoint tls deprecation we want come way customer tls endpoint test api call ac come proposal estimate share dave gerir jose schedule base work
659,14463636,2.0,new bastion host aws registry analysis spin can spin bastion host likely good aws gb ram read credential get list permission once i start manual scan i start write step need cc cc schedule
660,14430468,3.0,sidekiq restart chef deploy ref i try fix bug relate background job realize run sudo chef deploy worker old version app i require manual restart order fix work can fix cookbook
661,14424092,4.0,create internal api endpoint canary in order canary deployment use api internally need api internal load balancer canary api node create canary haproxy vm attach internal loadbalancer canary configure canary host use internal api endpoint
662,14424067,4.0,setup web api git canary receive small percentage web traffic this task canary project outline design doc x create infrastructure web api git x create utilitiesthat adjust weight server receive small portion gitlab traffic
663,14423994,2.0,configuration update introduce optional backend weight haproxy configuration describe design doc we want option backend introduce weight optionally include canary host backend weight zero
664,14423882,1.0,update elastic credential pipeline the credential date before lose log visibility need update credential pipeline
665,14385584,1.0,add redis alert secondary connect sync utilize metric alert secondary redi fail sync primary node x if possible create graph datum x ensure high priority alert pagerduty worthy
666,14385552,5.0,redis binary cache upgrade need restart run correct binary it discover sept redis application cache server run deleted binary this issue place restart redis gracefully cache server resolve concern
667,14373223,1.0,design document geo dr please fill idea follow design doc ac switch mr handbook google doc
668,14367226,2.0,add pagerduty webhook open gitlab incident issue pagerduty incident currently fairly clumsy process page open incident manually production tracker case gitlab use slack bot initiate i think need change i think automate way open issue manually i think good option probably use google cloud function webhook handle api request open issue incident what include way avoid spamme infrastructure tracker incident fire event for probably add api query issue open open new cc tasks update x understand information gitlab production issue contain x review recent pagerduty incident understand information capture note x understand condition gitlab production issue create requirement notes x figure gcp project use preference gitlab op note x figure team preferred faas provider gcp aws preference gcf note x understand alertmanager slack bridge x understand security pager not relevant issue note determine service account connect gitlab maybe generic service account x read gitlab rest api x create flow diagram proposal note x enable gcf api gitlab op google project write function code create pagerduty webhook connect deploy look zapier pagerduty gitlab integration seem like easy option come
669,14363523,3.0,introduce suspension change release important event during livestream decide suspend network configuration change i think worth discuss way formalize process what need accessible team company view query cicd stop automate change roll for production soc suspension change standard term we avoid popular term like blackout window blackday i google find standard love hear suggestion for i like signal boost put share meeting calendar bit create new calendar for google calendar work consider option internal company pipeline tool i feature force pipeline pause certain time interval like roadmap cicd name suggestion feel free vote add soc suspension change ghost change suspension office hour icl infrastructure change lock change freeze snowman cc
670,14349534,1.0,ci cd terraform require apply stage to close final implementation point successfully implement validate need update pipeline gitlab com gitlab com infrastructure apply stage setting false ensure block manual action require merge master
671,14349499,1.0,ci cd terraform tf apply per high level plan need update ci cd pipeline gitlab com gitlab com infrastructure manually run tf apply environment start gstg also section terraform documentation detail automate deployment multiple environment since currently terraform workspace mvc pass probably optional manual step production environment gstg op gprd build artifact eventually need consider organizing repository line comment
672,14349436,1.0,ci cd terraform tf plan per high level plan need update ci cd pipeline gitlab com gitlab com infrastructure run tf plan environment build stage build artifact plan file directory environment automate tf apply stage later see section terraform documentation detail
673,14333375,2.0,redirect page website please infra team set the rename blog category release release fyi future change check need redirect old category it cause backslash hn link page doc see reference
674,14309457,1.0,apply set change limit max push size merge thank see detail this production request important control improve stability see apply control what reasonable maximum size cc
675,14271485,2.0,chef fail prometheus alert managers chef fail run follow error ruby prometheus alertmanager action info processing prometheus alertmanager action create gitlab line error execute action create resource prometheus alertmanager undefined method nil nilclass resource declaration in chef cache cookbooks gitlab alertmanager recipe template source owner group mode notifie hup end compile resource declare chef cache cookbooks gitlab alertmanager recipe prometheus alertmanager action create retry default source variable template gitlab alertmanager default owner prometheus group prometheus mode path prometheus alertmanager verification end template context line url false platform linux info run queue delay notification raise exception run handler error run exception handler prometheushandler run handler complete error exception handler complete chef client fail resource update second fatal stacktrace dump chef cache chef fatal please provide content file file bug report error undefined method nil nilclass line url false
676,14268826,1.0,redirect old url new url
677,14254636,2.0,switch early adopters free october we extend early adopter program oct email impact user sept on october switch early adopter plan free plan
678,14242970,4.0,collect postgres log analysis db experiment these action issue as discuss i go collect sample minute query log wednesday sep start utc busy time utc the brief description action to i setup manual alter system set command master follow select the plan detail reach iteratively descend current value second step minute apart control io log size projected io impact the project io impact base view lack query parameter add small error forecast huge geo query master node base hour observation sql select record give estimate s write postgres log this base busy hour subject check tuesday mid day this estimate for high value include possible low positive excepted io low additional observation lot query if query time sql select order desc text query order desc limit the detailed plan action tuesday utc reset double check write io estimate in psql postgres master sql select select wednesday xx preparation set millisecond precision log timestamp use m instead t currently second level precision t alter system set m p select wednesday utc start descend in psql postgres master sql alter system set select observe current size log avg speed growth shell ls log gitlab postgresql current print avg bytes sec base current log file sudo head log gitlab postgresql current head se g xargs date s awk s print date awk wc log gitlab postgresql current awk print print byte and check io sudo iotop monitor wednesday utc continue descend set in psql postgres master sql alter system set select continue observe describe step wednesday utc continue descend set in psql postgres master sql alter system set select continue observe describe step check io sudo iotop monitor gcp graph wednesday utc continue descend set in psql postgres master sql alter system set select continue observe describe step check io sudo iotop monitor gcp graph decision to be make based on observed number if current io cause log s stop collect partial log threshold if continue step descend collect query log sql alter system set select collect log minimum minute max minute shell sudo cp log gitlab postgresql current return initial state psql master node sql alter system reset alter system reset select note possible risk no risk performance degradation expect in case unpredictable io step immediately apply it expect gb log generate collection phase project generation speed s equivalent day log current threshold this cause delay kibana processing todo work after complete todo separate issue process analyze log pgbadger poc usage log pgreplay experimental db nodes set permanently set well stop db role use separate one different app administration
679,14239602,3.0,design document terraform automation design doc mr
680,14239562,4.0,design document chef automation please fill idea follow design doc
681,14239539,5.0,design document monitoring review storages please fill idea follow design doc
682,14230666,1.0,chef run client an inadvertent action delete client default vault stop chef processing node azure
683,14219800,2.0,offline thread production
684,14219511,1.0,error deploy i get error try deploy customer app sudo chef client recipe compile error chef cache cookbooks gitlab server recipe encrypt public key contact administrator vault item encrypt cookbook trace chef cache cookbook library chef cache cookbook library chef cache cookbooks gitlab server recipe relevant file content chef cache cookbook library node be path key end def require chef vault vault try load databag end end end maybe related recent change vault
685,14213721,1.0,standup planning update proposing make slack channel standup planning record place team communication topic infra base hold make channel use sre lounge standup x todo setup bot remind cover what work what block opportunity pair thing i work hand milestone planning think good milestone issue level planning async the current thought use standup channel post request feedback issue need plan current milestone since async depend planning meeting instead work planning review issue design occur the manager team serve pos sure remind team get item plan early ideally plan review item milestone start item current milestone need review cc
686,14163089,1.0,remove dns record this elk cluster destroy dns record live
687,14162646,2.0,setup alert custom non custom ssl endpoint page this come page postmortem the end page ip release we critical pagerduty alert staging production notify immediately issue by time page production late need stage for alert i think good thing scrape ssl custom domain configure use a record
688,14155269,2.0,network peer monitoring subnet gprd gstg op for alertmanager cluster desire network peer monitoring subnet this cover prometheus alertmanager note internal dns resolution work project configure cross project connection need private ip address for context x peer gstg gprd op network x create firewall rule allow alertmanager communicate
689,14151997,3.0,enable object storage maven package in release maven package support suggest enable object storage support soon possible avoid migration future
690,14150172,1.0,whitelist metric public host the op prometheus instance need access metric port public host these host we tunnel traffic op zone use public ip
691,14149366,1.0,onboarde jfinotto ssh access environment hello username jfinotto ssh rsa ogmotbwlf jfinotto
692,14149086,2.0,check internal ip address update restriction list we recently incident blackbox ip address flag restricted list the follow repos would benefit pipeline check ensure ip internally reserve
693,14140069,1.0,empty home dashboard public metric server navigate time fresh browser show home dashboard it redirect gitlab triage
694,14133730,1.0,remove bootstrap false base role this necessary force reconfigure package install the right way deployment orchestrator takeoff this issue track revert takeoff change
695,14118575,5.0,backup restore hang fail on pipeline run trigger this spin new gce database instance pull late backup wal e backup fetch timeline sep instance boot sep wal e start backup fetch sep encryption error cause permission error opt gitlab postgresql data log harmless sep wal e download partition complete restore backup log message sep backup fetch process hang strace the backup fetch strace stall backup fetch process sl wal e bin wal e bin wal e backup fetch opt gitlab postgresql root strace strace process attach process detach time second usec call error syscall total in beginning see error harmless cause permission error certificate sep restore prd startup script info startup script info msg begin partition download sep restore prd startup script info startup script detail the partition download sep restore prd startup script info startup script hint the absolute key sep restore prd startup script info startup script structured sep restore prd info msg begin partition detail the partition download hint the absolute key structured sep restore prd startup script info startup script gpg read error sep restore prd startup script info startup script gpg read error sep restore prd startup script info startup script gpg warning encrypt message manipulate sep restore prd startup script info startup script gpg pende byte sep restore prd startup script info startup script gpg pende byte sep restore prd startup script info startup script lzop inappropriate ioctl device stdin sep restore prd warning msg retry encounter detail exception information dump traceback recent file wal e lib site package line return file wal e lib site package worker line file wal e lib site package line path file wal e lib line file wal e lib line file wal e lib line wb permissionerror errno permission deny opt gitlab postgresql data hint a well error message write handle exception please report output possible situation structured
696,14087853,3.0,design document canary deployment testing please fill idea follow design doc
699,14078014,3.0,hurricane florence readiness our region moncks corner sc hurricane path go near base nhc map this general issue group action ready disruption service region initial action look option alternate region backup git nfs datum x cloud storage multiregional x make sure important asset alternate region n big start work terraform build stack geo
700,16586793,3.0,service run patroni cluster need limit match omnibus gitlab omnibus run service bump limit we custom cookbook install pgbouncer postgresql patroni cluster need way apply limit service thing consideration postgresql manage systemd like spawn patroni need apply limit patroni service inherit postgresql bump limit likely need service restart cause brief disruption service careful planning need production roll action
701,16586751,1.0,pgbouncer mtail catch error after patroni migration start see error log error accept fail too open file it catch mtail regexp need correct action
702,16583833,6.0,setup ci cd terraform module with gcp module reside repository need include thing mvc x add changelog module repo x add readme module repo x add license module repo x add version file module repo configure ci x run tf lint x check terraform fmt x run terraform validate x ensure version get bump merge master x automatically tag repo merge master
703,16571102,1.0,add template patroni migration need use tmux screen deploy host add template patroni migration need use tmux screen deploy host
704,16563840,2.0,investigate pingdom check get delete this investigate pingdom check get delete
705,16561930,2.0,new patcher post slack when run new patcher ci cd process i find manually post message slack time stage complete the automation at slack message start patch staging finish patch staging finish patch canary prompt manual step patch production finish patch production
706,16561761,1.0,alert in staging staging send alert i silence fix it possible alert long necessary delete i information label alertname channel database environment gstg fqdn instance job postgre monitor gstg default pager pagerduty provider gcp region east replica severity critical stage main tier db type postgre annotation description this indicate run buggy query title postgres exporter show error hour source
707,16561374,1.0,disk full fail wal archive ensure backup work it look like need fix destination archive storage error msg storage prefix define hint either set prefix prefix prefix prefix option define environment variable structure gmt log archive command fail exit code gmt detail the fail archive command bin envdir wal env wal e bin wal e wal push gmt warning archiving transaction log file fail time try later detail
708,16559111,3.0,shared macos runners create minimal private cloud build macos shared runners working issue track macstadium setup iteration x sign macstadium account information x initiate minimal private cloud build vpn firewall mac pro tb storage vcenter vsphere vmware esx set esx clean base image mac os x mojave communicate ci cd team plan step setup share runner manager share runner label configure mac runner tag job look job mac tag this direction let api call platform vcenter manage creation destruction templating image
709,16553315,1.0,add feature toggle add feature toggle follow cc
710,16549271,1.0,use http datum source bootstrap teardown script access script relative path produce follow error plan error occur file open craig src gitlab gitlab com gitlab com infrastructure environment google teardown file directory google teardown this prevent split module standalone repository relative reference update use http data source
711,16541755,2.0,fail deploy summary deploy production fail need roll affect team attribution minute downtime degradation m incident doc work doc production issue impact metrics start following what impact incident service outage sub service brown exposure sensitive datum who impact incident external customer internal customer specific team how incident impact customer prevent x incorrect display y how attempt access impact service feature how customer affect how customer try access impact service feature include additional metric relevance provide relevant graph help understand impact incident dynamic detection response start following how incident detect pagerduty alert trigger pingdom prometheus do alarming work expect yes how long start incident detection incident detect immediately watch deployment how long detection remediation error start site report m downtime be issue response incident bastion host access service available relevant team memeber page able timeline deployment pingdom alert gitlab ce high web error rate jose tweet initiated rollback error jose tweet roll alerts resolve tweet everything normal root cause analysis the purpose document understand reason cause incident create mechanism prevent recur future a root cause person way writing refer system context specific actor follow s blameless manner core root cause analysis for necessary start incident question happen keep iterate ask time while hard rule time help question deep find actual root cause keep min come answer consider follow different branch example usage s the vehicle start problem why the battery dead why the alternator function why the alternator belt break why the alternator belt useful service life replace why the vehicle maintain accord recommend service schedule fifth root cause what go start follow identify thing work expect any additional out go particularly what improve start follow use root cause analysis explain improve prevent happen be improve detection time detection be improve response time response be exist issue prevent incident reduce impact do indication knowledge incident place corrective action list issue create corrective action incident for issue include follow bare issue link issue label action include estimate date completion corrective action incldue name individual own delivery corrective action guideline blameless rca guideline s
712,16524827,1.0,execute vacuum freezee table execute vacuum freezee table
713,16421696,1.0,fix chef find failover fix chef find failover
714,16413870,1.0,the mirror push gitlab com infrastructure work today discover repo sync op to fix immediate issue force push complete repo however failure come credential incorrect unamused i update vault dice halp
715,16394204,2.0,add arinsights spf dkim record update gitlab dmarc authentication policy summary gitlab subscriber arinsight software tool monitor analyst relation typically large tech vendor ms small one specifically i use send bimonthly newsletter industry analyst we use sw satisfy gdpr requirement give tool track open newletter link click this month newsletter bounce recipient block error all redmonk bounce forrester when i bring arinsight attention write following your email administrator need add spf dkim record update dmarc authentication policy this enable send email architect look like come directly domain trap spam filter i go dmarc record generate forward documentation as soon record generate i forward hesitate ask question process i send file i receive please let know i need
716,16381415,1.0,our host elastic search cluster start run low space we receive alert past week start run space cleanup script come clean index receive alert good that cleanup script report storage space complete cleanup process we scale number nodes log volume increase recently we issue pull start add item elastic search utilize issue bump storage available run space cluster currently node setup tb space tb tb
717,16359537,1.0,can redirect url early today come issue absolute link cause google bot index serve page search result the absolute link update eventually google index to resolve problem interim setup redirect thank
718,16326762,5.0,separate repository continuous delivery target environment create new repository environment currently new gitlab project lock old repository prevent merge request change organizing repository set push restriction gitlab project setting enable validate pipeline new repository remove environment directory old repository testing copy terraform file old repository x pipeline image terraform ci n validate tf init successfully configure remote state download provider module validate tf plan success output local repository add manual tf apply stage enable pipeline validate end end op change
719,16326720,2.0,share tooling docker build pipeline terraform ci create new terraform ci repository share tooling docker image environment pipeline this repository provide single place build manage container terraform ci cd pipeline require package utility include the exist terraform wrapper script gitlab com infrastructure repository need migrate
720,16326597,3.0,separate terraform module repository x create new terraform repository x update source line current reference new module repo path x update source line current repository reference new module repo path test from gitlab com infrastructure x rename module directory repository ensure relative path reference remain x remove directory x execute tf init environment x execute tf plan environment x remove rename module directory
721,16325790,3.0,service discovery database load balancing we support service discovery database load balancing apparently i look production configure service discovery implement i sure issue go i inclined think need service discovery support flawless failover
722,16316122,1.0,analyze database failover add step analyze database promote patroni
723,16312576,5.0,design doc gcp maintenance automation we need upgrade kernel version gce vms while process pretty straight forward complete task staging fleet api change request successfully area leverage automation look nearly vms staging environment production in addition type activity near future here design doc automation effort include mr propose project specific ask team possible please review design doc please review mr list design doc provide feedback relate issue
724,16302679,1.0,transfer gitlab ownership acquire domain transfer domain management
725,16299071,1.0,add gitter ssh sre offboarde add gitter ssh sre site reliability engineer offboarding troupe aws account address
726,16298813,2.0,network security diagram security customer consumption on infra production architecture page current architecture diagram to help security questionnaire network security diagram good produce this vpc help answer separate production traffic staging etc network separation place cc
727,16297405,1.0,redirect page build need redirect place press go week
728,16296712,1.0,add troupe gitter aw offboarde we need sure remove user troupe gitter aws space offboarde we remove ssh key i access project exist anymore for developers access infrastructure production gitlabber copy offboarding process infrastructure offboarde action spawn find old user list
729,16291768,1.0,various admin jim thavisouk help access request process please describe problem oncall engineer pick see
730,16289344,1.0,rename grafana folder postgres the grafana folder replace old postgresql folder objection
731,16281372,8.0,database reviews x recursive cte eye check cte pass x x x douwe x eye tentatively approve comment x eye review merge double check look ok eye comment wait feedback x x x x x urgent miss deliverable x x x ready approve minor comment leave x urgent blocker x quick x x x eye check pass x x x x x x x x
732,16250247,3.0,add grafana dashboard docker registry dedicated grafana dashboard observe operational status relate
733,16250107,3.0,structure log docker registry logs log gitlab send structured log make hard find parse adjust parse field correctly make sure multiline panic stack trace forward example log
734,16246365,1.0,crawl google i notice search summit i imagine want right
735,16245371,2.0,new change migration script add query select state idle remove step mandatory documentation troubleshoot automate test ssh block manual verification easy automate
736,16243688,1.0,alert investigation utc high cpu during investigation sshd git process tie cpu i perform strace see similar output point direction i continue look null leave chld null null leave chld null null leave chld null the host able process request fine it remove load balancer time i proceed force git command server despite tad slow git server request work fine there known stuck git process base command provide runbook at utc i server state drain load balancer the load immediately start fall remain git process apparently long run leave cpu pegged state at utc i decide pull plug process kill the cpu usage immediately drop i wait metric load alert clear proceed server rotation at utc i kill remain git ssh process there roughly old november at utc server rotation due lack knowledge visibility process i sure look type situation if long run git command nice i tie ssh session process if well idea i approach situation let know i mark incident have information prove abuse lifetime situation occur server able serve request successfully
737,16241705,1.0,public dashboard load graph datum the gitlab triage dashboard load graph datum point this confirm multiple people work
738,16239323,2.0,deprecate tls tls staging like set canary think ready apply tls update staging reference
739,16236688,1.0,upgrade the forum need upgrade info cc
740,16227960,2.0,rca api fleet unavailable utc summary all api server unavailable walk public project affect api team attribution sae minutes downtime degradation minute impact metrics start following the api completely inaccessible all user try access api affect the incident prevent api action include push ci runner fetch job graph log kibana project list request dashboard db host load lock dashboard workhorse availability dashboard haproxy dashboard incident detection response the incident detect pagerduty alert the alert resolve shortly fire the second downtime remediation happen timeline first alert go increasedbackendconnectionerrors first alert clear second alert fire increasedbackendconnectionerrors user begin report issue the api server recover service restore second alert clear root cause analysis why api server unavailable public project iterate api api pagination issue cause timeout overwhelm server what go we discover problem quickly quick response what improve this happen multiple time the runbook haproxy problem use work we limit request endpoint we improve pagination application corrective action dedicated fleet internal api well handling pagination api request guideline blameless rca guideline s
741,16224277,1.0,artifact bucket gcs we sync set sync artifact gcs nightly hope migrate gcs quickly time in encounter problem migration place backlog info we need artifact bucket gcs gitlab gprd artifact disable nightly sync close actually able execute it simply massive waste money store datum twice bandwidth transfer cost migration time sight edit as object storage dashboard store tb gcs artifact this amount gb month month kill on second thought mo lot additional engineer
742,16223440,2.0,readiness review new post deployment patcher this issue sre review post deployment patcher x sre sign new patcher tool x review new documentation x deprecate old patcher howto x deprecation notice gitlab patcher x deprecation notice post deployment patch x turn issue mr gitlab patcher post deployment patch repos x apply readme update point official doc x deploy noop patch add comment source file pipeline x announce backend previous developer issue patch process change
743,16222047,1.0,promote cluster patroni production ongre test promote cluster patroni prod environment test just remember cluster traffic let know cluster ready the test multiple sequence failover
744,16212197,1.0,user unable create project user face issue try create new project web ui they get error cancel statement statement timeout context rechecke update tuple relation update set zd
745,16201607,1.0,create gcp project disaster recovery create gcp project infrastructure environments call gitlab dr use set geo replication base disaster recovery site this project need quota expand region match production
746,16196302,1.0,the follow server add blackbox monitoring ssl certificate expiration reference
747,16188663,8.0,update monitoring script change query check database activity select count state idle select state count group state create pre flight check check health status new cluster auto check result instead human try reduce tmux topic remove add log solution
748,16188312,2.0,problem application lb problem test application lb nomethoderror undefined method nil nilclass ee lib gitlab database ee lib gitlab database ee lib gitlab database ee lib gitlab database ee lib ee api block lib gitlab util ee lib ee api app helper lib api lib api block class api ee lib strategy lib gitlab middleware lib gitlab ee lib gitlab jira lib gitlab middleware lib gitlab lib gitlab middleware lib gitlab middleware lib gitlab middleware lib gitlab lib gitlab metric lib gitlab middleware
749,16188286,1.0,restart cluster execute failover restart cluster create failover
750,16188272,3.0,pende restart patroni cluster pende restart patroni cluster
751,22677979,1.0,add statusio chatop doc general incident documentation relevant link
752,16145353,2.0,review runbook patroni we like cover follow example start stop instance patroni failover add node remove node start new cluster
753,16145313,8.0,change staging script migration stop postgresql cluster change step stop repmgr check replication lag parameter equal
755,16135243,2.0,disable geo staging i notice geo enable staging ruby geonode d primary false enable true url this leftover gcp migration rehearsal i reason disable geo completely staging backstory during rehearsal disable geo add dummy node admin dashboard this maintain geo event log come useful want fallback
756,16112133,1.0,permission request role kubernetes engine admin gitlab internal project hi i try create kubernete cluster gitlab internal project test i follow error forbid user rpereira create cluster scope require permission can i give require permission role kubernetes engine admin possibly cc
757,16109352,1.0,renew certificate ci prometheus server fleet certificate ci prometheus server expire november please renew manage sslmate update proper vault x vault gitlab runner prometheus ci prd x vault gitlab runner prometheus gce c ci prd x vault gitlab runner prometheus gce d ci prd
758,16102989,1.0,runbook alert improvement as need corrective action item x improve runbook troubleshooting tip x create alert
759,16071259,1.0,cleanup unify postgre dashboard the grafana dashboard postgre inconsistent use variable hardcode value environment type prometheus instance place make hard adapt new cluster envs like patroni we update dashboard consistently use selectable variable environment add new variable type switch patroni postgre possibly db host type future that dashboard usable add patroni production
760,16064210,1.0,redirect ux research panel please redirect please note redirect place this replace cc
761,16062249,1.0,use elasticsearch curator recur elasticsearch maintenance task the curator tool complete regular maintenance task trim index purge datum etc easy configure yaml definition file although activity currently simply complete bash script activity curator the curator offer additional functionality course once upgrade es current bash maintenance script wo necessary afaik functionality include es however curator offer functionality relevant maintenance es
762,16061970,3.0,upgrade host elasticsearch elasticsearch version available support gitlab upgrade advantage late feature speed improvement it highly recommend technical contact upgrade soon possible the upgrade automatically handle frictionless however need aware potential change underlying attribute field break change describe the upgrade rolling upgrade describe the upgrade require cluster restart cause cluster unavailable short period time to complete task i believe need determine good approach test upgrade version create new cluster base late version es re point datum ingestion point potentially staging new cluster ensure error datum available remove point upgrade production cluster hope good the disadvantage approach cluster restart lose log short period time acceptable i know configuration environment determine step sufficient doable
763,16057209,1.0,marvin return bundler error summary the marvin bot slack long respond command show error lib ruby gem lib uninitialized constant class nameerror lib ruby gem lib lib ruby gem lib bundler lib ruby gem lib bundler lib ruby gem lib new lib ruby gem lib lib ruby gem lib lib ruby gem lib lib ruby gem lib lib ruby gem lib lib ruby gem lib configure lib ruby gem lib lib ruby gem lib setup lib ruby gem lib bundler require lib rubygem require lib rubygem require lib rubygem require bundle cog exit status
764,16038360,1.0,recipe compile error find recipe server cookbook postgresql cookbook trace chef cache cookbooks gitlab sentry recipe relevant file content chef cache cookbooks gitlab sentry recipe cookbook name gitlab sentry recipe default copyright gitlab all right reserve do not redistribute fetch secret sentry nginx
765,16034098,2.0,setup cloudwatch exporter cloudwatch metric prometheus aws as long critical infrastructure aws monitor instance nice storage metric bucket migrate
766,16021789,1.0,load balancer redundancy threshold high we feloadbalancerlossofredundancy alert pager time any loss redundancy load balancer while want know happen informational thing redundancy what load balancer automate manage redundancy we able lose redundancy load balancer set pager to implement i change threshold please discuss appropriate threshold well
767,16017987,2.0,create alert miss haproxy log it happen time our log disappear stackdriver whether incorrectly configure exclusion rule misconfigured td agent both negatively impact visibility environment let prevent happen alert log available acceptance criterion determine method alert determine appropriate threshold necessary
768,16017584,1.0,adapt postgre dashboard patroni we need adapt postgre dashboard patroni cluster duplicate exist dashboard change point patroni tag instance necessary cc
770,16016939,1.0,handbook update include service list owner responsible as onboarding identify potential improvement list service currently know potentially manage gl infra this update handbook some information capture google doc once complete essential information extract add handbook potentially table following service url description owner responsible team any potential documentation
772,16009745,1.0,streamline service access request gl security access requests streamline access request provision gl infra gl security access request model ensure compliance simplify process flow the idea ensure access request manage request require gl infra provision pass gl infra suitable timely manner the issue create proper process technical flow ensure achieve we like avoid access request create directly gl infra issue tracker avoid tasks agree appropriate model gl security update access request template update security own application list agreement update handbook appropriate link
773,16009513,2.0,refactor common config fluentd template there lot copy paste fluentd template refactore include it notice prometheus mixin miss redis config this issue pull common config template bit manageable
774,15991313,2.0,adjust disk capacity postgre currently provision tb ssd disk space database instance gstg gprd actual current usage gstg tb gprd tb leave room wal maxe io performance proposal reduce disk capacity gstg tb utilization approx gprd tb utilization approx we instance environment the saving gstg tb gprd tb this translate saving roughly usd month performance wise impact gprd for gstg io performance change reduce sustained read performance roughly half currently however small instance type gstg anyways worth note increase disk easy require reboot
775,15979239,2.0,logging ip client address nginx elasticsearch for troubleshooting purpose useful client ip address available elasticsearch kibana troubleshooting correlation activity at moment ip address proxy this information likely available nginx proxy forward elasticsearch far i understand
776,15977306,3.0,configure wal e archive backup patroni cluster after staging fail patroni cluster configure wal e sure backup work patroni x setup wal e patroni instance x review archive relate postgre config
777,15977260,3.0,review stage env cookbook patroni please consider review maintenance plan note disk size tb probably tb gprd tb gstg save check performance implication x compile postgre config diff gprd gprd patroni go live sure aware change do need manage shmall shmmax relict afaik necessary old postgre version x increase disk size root volume gb
778,15958497,3.0,clean old registry bucket with successful migration registry gcs need look clean remove old bucket the size registry bucket selection object folders total size tb total object
779,15940463,8.0,database reviews x sean x x x x x x x x x x x andreas x fri x fri
781,15913358,1.0,rca incident merge request queue processing sidekiq please note incident relate sensitive datum security related consider label issue mark confidential copy information summary working note event start issue slack thread thread thread thread root cause thread contributor stan valery northrup mike kozono dave jose affect team attribution minute downtime degradation impact metrics start following what impact incident service outage sub service brown exposure sensitive datum action normally finish second delay minute appear break finish some action run multiple time who impact incident external customer internal customer specific team all user how incident impact customer prevent x incorrect display y action normally finish second delay minute idk peak delay appear break finish merge request diff update merge request widget update repo mirror work some action run multiple time many system note duplicate how attempt access impact service feature how customer affect how customer try access impact service feature include additional metric relevance provide relevant graph help understand impact incident dynamic detection response start following how incident detect do alarming work expect how long start incident detection how long detection remediation be issue response incident bastion host access service available relevant team memeber page able the alert mirror update overdue sidekiq availability service operation rate updatemergerequestsworker running job updatemergerequestsworker error postgres lock sidekiq queue detection response how incident detect pullmirrorsoverduequeuetoolarge alert report production issue create do alarming work expect yes yes pullmirrorsoverduequeuetoolarge work no far alarming system operate outside normal pressure redis area go alert general sre think ready push alert pagerduty how long start incident detection the alarm warning how long detection remediation about minute shut feature recycle worker queue take hour drain duplicate job lock row kill retry be issue response incident no take time diagnose mitigate moment hrs detection root cause timeline utc feature flag enable production staging utc john northrup notice queue grow utc we get alert pullmirrorsoverduequeuetoolarge utc feature flag disable production staging utc alert manager report pullmirrorsoverduequeuetoolarge resolve utc we get report gitlab team member take time mr update utc it take minute mr update utc we lot updatemergerequestsworker job fail error cancel statement statement timeout utc we tweet message status utc we tweet normal utc we notice duplicate job multiple sidekiq worker process job jid time root cause analysis current theory a bug reliable fetch gem cause job duplicate time sidekiq worker start certain kind duplicate job updatemergerequestsworker particular cause sql update query target record it cause lot lock database response cause number sidekiq job grow rapidly what go we alarmed problem immediately start fix we alarm get user report the situation stabilize pretty quickly what go wrong we know root problem probably sufficient monitoring log regard new queue miss communication sre engineering we declare incident early cmoc tweet minute frequent update customer we rollback option notice issue this data migration compatible previous state the sidekiq reliable fetch gem fork inactive project run environment sidekiq worker we proof prove production environment treat caution more testing comprehensive staging test catch bug increase chance where lucky we get alarm notification expect place open question we identify likely cause duplicate job confirm sure corrective action we need find mr diff we find duplicate we need reproduce work multiple sidekiq process issue confirm root cause then fix guideline blameless rca guideline s
786,15904200,1.0,evaluate pingdom request timeout to reduce page volume increase pingdom timeout this high time evaluate long request time isolate cause need high
787,15897517,2.0,incident slack tooling update we create epic track overall incident management automation effort originate issue keep track effort epic sense subtask enumerate pretty quickly therefore close issue br in order streamline incident handling reduce cognitive load incident team one way ensure procedure simple possible incident team particular eoc imoc focus technical resolution incident instead necessary procedural call intend provide visibility awareness incident another way improve automation support say procedure in particular incident declaration open production issue create tracking document create infrastructure root cause analysis issue optionally severity assignment possibly imoc escalation security escalation case abuse notify security team incident management edit incident severity possibly imoc escalation security escalation incident datum management ensure incident issue severity label ensure incident issue service label ensure incident issue attribution label ensure incident issue timely rca sample as i entirely familiar slack custom command capability follow example provide cli implementation incident declaration resolution incident declare severity incident description create tracking document google doc template ideally fill datum create incident issue production queue assign user specify assume eoc create corresponding issue infrastructure queue link incident issue tracking document assign eoc when flag security page pagerduty this command return way d refer incident slack in command normally use d omit assume currently open incident incident resolve d resolve incident close production incident issue incident management incident list list open go incident incident status provide status current incident open closed incident d severity severity change incident severity severity severity escalate imoc incident data management a bot ensure incident issue proper label
788,15892832,2.0,move pingdom check runbook configuration currently subset pingdom check manage yaml see now confident approach bring manage yaml configuration relate slack thread cc
789,15887120,1.0,rca pingdom check failure please note incident relate sensitive datum security related consider label issue mark confidential summary follow gitlab com gl infra pingdom report fail check access impact metrics start following what impact incident service outage sub service brown exposure sensitive datum who impact incident external customer internal customer specific team how incident impact customer prevent x incorrect display y how attempt access impact service feature how customer affect how customer try access impact service feature include additional metric relevance provide relevant graph help understand impact incident dynamic detection response start following how incident detect do alarming work expect how long start incident detection how long detection remediation be issue response incident bastion host access service available relevant team memeber page able timeline yyyy mm dd utc happen utc happen yyyy mm utc happen utc happen root cause analysis the purpose document understand reason cause incident create mechanism prevent recur future a root cause person way writing refer system context specific actor follow s blameless manner core post mortem for necessary start incident question happen keep iterate ask time while hard rule time help question deep find actual root cause keep min come answer consider follow different branch example usage s the vehicle start problem why the battery dead why the alternator function why the alternator belt break why the alternator belt useful service life replace why the vehicle maintain accord recommend service schedule fifth root cause what go start follow identify thing work expect any additional out go particularly what improve start follow use root cause analysis explain improve prevent happen be improve detection time detection be improve response time response be exist issue prevent incident reduce impact do indication knowledge incident place corrective action list issue create corrective action incident for issue include follow bare issue link issue label action include estimate date completion corrective action incldue name individual own delivery corrective action guideline blameless postmortems guideline s
790,15887023,2.0,rca certificate expire please note incident relate sensitive datum security related consider label issue mark confidential summary for approximately minute unauthenticated request request return error expired certificate impact metric the follow service impact unauthenticated request redirect the follow service remain unaffected outage authenticated login page registry api service detection response start following how incident detect do alarming work expect how long start incident detection how long detection remediation be issue response incident bastion host access service available relevant team memeber page able timeline utc start see expire certificate error utc curl cert reveal good utc curl cert reveal expire utc start purchase process cert renewal utc update chef vault gitlab com default new cert utc manually run chef client roll new certificate utc serve new cert utc modify fastly configuration origin validation accept tls utc roll fastly configuration origin ssl tls change utc operate normally root cause analysis the certificate origin server expire cause cdn provider throw error we realize cert expire expire move production site fully cdn resolve brand origin server stead leave old certificate server cdn provider generate new this compound fact certificate monitoring prometheus advanced alerting reconfigure look origin server cert expiration alert what go alerting trigger immediately slack ad pagerduty issue what improve we need automate mean iterate domain cert monitor go expire corrective action guideline blameless postmortems guideline s
791,15882989,8.0,template patroni failover postgresql step please review mr the repo git create step maintenance patroni failover postgresql repmgr patroni step setupthe new cluster production please automate wo allow direct interaction host all action bastion consider update clean monitoring consider repmgr master cluster ongres work postgresql fyi
792,15882948,2.0,template failover patroni please review mr the repo git create step maintenance patroni failover close open traffic redirect application correct load database please automate wo allow direct interaction host all action bastion ahmad ongres work postgresql fyi
793,15870615,1.0,create pagerduty slack command the idea oncall engineer type page pagerduty engineer cmoc type page pagerduty cmoc slack current command reference
794,15870563,5.0,review clean pagerduty alerts check review source event generate alert add report design doc number incident
795,15867797,2.0,stackdriver log miss haproxy log currently stackdriver appear miss large volume haproxy log relate slack thread this relate week haproxy slowdown example
796,15859519,1.0,block search crawler related domain discover url index google original message he point result google result result what good option manage crawl status domain subdomain i hope simple update fix do documentation domain use gitlab i love list find outlying index issue cc
797,15850458,5.0,add nginx config test restart nginx in go bad redirect this immediately remedie force config test before restart nginx currently cookbook restart nginx config change cookbook gitlab com relevant recipe
798,15849777,1.0,admin access tony carella grant admin access tony carella acarella support security team take responsibility it operations time no security team admin access staging time this help cater access request come staging access request
799,15836792,1.0,evaluate ubuntu advantage livepatche support ubuntu offer ubuntu advantage programme cost offer benefit particular benefit livepatche with large fleet instance major concern reboot reboot order live patching kernel relevant additionally help mitigate attack spectre meltdown require urgent attention particularly public cloud host instance the late example speculative execution attack enforce need focus feature the cost ubuntu advantage high instance year i believe benefit outweigh cost list benefit i think official ubuntu benefit landscape ubuntu server desktop management tool official ubuntu benefit telephone online support portal official ubuntu benefit canonical livepatch service official ubuntu benefit ubuntu extended security maintenance personal opinion reduce cost have task prepare execute reboot personal opinion reduce potential financial reputational cost fail reboot fail reboot chain personal opinion reduce security risk improve security posture mitigate day similar attack personal opinion provide overview fleet state operating system fleet disadvantages introduce tool make automate change environment impact service stability cost probably i think right i currently access datum number instance reduce cost consider use ubuntu advantage select instance troublesome reboot instance public facing my preference cover entire fleet financially unvailable however impression gitlab infrastructure number instance shutter reduce overall cost infrastructure maybe offset ubuntu advantage cost any thought welcome thank peter
800,21464080,1.0,mailroom lock arbitration problem just raise note local discussion like notice currently get message state age month i wonder way forward i competent redis client use comment interested author original patch
801,21458137,1.0,credential new snowplow bucket data team as datum team need read credential able pull event data warehouse all need bucket access key d secret access key
802,21457788,1.0,window machine gitlab runner ci test for gitlab runner ci need windows machine windows windows on exist one execute basic test build image windows docker containers near future start docker window run test the new test support docker windows linux container for need ensure hyperv instal machine it machine proper windows instal hyperv docker guidance find handle installation registration runner installation docker couple
803,21457021,5.0,rca page service interruption summary at approximately may may multiple alert receive backend connection error relate pages service affect page team attribution minute downtime degradation tbd impact metrics start following what impact incident page service who impact incident external customer internal customer specific team how incident impact customer prevent x incorrect display y how attempt access impact service feature how customer affect how customer try access impact service feature include additional metric relevance provide relevant graph help understand impact incident dynamic detection response start following how incident detect do alarming work expect how long start incident detection how long detection remediation be issue response incident bastion host access service available relevant team memeber page able timeline yyyy mm dd utc happen utc happen yyyy mm utc happen utc happen root cause analysis the purpose document understand reason cause incident create mechanism prevent recur future a root cause person way writing refer system context specific actor follow s blameless manner core root cause analysis for necessary start incident question happen keep iterate ask time while hard rule time help question deep find actual root cause keep min come answer consider follow different branch example usage s the vehicle start problem why the battery dead why the alternator function why the alternator belt break why the alternator belt useful service life replace why the vehicle maintain accord recommend service schedule fifth root cause what go start follow identify thing work expect any additional out go particularly what improve start follow use root cause analysis explain improve prevent happen be improve detection time detection be improve response time response be exist issue prevent incident reduce impact do indication knowledge incident place corrective action list issue create corrective action incident for issue include follow bare issue link issue label action include estimate date completion corrective action incldue name individual own delivery corrective action guideline blameless rca guideline s
804,21455616,4.0,git storage node repository metric i interested know following git storage nod number repository node absolute value number repository user min max average distribution node fleet size repository min max average distribution node fleet number repository base activity distribution hour hour hour hour day day week month month month month month we track metric ongoing basis this request intend able help guide decision relevant storage architecture
805,21454594,2.0,add pathfactory cname branded url we mktg bring new tool handle content distribution syndication can brand url set tool pathfactory instructions desire cname mktgops main point contact please let know question
806,21422105,3.0,bump bootstrap script version the late version bootstrap script include check ensure request kernel version available remove current running kernel attempt upgrade naturally fail however bootstrap script default this issue track effort update version reference module source line ensure version module load environment reference default version script in order prevent terraform unnecessarily reboot running instance use gcloud cli update metadata key running instance
807,21366725,1.0,personal staging credential go access request my login long present db access staging anymore this issue track request access not sure account get wipe db
808,21357863,2.0,send slack notification production channel infra lounge oncall report generate this nice addition oncall report
809,21350342,1.0,packagecloud backup fail accord dms work may complete exit code the error log error upload opt packagecloud backups packagecloud stream database fail abort multipart upload bucket look like correct list file byte size something weird go
810,21335800,1.0,break postgre upgrade okr we okr upgrade postgre version log centralization reporting capture milestone epic i propose split apart multiple epic postgre major upgrade log centralization report those largely independent chunk work parallelize address independently team cc
811,21331275,4.0,evaluate usage recent zol version kernel zol release ubuntu bionic ship zol lot improvement want use possible we compile package this give ability apply patch need include patch violate gpl long redistribute binary i develop prototype ci pipeline compile kernel module comment it work bionic xenial but run prod
812,21330923,1.0,increase thanos compact disk size the thano compact component need bit disk space perform compaction currently gb increase gb
814,21278361,2.0,codify update rule gitlab status page our determine issue experience know support reliability team the status page primary mean broadcast public information incident to ensure case produce set rule checklist we update tool communicate status update ensure status page secondary medium communication
815,21276815,1.0,sres zendesk light agent access as outline sre onboarde setup zendesk light agent account access because addition list add start date follow instruction provide internal support setup access please confirm x x x x x x x x x x x x x x x x x optional x
817,21257530,5.0,consolidate incident management documentation we issue incident management documentation documentation place different outdated content the handbook page long read fine learn process time work incident need figure asap we documentation find eoc imoc cmoc page eoc imoc cmoc chatbot usage we need consolidate documentation easy find useful everybody incident
818,21239178,2.0,write runbook codesandbox bucket in set custom bucket host codesandbox we write runbook set deploy
820,21226494,5.0,complete container registry deployment kubernete cluster pre we build poc past let poc real environment this realization decision issue poc real environment
821,21193998,1.0,rca sso enforcement feature break pipeline summary enable feature make group inaccessible member ci pipeline fail customer saml affect ci runners team attribution backend manage minutes downtime degradation impact metric what impact incident customer saml sso authentication access group proper permission both user attempt authorize view ui ci runner own encounter failure who impact incident any customer saml sso authentication how incident impact customer customer browse ui receive obfuscate existence group path and customer ci runner fail error while feature flag set customer authenticate sso provider project visible how attempt access impact service feature there moderate raise error groupscontroller compare total indicate user affect how customer affect customer report user affect at peak issue encounter nearly error minute period give natural error rate estimate legitimate error detection response how incident detect customer begin report issue support zendesk do alarming work expect we receive alert issue error rate low visible dashboard sentry how long start incident detection after m customer report zendesk how long detection remediation minute be issue response incident yes it take m customer report response result gitlab org gitlab ee dialogue issue include feedback customer initial conversation take place zendesk additionally prove difficult open incident infrastructure department production tracker which unnecessarily delay page reliability engineer mark cite confusion handbook language incident management source confusion finally find instruction use incident command incident management slack channel imoc bot receive error fail create incident issue production tracker sre timeline see incident ticket root cause analysis chatbot permit feature flag flip ongoing production deployment the line communication escalation support include reliability engineering documentation engage reliability engineering difficult interpret follow metric monitoring detect incredibly low number relative anomalous what go support great job create gitlab ee issue point reporting customer information centralize after escalate issue incident cause find minute mitigate minute what improve we improve feature flag change process well observability awareness we responsive gitlab ee issue customer know work incident issue create creating escalate incident easy possible everybody document we broaden incident management training outside reliability engineering exposure organization gitlab corrective action guideline blameless rca guideline s
822,21192445,1.0,broken mirror terraform module ops it appear credential change validate relocate terraform module op mirroring currently break module repository we need update mirroring configuration use update correct credential op gitlab net
823,21165208,3.0,update terraform gke module provide ability separate node pool create our current gke module rely use default node pool this great default limit ability customize cluster future case need beefy node x reason need enable disable preemptible instance reason disable default node pool provide ability configure node pool
824,21165169,3.0,update terraform gke module provide ability create regional cluster currently gke cluster default create single master node default region choice what region go x reason let enhance gke module include ability enable regional cluster optionally this provide redundant kube api server
825,21164859,1.0,update terraform gke module ability lock network access kube api server by default gke cluster open access internet access api server we limit least provide mechanism update list future
826,21164819,1.0,update terraform gke module provide option enable network policy feature gke gke network policy provide method lock traffic inside kubernetes cluster update module provide method enable optionally form configuration
827,21164731,3.0,update terraform gke module ability create private cluster in general reason cluster public these node public ip address egress traffic come nodes ip address let fix this improve security stature limit ip address expose outside world easy community set ip address traffic come feel like cleaner sane configuration this require use nat sort egress traffic wo know reach internet
828,21164677,1.0,update terraform gke module allow oauth scope configurable right gke module allow customize this important choose utilize specific google service we limit currently deploy cluster generous
829,21164661,1.0,update gke terraform module configuration disable legacy endpoint the terraform module currently allow configure this bad practice regardless future cluster forcibly disable option let proceed remove set default ensure disable default cluster create
830,21163552,2.0,create consul server fleet pre for purpose implement consul server cluster probably start pre environment useful low impact place test necessary change
831,21162278,5.0,register service necessary consul host inventory as base note node register service consul purpose node inventory chef repo mr
832,21152880,1.0,help debug sso enforcement run line rail console staging why i test sso enforcement staging work group particular project what on staging run following rail console ruby false policy jamedjo group saml test test subproject enforce ssso
834,21103659,4.0,can terraform git storage node use zfs after complete implement terraform code allow provision new file server decide hardware configuration these new file server separate tf resource declaration count the mr actually cause new node provision set count new resource declaration old storage node delete
835,21103616,1.0,decide disk layout storage node follow result decide storage layout vdevs the main contender moment disk usable parity single disk single disk call bad practice zfs gcp pds advertise have high availability data redundancy error correction we need evaluate weigh worth add redundancy zfs level the throughput configuration factor we want reduced throughput compare today the total usable filesystem space node tb overlaps discussion quota today note zpool space reservation filesystem zpool space total pd disk space provision raidz redundancy choose raidz we intend shard size today at time writing x tb prod change involve change thing another thing consideration zpool space need substantially large expect snapshot bloat git repack operation a zfs snapshot take repack reference disjoint block take therefore zpool disk usage repo double old snapshot pass retention window if repacke occur simultaneously repo gitlab installation unlikely filesystem usage spike double therefore bad case need twice usable filesystem space datum intend store node however repacke different repos spread time realistic scenario use low multiple also decide storage layout there debate time writing the initially propose configuration x gb ephemeral local ssd fix size connect nvme we decide vms storage node memory cpu change the deliverable issue necessarily code need tackle early project comment use reference come write terraform performance characteristic candidate configuration finally satisfy life hard come restore old backup gce snapshot choose raidz
836,21079679,3.0,grant db geo access toon douglas ash my apology accord process request thank grant access but additionally i like request access geo tracking database dr i try connect fail ssh db geo db geo permission deny publickey unix username dbalexandre
837,21070907,1.0,extend source port range network acl fix what go change port range inbound acl rule production external network acl network acl why right roughly outbound tcp connection fail randomly choose source port current low limit port range reply syn ack packet drop network acl want change see report diagnosis issue when go start time utc duration minute estimate end time utc how go manual edit acl rule aws web console how prepare co ordinating gitter engineer gitlab sre what check start nothing what check ensure work use debug liner seq curl we expect call succeed quickly any failure indicate fix desire effect failure indicate inexplicably situation bad impact type impact expect what happen nothing negative do expect downtime set override pagerduty none how communicate customer nothing require what rollback plan change lower bind source port
838,20971792,1.0,update certificate we see odd issue internal load balancer notice failing certificate we update pre condition execution step the newly renew certificate download sslmate sslmate download the proper field update gkms vault identify old cert verify match new in chef repo run command gkms vault frontend loadbalancer gstg gkms vault gitlab omnibus secret gstg a backup copy old certificate field store locally a properly format version new cert format json awk nf printf execution command step stop chef service fleet serve cert question knife ssh role gstg base lb fe sudo service chef client stop edit vault contain cert command like gkms vault edit frontend loadbalancer gstg find replace cert field i identify early json save change the follow field update gitlab haprox ssl there load balancer certificate gcp need update this certificate delete replace new certificate force chef run server like verify new certificate cert verification echo openssl null openssl restart chef node step knife ssh role gstg base lb fe sudo service chef client start rollback edit altered vault gkms vault edit command replace change cert old copy file delete replace gcp certificate backup old certificate record pre step save change force chef run test system verify fix normal
839,20964347,3.0,gitlab hosted version codesandbox sandpack overview as gitlab org gitlab need setup new bucket serve static sandpack script require gitlab host codesandbox proposal serve javascript bucket custom domain ssl enable requirement domain need purchase bucket configured ssl enable the entirety link feature target release dependency links references
840,20943994,2.0,setup salesforce omniauth with ask setup omniauth salesforce there promotion look live may clear understanding expect date for infra look initial setup run week week may documentation todo disable salesforce oauth sign source staging consistency provider
841,20786207,2.0,clean terraform run gstg gprd right tf plan run master return change apply this cause issue recently force target run it troublesome move forward plan effort follow current change output tf plan need address x update metadata this cause work update bionic it harmless apply change place metadata value bootstrap script only new instance care value exist one attempt update kernel version x non change like project ssh key true true fine apply shrink prometheus disk force new resource new resource require d prometheus inf gprd data compute force new resource compute compute compute true true gprd gprd prometheus app prometheus app compute compute prometheus inf gprd data prometheus inf gprd datum project gitlab production gitlab production compute size force new resource compute compute type pd standard pd ssd force new resource user compute zone c c x there lot shuffle port prometheus instance reason prometheus app prometheus app these fine behavior another set apparent non change attached disk prometheus instance x pubsubbeat instance topic recreate new resource require d project gitlab production topic pubsub geo inf gprd compute force new resource pubsub geo inf gprd pubsub rspec inf gprd force new resource project gitlab production compute new resource require d pubsub geo inf gprd compute force new resource x metadata pubsubbeat instance x false true change x delete specific info change
842,20761748,1.0,snowplow logging log shipping without specific need local logging shipping local bucket probably simple way log system aws be requirement log integrate log
843,20709802,1.0,update salesforce sandbox credential we need update sandbox credential salesforce new credential ara available subscription portal share vault
844,20703353,1.0,snowplow docker vs vm in order quickly deploy snowplow collector aws outstanding question should use ecs aw host component snowplow pipeline
845,20648598,2.0,miss haproxy log upgrade we currently receive log haproxy log log message i may this roughly correspond haproxy upgrade commandline apt install request by alejandro install haproxy end date cc
846,20615587,1.0,chatops match statement timeout production setting chatops currently plan query return that low set default production setting this mean use chatops expensive query oftentimes query actually care want improve need plan currently procedure reach somebody database access plan query example the proposal match production set chatops plan chatop query
847,20615133,5.0,improve high frequency database query this track gitlab ee code change improve high frequency database query find the query take total database time primary expect generate significant load database
848,20575927,1.0,ssl certificate expire soon expire friday may pm eastern daylight time
849,20535738,1.0,make redirect
850,20533888,2.0,set lifecycle policy change storage class object log archive bucket the log archive bucket tb represent significant cost come object storage this bucket month datum coldline storage cost this issue discuss policy change storage class object coldline storage before work issue need decide lifecycle policy i suggest start old references
851,20420544,2.0,ssl certificate expire soon track work page ssl certificate expire soon
852,20353680,1.0,update cert production change criticality change objective prevent certificate change type services impact change team members change severity buddy check test stage i think test staging will pair colleague coverage schedule change see comments duration change see comments detailed step change each step include see verify certificate valid locally re format certificate vault entry use gkms vault edit replace ssl certificate find way verify change work production
853,20353636,1.0,update cert production change criticality change objective prevent certificate change type services impact change team members change severity buddy check test stage i think test staging will pair colleague coverage schedule change see comments duration change see comments detailed step change each step include see verify certificate valid locally re format certificate vault entry use gkms vault edit replace ssl certificate find way verify change work production
854,20349077,1.0,update cert production change criticality change objective prevent certificate expire change type services impact change team members change severity buddy check test stage i think test staging will pair colleague coverage schedule change see comments duration change see comments detailed step change each step include see verify certificate valid locally re format certificate vault entry use gkms vault edit replace ssl certificate find way verify change work production
855,20347703,2.0,standard database console access archive replica analytic follow to knowledge standard read database console access production replica ie participate patroni ha cluster i suggest implement access archive replica case like this allow people run adhoc query freedom timeout concern affect production ha cluster datum
856,20326363,3.0,rca for google load balancer anomalies summary there problem google ilb health check cause database load balancer send traffic read secondary node instead primary the incident issue the problem start slowly afternoon manifest pull mirrors fail course hour start compound sidekiq job start retry web node start direct read database affect database d zone team attribution minute downtime degradation impact metric what impact incident increase error especially sidekiq job pull mirror work who impact incident all user especially pull mirror how incident impact customer mirror unable run sidekiq job fail web hook fail how attempt access impact service feature how customer affect how customer try access impact service feature include additional metric relevance provide relevant graph help understand impact incident dynamic detection response how incident detect the alert pull mirror overdue there little actual evidence hour seemingly unconnected suspicious behavior the incident issue open base instinct suspicion datum do alarming work expect the exist alert work base number error observe helpful identify source problem how long start incident detection minute how long detection remediation hour remediation be issue response incident bastion host access service available relevant team memeber page able the rackspace support layer add lot overhead google timeline utc pull mirror jobs stop process utc first alert pager utc troubleshooting utc ran resort command runbook change fix utc incident create troubleshooting continue utc incident start utc tweet send update utc rackspace ticket open utc first attempt rackspace utc page dbre assessment utc got hold rackspace team hop zoom troubleshoot issue end utc gcp respond confirm small traffic send unintentional node utc confirmed theory lb utc gcp respond update narrow issue utc remove read patroni node respective instance group primary utc reset redis key set trigger worker pick job utc queue job alert clear tweet update utc gcp investigate utc gcp resolve inconsistency utc start sidekiq cluster node d zone root cause analysis an update google what exactly happen health status ilb backend fail propagate instance d request forward backend configure healthy that reason observe traffic go ilb issue happen close server vm close client vm last update google after investigation believe temporary inconsistency ilb backend health observe vm instances d cause rollout control plane component zone this issue affect small virtual private networks likely trigger vpc peering the engineering team resolve inconsistency believe impact follow s blameless manner core root cause analysis why job intermittently fail why they write database why the load balancer send traffic wrong database why the load balancer interpret health check correctly why the rollout google control plane component d cause health check report incorrectly load balancer what go start follow identify thing work expect the team pile nicely everyone contribute spread workload even alert cover case exist alert let know right any additional out go particularly the team amazing job difficult troubleshooting narrow issue conclusive evidence point rackspace support able help troubleshoot issue sure basis cover stackdriver monitoring metrics helpful check lb egress traffic good proof point traffic send unintended nodes zone the graph exactly show impact recovery nice even midst chaotic incident take time create execute peer review change request remove instance instance group instead take action fly what improve whether happen google we pressure careful build resilience job especially sidekiq job idempotent the incident start late afternoon hawaii time the us leave day eu wake lot help available hour the runbook pull mirror say post ask help backend slack channel when post eu early morning hour take long time our database alert monitoring cover problem like there indication happen exist issue relate reason think plan we brainstorm plan test scenario like system react we practice make change production infrastructure scenario feel comfortable similar change incident during incident remove instance instance group vs remove instance group lb take little bit time uncertainty setup system react we able collectively talk write step execute able practice non incident time corrective action list issue create corrective action incident for issue include follow bare issue link issue label action include estimate date completion corrective action include name individual own delivery corrective action guideline blameless rca guideline s
857,20325677,4.0,update fe lb host ubuntu bionic part unblock pre chef repo mr gitlab com infrastructure mr gstg chef repo mr gitlab com infrastructure mr gprd chef repo mr gitlab com infrastructure mr production change issue
858,20325450,3.0,reconsider postgres node current picture temporary file generate currently set mb postgres nod its insufficient temporary file creation statistic postgre checkup datum master indicator value stats since stats age temp file total size temp files total number file temp files total number file day temp files avg file size mib gib temporary file write day average accord day stat a replica indicator value stats since stats age temp files total size temp files total number file temp files total number file day temp files avg file size gib tmp files data daily this look like huge inefficiency relatively easy fix here memory current picture use memory the master memtotal kb memfree kb memavailable kb buffer kb cache kb swapcache kb active kb inactive kb kb kb kb kb a replica memtotal kb memfree kb memavailable kb buffer kb cache kb swapcached kb active kb inactive kb kb kb kb as ram available right gib os file cache on master average tmp file size on replicas consideration increase allocate fully session session use fraction on hand single session use multiple portion statement execute multiple operation need memory so rough calculation bad case scenario session consume fully time mib this small fraction total ram available if raise mb time current value mib it problem gib gib os file cache and bad case actually happen but mb allow rid temporary file additionally know postgre checkup report read os file cache rare hundred block second this mean work datum set present buffer pool this tell easily increase value like mb log analysis let average stddev temporary file size give day april bash sudo journalctl grep temporary file awk print awk end printf f f average size mb standard deviation mb this mean raise mb insufficient want decrease number file generate day dozen additional increase value like mb need proposal raise mb postgres node restart require config reload after day analyze tmp file generation memory consumption if need reconsider
859,20320734,1.0,chef fail gprd board chef run fail gprd hour fatal stacktrace dump chef cache chef fatal please provide content file file bug report error cookbook version depend chef version run chef version fatal chef run process exit unsuccessfully exit code look like generate cookbook version depend chef version run chef version chef embed lib ruby gem lib chef cookbook chef embed lib ruby gem lib chef cookbook block validate chef embed lib ruby gem lib chef cookbook chef embed lib ruby gem lib chef cookbook validate chef embed lib ruby gem lib chef chef embed lib chef embed lib ruby gem lib chef chef embed lib ruby gem lib chef run chef embed lib ruby gem lib chef block chef embed lib ruby gem lib chef fork chef embed lib ruby gem lib chef chef embed lib ruby gem lib chef block chef embed lib ruby gem lib chef chef embed lib ruby gem lib chef chef embed lib ruby gem lib chef application chef embed lib ruby gem lib chef application block chef embed lib ruby gem lib chef application loop chef embed lib ruby gem lib chef application chef embed lib ruby gem lib chef application chef embed lib ruby gem lib chef run chef embed lib ruby gem bin chef require bin chef load bin chef main we similar problem version
860,20305152,1.0,update cert production production change criticality change objective prevent certificate change type services impact change team members change severity buddy check test stage i think test staging will pair colleague coverage schedule change see comments duration change see comments detailed step change each step include see verify certificate valid locally re format certificate vault entry use gkms vault edit replace ssl certificate find way verify change work production
861,20287213,2.0,pull mirror run production pullmirrorsoverduequeuetoolarge alert fire pull mirror job process follow runbook cause mirror job run moment stop soon afterward
862,20272407,1.0,rebalance git nodes file file we look rebalance figure growth steadily grow month
863,20263041,1.0,chatops access request similar request what i like give chatop access i enable feature flag i think entail add group why in manage team group feature flag time i like test feature process enable feature flag
864,20253280,1.0,dr gitaly job prometheus use query line like the service run server stor sudo gitlab ctl status run gitaly pid run log pid run logrotate pid run log pid
865,20247897,1.0,important ssl certificate expire may open issue extra attention ssl certificate expire contribute may gmt may gmt may gmt
866,20243671,2.0,add runner version var gitlab com cookbook in cookbook gitlab gitlab runner package version lock prevent unwanted upgrade we add version role variable intention clear automate upgrading chef instead have manually
867,20240684,2.0,upgrade runner gitlab runner old version version recently introduce feature runner need version this block review app deploy we need upgrade runner
868,20235184,1.0,clear artifact bucket as week officially move gcs artifact we delete artifact remove bucket
869,20183369,3.0,omnibus version incorrect chef the omnibus version value correct environment after deploy role look like production role omnibus gitlab package key gitlab ee repo gitlab pre release true version description this be a placeholder for takeoff version be not valid do not upload to chef gprd omnibus version staging appear correct role omnibus gitlab package key gitlab ee repo gitlab pre release true version description this be a placeholder for takeoff version be not valid do not upload to chef gstg omnibus version dr note lack version role omnibus gitlab package key gitlab ee repo gitlab pre release true description this be a placeholder for takeoff version be not valid do not upload to chef dr omnibus version pre update production role omnibus gitlab package key gitlab ee repo gitlab pre release true version description this be a placeholder for takeoff version be not valid do not upload to chef pre omnibus version
870,20175680,1.0,chatops access request what i like give chatops access order enable disable feature flag why be able enable disable feature flag allow test feature hide feature flag qa process background i follow example issue
871,20173929,2.0,automate postgre checkup report let automate running produce new checkup report week the result paste issue infrastructure queue review an example report lead lot great insight et al
872,20150946,1.0,access dr prometheus my account able access
873,20141253,1.0,update credential we need update follow field chef vault the value store subscription portal vault share infrastructure team
874,20137208,3.0,switch hourly rotation elastic index some elastic index big tb size this cause problem see elastic cluster in order fix cluster i discuss solution switch hourly logging fairly straight forward approach help we aim follow log rail gitaly workhorse the nginx log large intend stop send elasticsearch store gcs note change need retention tool
875,20131255,1.0,rebuild invalid index index project table invalid btree invalid detect recent checkup it week ago report day ago show invalid index expect time operate use name index rebuild index an open question cause action rebuild perhaps good option
876,20081851,2.0,open port op prometheus server i try setup pushgateway ops prometheus server scrape currently op prometheus server able connect port server anyone help open port op prometheus server
877,20075787,2.0,fix test kitchen patroni cluster create fix test kitchen patroni cluster chef
878,20015768,2.0,apply gcp size recommendation downsize node dr in look gcp size recommendation appear prod difficult similar potential saving dr case file nn stor nodes move persistent disk standard cpus able scale recommendation scale file nn node thing perform turn geo we able downsize geo run recommendation this initially save vcpu month cc assign devin give work dr geo item
879,19972071,1.0,turn ssl compression postgresql zfs replication slot the database channel get alert replication slot currently ssl compression sql select compression t record pid ssl t version cipher ecdhe rsa gcm bit compression t clientdn select record plugin physical datoid database active t xmin
880,19972057,1.0,poc create test environment stage database datum context any comment appreciate
881,19960991,2.0,osqueryd logrotation log rotate especially worker log grow fast
882,19937588,3.0,make cron cleanup temp file share certain age as research ahead add cron clean share set time this bandaid place
883,19923459,2.0,daily runner usage extract report ci give daily runner usage aggregate total ci runner minute day follow category gitlab gitlab gitlab fork simple check gitlab ee project path free public share runner non pay public project free nonpublic share runner non pay non public project pay share runner non free project private runner non shared runner project
884,19920571,1.0,database review x x x x
885,19913546,1.0,consider increase runner timeout runner minute currently runner minute timeout lead rspec job timeout test actually pass we consider increase timeout like minute what think
887,19796443,3.0,investigate tb directory in begin investigate server fill quickly it determined directory opt gitlab gitlab rail tmp tb datum need figure get decide delete give tmp i imagine delete need verify purpose get point
888,19752181,1.0,jump link slack lead old archived channel when click jump alert recovery acknowledgement un acknowledgement production channel open archive infrastructure channel i expect lead original alert message production channel
889,19751852,3.0,add alert osqueryd metric we add alert metric cpu disk usage
890,19751762,3.0,define production readiness process service define document criterion service declare ready production this lead process ensure production readiness new exist service
891,19748639,2.0,rca osqueryd consume resource production summary affect n a team attribution infrastructure security minutes downtime degradation n a impact metric what impact incident cpu load disk io machine go consume equivalent core overall production fleet who impact incident security infrastructure team have spend time finding fix root cause prevent impact how incident impact customer prevent x incorrect display y as cpu usage osqueryd isolate core server measurable slowdown service contribute slow response time high load site how attempt access impact service feature n a how customer affect n a how customer try access impact service feature n a overall osqueryd cpu consumption env miss week switch thanos longterm storage week overall cpu consumption detection response start following how incident detect hour prod deployment sre oncall page root disk fill git server grow osqueryd datum dir investigation show issue embed rocksdb high cpu usage do alarming work expect the exist alarming work notify system come critical state specific alert monitoring osqueryd misbehave also filesystemfullsoon alert go pagerduty disk fill fast risk fill sre oncall take notice how long start incident detection prod deployment start utc filesystem alert come slack channel pagerduty sre oncall start action how long detection remediation immediate remediation filesystemfull alert trim osqueryd profile take place day finally stop production day rocketdb corruption issue stop be issue response incident bastion host access service available relevant team member page able security team have insight metric log osqueryd slow process analyse fix issue timeline utc start deploy osqueryd production utc filesystemfullsoon alert start nodes utc sre on stop osqueryd git node clean datum dir utc sre on noticing uptycs cookbook instal restart osqueryd chef client run utc security move git host new profile collect datum utc sre on deploy mr fix uptycs cookbook reinstall issue utc sre on noticing git node get new profile utc security apply new profile miss git nodes utc sre on providing list host have osqueryd issue utc new profile local caching apply host utc sre oncall see host issue ping security slack utc process monitoring osquery deploy show lot cpu usage fleet utc osqueryd stop production stop waste cpu resource solution find utc deploy uptycs wich contain fix rocksdb staging solve cpu utilization issue root cause analysis osqueryd show anomalous behavior production that cause profile collect datum high frequency bug specific rocksdb uptycs lead db corruption spinning cpu usage core we collect different metric assume default uptycs profile negative impact host local caching that assumption proof wrong test presumably test host workload like production visibility behavior osqueryd we miss visibility implement monitoring behavior osqueryd process go production maybe examine osqueryd log test collect solve issue production take security team direct access staging production system observe log system resource usage infra team insight osqueryd configuration profile expertise product what go our exist monitoring alert aware issue cause issue production first response infra security team fast work closely find root cause fix osqueryd configure resource usage limit exhaust node core node maximum what improve we proper testing bring new service production sre security work close planning develop new service sre strong lead ensure production readiness new service go live monitoring alerting runbook we solid cookbook proper test way enable disable service initially cookbook instal osqueryd chef run way control service we collect necessary metric avoid collect metric available exist monitoring system we stop osqueryd production early instead try fix discover issue process corrective action formalize document production readiness process add alert osqueryd cpu disk usage review profile osqueryd metric collect need deploy fix uptycs version
892,19745425,2.0,change patroniisdown back metric right use metric export mtail determine patroni prove unreliable time instead use process exporter
893,19717209,8.0,blueprint machineresourcemanager we want able upgrade os kernel compute machine want downgrade find issue the traditional way apt directly host succeed process like unattended upgrade however take level go base image creation process the idea fully automate process build new image new os kernel test sure basic functionality work issue there work what need we need come process base image orchestration work configure way want give service replace exist node new able host this issue track blueprint initiative it call machineresourcemanager in work field human resource function deal hiring fire provide career growth development promotion etc employee when demand increase hr hire people temporarily hr let employee onboard new employee these similar try achieve work difference machine we want able replace machine add remove need
894,19702775,1.0,access request mr access ops gitlab cookbook request give access submit mr gitlab cookbook project this simplify bump version number add cookbook area like op gprd gstg environment json
895,19683326,5.0,setup omnibus testbe env setup omnibus testbe env
896,19683283,5.0,setup monitoring testbe env setup prometheus server testbe env network peer op configure prometheus alertmanager
897,19683151,3.0,create testbe bastion host stand bastion host testbe add ssh config
898,19682974,3.0,create chef env testbe create chef environment testbed environment
899,19680849,5.0,create terraform env testbe create new terraform env testbed environment make work gitlab testbe gcp project store terraform env credential
900,19680731,2.0,create new gitlab testbe gcp project create new gcp project set testbed environment enable gcp api kms api setup kms key credential
901,19668388,1.0,loggingvisibilitydiminishe alerts fire loggingvisibilitydiminishe pubsub message queue unacked message old second exist queue minute this lead loss log datum service pubsub queue high pubsub message queue unacked message old second exist queue minute this lead loss log datum pubsub queue high pubsub message queue unacked message old second exist queue minute this lead loss log datum label label alertname loggingvisibilitydiminishe channel production env gprd monitor default provider gcp region east severity warn pubsub workhorse inf gprd sub
902,19657128,3.0,cleanup fix obsolete alert the expression back follow alert return datum need fix remove x highgitcatfilecount x blackboxgitpullhttp x blackboxgitpullssh x blackboxgitpushhttps x blackboxgitpushssh cicdtoomanyrunningjobspernamespaceonsharedrunner cicdrunnermachinecreationratehigh x cicdrunnerscachedown x gitlabcomlatencyweb x gitlabcomlatencywebcritical x gitlabcomlatencyapi x gitlabcomlatencyapicritical x gitlabcomlatencygit x gitlabcomlatencygitcritical x gitlabcomdown x wwwgitlabcomdown x monitorgitlabnetprometheusdown x monitorgitlabnetnotaccessible x otherprometheusdown x x x x x snitchheartbeat x staginggitlabcomdown x x frontendworkerdown
904,19645992,2.0,investigate difference redirect behaviour nginx fastly redirect break case fix follow mr self manage category codefresh investigate behave differently nginx redirect
905,19625912,1.0,move zoom sync script gitlab server currently zoom sync script house gitlab server cookbook we need create repo cookbook public ultimately want run ci cd similar decommission current cron server run
906,19572973,1.0,upload nessus package per request nessus package add aptly repo file available
907,19550066,1.0,upgrade packagecloud packagecloud release resolve issue have backup upload implement multi upload time upgrade this basically big deal certainly comparison smile
908,19544925,1.0,analyze checkpoint frequency currently peak checkpoint minute grafana peak opm this base wrong reading graph the primary actually low checkpoint frequency this strike frequent checkpoint we increase order reduce frequency concern watch decrease checkpoint frequency mean increase recovery time what deem tolerable we need measure would interesting measure io impact decrease frequency cc
909,19543692,1.0,create osqueryd runbook write runbook deal osqueryd
910,19511512,1.0,deprovision per slack discussion machine unmaintained service deprovisione it single machine run security monkey gcp project cc scheduling
911,19447805,1.0,analysis haproxy alert tuning impact as fatigue fine tune haproxy alert base datum analysis this issue track tuning effort help oncall alert fatigue
912,19434373,2.0,lower repository size limit gb as final step let x mail affect user repository gb x adjust limit gb
913,19434064,2.0,many staging alert page production staging alert page production staging silence this happen alert tag incorrectly this mr fix thing alert the thing need we need come well way sure alert tag default send alert manager
914,19433426,5.0,monitor osqueryd depend osquery profile server activity osqueryd consume significant system resource when cache event locally break local db lead fill disk old data file clean anymore we need monitor alert datum dir size cpu usage
915,19433078,1.0,create repacke dashboard in order execute repacke want good insight lock behavior this help decide kill repacke process let add useful lock monitoring grafana
916,19426091,5.0,database reviews x x x x x x x x x contribution x x x x x x x x x x x x x x
917,19422644,2.0,fix logrotation log log gitlab gitlab rotate we need enable logrotation stop fill disk
918,19413917,2.0,add field validation it look like variation field value this cause service catalog app throw error try parse file this issue track work add field validation exist variation identify correct future variation prevent
919,19411173,1.0,chatops access request what i like give chatop access i enable feature flag staging i think entail add group why in manage team group feature flag time i like test feature process enable feature flag in particular add ability chatop enable feature group instead project i unable verify actually work i access from slack how i access chatop when i try run feature list saml i error whoops this action allow this incident report a slack search suggest i ask include username
920,19410413,1.0,update service catalog app late change deploy late app
921,19349042,2.0,do install osqueryd chef client run the gitlab uptyks cookbook instal osqueryd package unconditionally local file lead installation chef client run we need resource idempotent
924,19330376,5.0,enable redirect marketing infrastructure project move discussion infrastructure project x create yaml file redirect definition start follow content use format source old target new source old old path target new x add ci job build stage validate yml file x source path appear twice collection x target path appear twice collection x target path appear source path x redirect loop x create manually fastly x an edge dictionary name redirect x link dictionary version config x create redirect condition use table follow docs redirect actually simple need vcl snippet x update redirect api x write ruby script x read yml file parse part exact match simple regexe literal match regexe x exact match x item fastly edge dictionary curl x compare exact match yml x item exist fastly yml delete fastly batch update json json item op delete op delete x use batch upsert api json json item op upsert op upsert x literal match regexe x generate upload recv dynamic vcl snippet error dynamic vcl snippet they need include rule inside recv curl put content type application content ee error permanent error permanent error curl post content type application content set set moved set obj synthetic return set set moved set obj synthetic return x add protect env var api key fastly api tie fastly service an env var key present ci cd config i turn protect add key staging add ci job build stage validate script x add ci job deploy stage trigger script work confirm redirect fastly fully operational remove redirect chef chef repo batch update curl curl patch content type application json batch update upsert
925,19316608,4.0,set initial sync artifact as migrate artifact gcs need set initial sync the set initial sync quick process path migrating
926,19315275,1.0,pullmirrorsoverduequeuetoolarge staging we get error go pagerduty they correct alert time go right place it go production time like and error the threshold k overdue update graph look like dashboard link
927,19299442,3.0,add thanos end similar need oauth proxy access thanos query end
928,19269541,2.0,restore project generate time ssh key pipeline fail error error login profile size exceed delete profile value additional space like lot ssh key pipeline exist clean obsoleted sshkey gcloud compute os login ssh key list grep fingerprint echo gcloud compute os login ssh key remove i think consider publish script gcs bucket let instance download script gcs bucket run startup script instead scp ssh instance execute script
929,19256881,1.0,iam policy gitlab internal tread request iam permission gitlab project role role tread instruction
930,19234335,1.0,add outreach cname brand url to help deliverability email send outreach create brand url suggest csm instructions set outreach support article and step point cname brand url go please let know question
931,19231125,2.0,nfs high load sidekiq archivetraceworker jobs please note incident relate sensitive datum security related consider label issue mark confidential summary a brief summary happen try executive friendly possible affect team attribution minute downtime degradation impact metrics start following what impact incident service outage sub service brown exposure sensitive datum who impact incident external customer internal customer specific team how incident impact customer prevent x incorrect display y how attempt access impact service feature how customer affect how customer try access impact service feature include additional metric relevance provide relevant graph help understand impact incident dynamic detection response start following how incident detect do alarming work expect how long start incident detection how long detection remediation be issue response incident bastion host access service available relevant team memeber page able timeline yyyy mm dd utc happen utc happen yyyy mm utc happen utc happen root cause analysis the purpose document understand reason cause incident create mechanism prevent recur future a root cause person way writing refer system context specific actor follow s blameless manner core root cause analysis for necessary start incident question happen keep iterate ask time while hard rule time help question deep find actual root cause keep min come answer consider follow different branch example usage s the vehicle start problem why the battery dead why the alternator function why the alternator belt break why the alternator belt useful service life replace why the vehicle maintain accord recommend service schedule fifth root cause what go start follow identify thing work expect any additional out go particularly what improve start follow use root cause analysis explain improve prevent happen be improve detection time detection be improve response time response be exist issue prevent incident reduce impact do indication knowledge incident place corrective action list issue create corrective action incident for issue include follow bare issue link issue label action include estimate date completion corrective action incldue name individual own delivery corrective action guideline blameless rca guideline s
932,19227312,3.0,improve postgresql configuration file presently chef recipe result file place opt gitlab postgresql appear default configuration file version early distribution contain comment parameter remove later release it lack new setting comment noise frankly mess difficult read patroni override base configuration variable patroni need able control it include previous file override add configuration there effective setting match file default setting confusing i like configuration file clear correct current postgresql version setting explicitly define write match order documentation chef templating able handle fairly easily ideally chef recipe hardcode use specific version instead set default major version overridden
933,19205575,4.0,haproxy alert fire base error rate static value from like alert base error rate second average rate increase total error alert scope work increasedbackendconnectionerrors m what mean error trigger alert regardless total connection increase time increasedserverconnectionerrors m here mean error trigger alert increasedserverresponseerror m here error the proposal calculate base error rate calculate error count total count for example increasedserverresponseerror backend m we set threshold error rate the high number request process week request min ref the current threshold mean error request trigger alert page this
934,19203347,1.0,adjust haproxy alert threshold connection response error from issue track work increasedbackendconnectionerrors increasedserverconnectionerrors increasedserverresponseerrors set threshold for reason proposal look
935,19162671,2.0,use disk snapshot restore instance testing database review need test query database review restore production wale base backup pipeline normally take hour i propose add step snapshot boot disk datum disk nightly restore schedule delete restore instance the snapshot restore instance quickly test query database review
936,19161532,2.0,rebuild dr file node cheap storage file node currently little ssd storage due cost concern need spin disk step involved x tear delete file node disk x request quota increase spinning disk optionally decrease ssd x spin new instance disk type change i help geo team clean way reset replication state replication start currently pause
937,19158458,2.0,lower repository size limit gb as reach gb limit let x adjust limit gb x mail affect user repository gb
938,19154037,1.0,new training gcp project we plan couple workshop contribute like use dedicated gcp project attendee create delete cluster we gitlab training thank
939,19137570,2.0,request wildcard cert in distribution team add eks ci cluster chart like wildcard cert this copy cloud native vault alongside exist wildcard cert cloud native ci domain
940,19113981,2.0,cookbook publish haproxy break it look like fail time
941,19112999,1.0,return correspond error code haproxy we error page report appropriate error end user instead code errorfile haproxy errorfile haproxy errorfile haproxy errorfile haproxy errorfile haproxy errorfile haproxy errorfile haproxy errorfile haproxy
942,19066903,1.0,problem google search index someone point appear search result use site operator i find page google index be setting sub domain toggle expand coverage search cc
944,18995094,1.0,configure sentry dr plan
945,18982972,5.0,database reviews x rail x x x x x x x db office hour x db office hour x x x
946,18923188,1.0,ci d readiness review addendum runbook responses in preparation take day day ci cd issue runbook cover alert frequently occur cpu use percent extremely high past hour no disk space leave prometheus prometheus datum no disk space leave gitlab runner manager cicdtoomanypendingjobspernamespace cicdtoomanyrunningjobspernamespaceonsharedrunnersgitlaborg cicdnamespacewithconstantnumberoflongrunningrepeatedjobs cicdjobqueuedurationunderperformant cicdtoomanypendingbuildsonsharedrunnerproject cicdtoomanyarchivingtracefailures
947,18921739,1.0,no log gitaly dr reach kibana it appear log unstructured fluentd expect json log must solve
948,18920687,1.0,raise package upload size limit packagecloud our nightly package mb current we need raise high enable package upload however wait gitlab com gl infra complete require restart packagecloud change job failure slack thread
949,18918858,1.0,run optimize table packagecloud database now finally able clean extra row metadata table need run optimize table order reclaim disk space this lock table run mean able upload package time coordinate ensure block package release it likely minute complete table substantially small upgrade
950,18918317,1.0,update packagecloud backup use xbstream with upgrade able use xbstream backup mysql database this improve speed reliability backup eventually assist move use rds backend enable config need wait gitlab com gl infra complete
951,18914779,2.0,monitor rate pg temporary file creation quoting no alert warning fire database in situation alert warning temporary file write disk
952,18914710,2.0,add useful postgres graph triage dashboard quoting the triage dashboard useful postgres graph lack make postgres item check compare gitaly useful graph indicate gitaly cause trouble
953,18888085,1.0,transfer control
954,18847097,5.0,api reboot investigation troubleshooting api node reboot increase frequency look figure wrong track solution
955,18843826,2.0,cleanup file metadata packagecloud summary in packagecloud add ability permanently disable repo file list metadata result far few row add file table kick background job cleanup exist file row repo turn see detail new feature background packagecloud default keep track file package new role file table database use information generate repo file list metadata file due number file omnibus package size table large impact ability backup database in prior version packagecloud turn metadata file generation change number row add database additional info this relate information recently get contact packagecloud turn file once enable setting kick job delete unnecessary file row database job likely day run lock process note as suggest permanent setting filelist disable permanently enable optional after ensure deletefilesjob job finish job progress indexer status section administrator dashboard run optimize table optimize table significantly reduce disk space size database backup actions x permanent disable metadata gitlab pre release package x deletefilesjob complete x permanent disable metadata gitlab nightly build package x deletefilesjob complete x permanent disable metadata gitlab raspberry package x deletefilesjob complete x permanent disable metadata gitlab unstable package x deletefilesjob complete x permanent disable metadata gitlab gitlab ee package x deletefilesjob complete x permanent disable metadata gitlab gitlab ce package x deletefilesjob complete x run optimize table note table lock upload block x run optimize table note table lock upload block
956,18826757,2.0,look set old gsrm runner gitlab ci gcp project there batch runner run feb gitlab ci project we look run job happen sure properly destroy cc look usage runner anomaly maybe day
957,18825958,1.0,dr site codebase sync primary production site the geo node dr run production node run we need run version activate dr site this matter trigger pipeline make sure succeed if i quick instruction proper way i sure get dr runbook
958,18825664,1.0,dr database stop replicate information add primary database show dr database hour be monitoring the query i select from once fix minimal runbook troubleshoot
959,18824447,2.0,chef client fail dr pubsub checksummismatch below error pubsubbeat pubsubbeat action info processing pubsubbeat pubsubbeat action create gitlab line error execute action create resource pubsubbeat pubsubbeat checksum resource match checksum content resource declaration in chef cache cookbooks gitlab elk recipe source checksum owner group mode true notifie restart delay end compile resource declare chef cache cookbooks gitlab elk recipe pubsubbeat pubsubbeat provider action create retry default source true true gitlab elk pubsubbeat checksum owner root group root mode true path pubsubbeat pubsubbeat verification end system info platform ubuntu ruby ruby revision linux chef client worker chef bin chef client this fix ensure log geo work see
960,18823475,4.0,ensure log geo place plan
961,18815808,2.0,move license gitlab com version gitlab com overview we like license gitlab com version gitlab com project gitlab org group the fulfillment team work app issue board visible non nested group as result single source truth give accurate assessment fulfillment work single release deliverable slip reason simply need remember check issue different place x move gitlab com version gitlab com gitlab org group namespace x move gitlab com license gitlab com gitlab org group namespace
962,18803913,3.0,upgrade kernel api fleet most production alert cause random api server reboot the current theory cause nfs issue lead kernel panic fix kernel upgrade as sequentially reboot api server easily customer impact drain lb priority work framework safely upgrade kernel fleet
963,18791653,3.0,create ansible playbook database service discovery migration for production change wip mr
964,18786952,2.0,make rackspace user infra team item we rackspace user infra team issue track x make user list x update sre entitlement access template x make sure onboarding template verifie access x double check runbook date x broadcast team cred support ticket x amar x ahmad x jarv x michal x hendrik x henri x andreas x jose x skarbek x yun x cameron x alejandro x alex x casey x anthony x craig x devin
965,18783109,2.0,make alert sensible staging the trigger response return on stage nearly request lead alert soon response trigger this happen utc cause slow scan non existing endpoint cause alert hour we maybe add minimum threshold x request s trigger alert bese error rate low traffic server
966,18759420,2.0,implement epic issue link gitlab gitlab implement epic issue link api implement glork
967,18759404,2.0,implement epic links gitlab gitlab implement epic links api implement glork
968,18729497,1.0,configure name servers we want host site siteground i believe need point nameserver let know additional step i aware thank
969,18728253,8.0,upgrade packagecloud we run issue rpm file table packagecloud database sign integer d column run limit we work issue keep rail app update table table possible rpm base package manager issue instal package there new version packagecloud resolve issue this upgrade require downtime we packagecloud discuss detail upgrade upgrade doc disable metadata table doc
970,18726709,1.0,transfer ownership the meltano team need access manage domain setting order reduce request gitlab infrastructure team if need additional information reach cc
971,18719563,3.0,fix gstg alert find way pd all alert pager pagerduty envs pager sre this result consolidate op gstg dr alertmanagers
972,18693446,1.0,can connect port new aws instance hi idea i access port instance aws i check security group instance network acl subnet vpc i check route table port work great netstat tcp listen chain input policy accept target prot opt source destination chain forward policy accept target prot opt source destination chain output policy accept target prot opt source destination
973,18693120,1.0,clean record follow item note gitlab com gl infra need remove record gitlab com account quick review ansible terraform codebase gitlab com gl infra gitter infrastructure verify additional impact change require case i miss detail context
974,18692812,8.0,zfs research best disk layout approach spin ubuntu node zfs iops testing determine good number disk size disk zil placement db nodes storage nodes document test methodology result issue
975,18655131,1.0,redirect codefresh page add redirect tool tool codefresh vs we inbound like codefresh blog post go helper page use generate real comparison page log issue cc
976,18652339,2.0,new gcp project gitlab paas poc testing alpha feature enablement the configure group currently work poc gitlab paas we like create dedicated project gcp enable gcp alpha feature gke sandbox ensure ip space available currently project run ip space block analyze cost easily have filter resource exist project we require project billing enable recognizable gitlab paas also possible great limit access project member configure team resource creation tkuah dgriffith jfargher twatson dgruesso tdavis mcabrera ddavison jcunha jerasmus gbizon mgreile thank
977,18635326,2.0,no registry log kibana i wonder timestamp precede json log
978,18625609,1.0,update email setting greenhouse there additional dns entry greenhouse email configuration type hostname required value cname also i believe whiteliste ip address help reduce instance greenhouse email go spam i provide request thank
979,18612534,2.0,staging delay replica unable follow timeline the delay replica staging unable recover gmt log new timeline fork current database system timeline current recovery point additionally get page production pagerduty schedule i know relate intend staging example alert slack cc
980,18605002,8.0,database reviews x x x x x nik check restore box x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x rail
981,18599765,8.0,zfs establish qa harness packer image build qa harness packer builds should x validate image bootable validate image bootstrappe chef validate entire gitlab suite functions pass exist qa harness promote image select train current stretch goal validate minimum convergence time spin functional machine m
983,18597525,2.0,staging alert mis label go production pager duty the follow alert go production high priority channel pager duty weekend postgres replication lag hour delay replica normal hour patroniisdown all hostname staging additionally op alert go production high priority channel clearly high priority deadman switch db postgre expire
984,18595305,1.0,dr redis server excessive load load average on connect grep opt gitlab redis gitlab embed bin redis cli info replication replication role master
985,18578561,2.0,adjust log retention logrotate keep year nginx log root disk run month we forward stackdriver eventually
986,18576898,2.0,fix alert azure node we get alert alertmanager azure alert route alertmanager so notify disk instance
987,18524638,2.0,reference bootstrap teardown script module version currently gcp bootstrap module iterative improvement versione bootstrap script file legacy terraform monorepo versione file reference pass attribute module ideally versione filename deprecate go forward subsequent version reference change module version we need test sure viable verify case multiple bootstrap version parallel if validate multiple instance bootstrap module time namespace version add copy late bootstrap script module version filename make attribute optional reference old version bootstrap script update call reference module deprecate attribute switch module version
988,18524248,5.0,deploy uptycs production after successful deploy staging step roll production to x setup gitlab uptycs cookbook mirror issue x add ci cd pipeline gitlab uptycs cookbook issue x determine rollout plan role host phase phase issue x deploy issue
989,18523598,2.0,check db relate metric alert dashboard work after scrape db metric new prometheus db instance instaed prometheus gprd need sure complete metric alert work especially general alert grafana dashboard work
990,18523175,2.0,rca loss db metric visibility switch dedicated prometheus instance db metric summary by start scrape db metric new dedicated prometheus instance grafana db dashboard stop work affect postgres team attribution infrastructure minutes downtime degradation impact metrics start following what impact incident we lose visibility db metric metric scrape instance notice who impact incident sre oncall release team have delay release hour how incident impact customer customer impact detection response start following how incident detect get alert miss metric do alarming work expect get alert miss metric datum specific immediately aware go how long start incident detection how long detection remediation be issue response incident bastion host access service available relevant team memeber page able notice early alert db monitoring work anymore page dbre support work dbre schedule communicate clear escalation go ongres support anymore timeline utc general alert operation rate datum service component patroni service miss utc raise concern alert utc incident issue create utc dbre oncall page team start fix grafana dashboard utc release team want deploy sre oncall hold db metric visibility utc sre oncall give green light deploy root cause analysis general db alert datum anymore because db dashboard grafana work anymore because prometheus gprd scrape db datum anymore because change scrape new dedicated instance prometheus db because db metric make load prometheus gprd what go general alert warning miss db metric data notice alert take action make team aware consequence create incident jump adjust dashboard what improve well communication everybody aware change monitoring make sure dashboard alert work change prometheus setup well response alert fully understand miss metric alert corrective action fix grafana dashboards check db metric alert prometheus db communicate new prometheus db instance
991,18519468,2.0,properly secure domain we purchase domain manage appropriate internal group see relevant slack thread
992,18516038,2.0,fix db dashboards new db prometheus server we need adjust postgres grafana dashboard work new prometheus instance
993,18487590,1.0,certificate expire
994,18482930,1.0,raise gcp limit restore project for current limit disk cpu for database relate task benchmarke test new code background migration need production clone when small task new index idea verification small instance usually need test big background migration processing rows in case need core machine give well disk performance the project constantly automate backup verification additionally i raise default disk space grpd type instance grow old size please raise quota if possible double cc
995,18460105,1.0,make automatic terraform module versione mandatory remove so trip time i know trip the new automatic versioning terraform module require fix feat mr break manually tag this easy mess require board change small infrequent i foresee long time i right that mean lot mess merge ahead i propose make impossible merge mr prefix remove automate pipeline make manual tag standard if merge button appear pipeline find prefix title ideal then use prefix want version bump
996,18459505,2.0,reduce space usage leave we need rebalance try control change issue
997,18403832,2.0,create blueprint kernel patch process try abstract reuse
998,18381918,1.0,alerts mis label gitalyversionmismatch alert staging go production pagerduty in alert manager label gstg default pagerduty label gitlab production alert gitalyversionmismatch expr count sum environment version m label channel gitaly pager pagerduty severity critical annotation description during deployment distinct version gitaly run alongside case visit detail version deploy fleet runbook troubleshoot gitaly version title gitaly version gitaly run alongside production minute
999,18346260,3.0,enable geo tracking database the step enable geo enable tracking database this straightforward documentation suggest i create issue track the replicated main database set there separate instance configure tracking database secondary site we point run migration set schema this issue close section documentation successfully complete
1000,43413000,4.0,configure flipper http adapter details now smoothly connect gitlab feature flag feature class we start dogfoode pre production server order evaluate new architecture work properly todo announce reconfigure instance engineer update feature flag maintaince period reconfigure use http adapter the url point gitlab feature flag server project migrate exist flag datum local postgre gitlab feature flag announce migration engineer update feature flag chatop probably well introduce maintenance mode chatop feature flag prevent update feature flag term we production incident feature flag servers note developers change flag chatop rail console you strategy vs gate mapping developer change flag gitlab feature flag ui readonly this chatop extend feature freeze flag production incident we likely follow this issue
1001,38020114,3.0,put osquery grafana dashboard version control we want cleanup manually manage grafana dashboard as osquery dashboard need monitor uptycs deployment roll new version security soon add runbook dashboard folder
1002,36062339,1.0,add cname dns sigstr goal we onboarde new tool marketing operation require new cname dns the record value point
1003,35424143,3.0,ignore instance envs terraform we random change instance plan presumably gcp internally decide run vm different cpu family maybe trigger reboot theory this unfortunate cause unclean plan apply plan need reboot instance intel skylake null as interested far select terraform ignore attribute instance envs instance
1004,35386491,2.0,plan observability team milestone sprint milestone calendar m jun t jun w jun th jun f jun pager wrench wrench wrench wrench working day m jun t jun w jun th jun f jun working day hr total working day
1005,35206403,1.0,update review app prometheus probe we probe configure we serve review app domain we update probe
1006,35205922,1.0,move legacy redirect src fastly we get pagerduty alert cert expire we think actually legacy redirect serve host we terraform redirect environment fastly manage ssl certificate x x x knife vault edit gitlab com default remove old ssl cert fyi case see alert
1007,35178244,3.0,runbook update runbook create restore delay archived replica review existent runbook update process postgresql version also update reference command usage wal g apply wal in environment postgresql instal omnibus please consider case replica sync troubleshoot fix instance
1008,35086129,5.0,completely decommission uptycs fleet security want rollout new version uptycs to prevent glitch uptycs recommend completely remove previous version fleet we need update gitlab uptycs cookbook support de instal package delete datum dir execute decommissioning old uptycs version
1009,35055230,3.0,sidekiq log metric miss op the sidekiq dashboard op datum prometheus metric miss sidekiq log elastic client log server log
1010,35021099,3.0,postgre checkup include information database primary the late postgre checkup report include information primary database instance i suspect configuration issue pipeline access it helpful primary include particularly understand read write workload well the project look i recall list database instance connect ci config i sure true cc
1011,35011535,8.0,integrate db op chat op the db op automation tool integrate chat op
1012,35011423,5.0,setup ci pipeline db op automation with new database op automation code live want ci pipeline setup run different database maintenance task automate ci this help integrate chat op future first iteration run rolling postgre restart replicas ci
1013,35011090,8.0,create ansible playbook postgre restart failover for plan db failover want ansible playbook order task automate repeatable the playbook live new db op repository first iteration automate rolling postgre restart select replicas automate primary failover
1014,34935310,2.0,ssl certificate expiring the certificate expire today
1015,34881743,1.0,redis cache sentinel host label type redi prometheus currently redis cache hosts label redis redis service they redis cache service
1016,34815865,3.0,rebuild fail take cluster in order align patroni cluster node properly number able decommission node need build scratch force instantiate different gcp hardware node hopefully we need rebuild plan primary switchover
1017,34812779,1.0,remove overly broad page rule cloudflare configuration incident this page rule remove configuration the global security setting consider final fall page rule global security cache setting zone level page rule
1018,34812470,2.0,create modify runbook describe identify authenticate vs unauthenticated api call incident as sre i able identify characterize api traffic include traffic authenticate unauthenticated and possible programmatically isolate traffic well craft page rule defense cloudflare haproxy protect site abuse
1019,34812201,2.0,create update cloudflare runbook well address abuse attack event incident as sre i runbook help describe identify abuse attack cloudflare describe tool use cloudflare mitigate event this probably include section create specific page rule ride security level uri add ip address block list change zone wide security level explain global i m attack toggle affect site
1020,34809692,1.0,plan patroni failover shrink cluster we want execute change production patroni cluster require short downtime failover shrink cluster size increase reconfigure terraform template anything we need plan execute change evaluate risk impact failover action need cleanup failover
1021,34790558,1.0,deploy thanos thanos fix
1022,34790154,2.0,export version database load warehouse runbook
1023,34769941,3.0,fix creation wal g grpd postgre replicas during execution chef create wal file instead fill datum i copy file wal e fix confirm overwrite chef need fix chef
1024,34768232,1.0,server find accessible recently today yesterday maybe db replication
1025,34730270,1.0,adjust node disk io quota metric node device as mention observability meeting turn gcp disk iop quota node sum total hdd ssd disk size node not iop disk node we need adjust automatic calculation metric calculate disk we need info metric identify device hdd ssd calculate node saturation
1026,34726385,3.0,alert job process sidekiq corrective action incident rca to avoid situation unaware sidekiq queue process implement alert low rps useful page oncall similar problem link incident as iteration good generous low threshold suggest slack we alert job maintain minimum rps course day if hour alert obviously bit noisy queue decommission
1027,34712939,1.0,clean unused deployment gs staging cluster the gs staging cluster bunch fail deployment deployment relate project long exist version gitlab dast default version gitlab com review sync upstr version gitlab com staging license gitlab com review enable aut t license gitlab com dast default license gitlab com staging
1028,34684452,2.0,node provisioning break gcloud gem install failure the ruby version bundle td agent install gcloud gem complain google protobuf require ruby occur chef run gcloud gem line error expect process exit receive begin output sbin td agent gem install gcloud document stdout stderr error error instal gcloud google protobuf require ruby version end output sbin td agent gem install gcloud document ran sbin td agent gem install gcloud document return one working node google protobuf linux linux linux instal google protobuf manually work sbin td agent gem install google protobuf document sbin td agent gem install google protobuf document fetching successfully instal google linux gem instal
1029,34684174,3.0,gitlab uptycs cookbook stop osqueryd service when set enable false chef role gitlab uptycs osqueryd service affect node stop disabled work this mr fix
1030,34614717,1.0,decomission vfile module revert bootstrap migration change this epic outline series step alter bootstrap process work chef gcp that work hold option place future terraform change break new approach we remove vfile module environment ci org
1031,34592926,1.0,warn alert hackathon gather list warning fire recently frequently fix trivial issue file issue non trivial issue
1032,34558537,2.0,cleanup prometheusruleevalfailure there persistent issue prometheus rule evaluation we need investigate fix
1033,34519880,1.0,fix patroni installation chef when build new patroni node chef run gitlab patronictl miss dependency root gitlab patronictl list traceback recent file patroni bin patronictl line module import ctl file patroni lib site package patroni line module import config file patroni lib site package patroni line module import confighandler file patroni lib site package patroni line module import importerror no module name we need fix chef
1034,34515750,3.0,rebuild add cluster as replacement fail need rebuild bring sync see
1035,34503932,5.0,decommission leave patroni node during postgre upgrade leave cluster version able roll as cluster run fine node constantly need silence alert node expensive commission asap the current primary make decommission complicated savely remove node terraform start high number this mean probably need rebuild add cluster failover lower number node remove cluster decommission
1036,34467351,1.0,allow set specific concurrency limit canary file node the current limit concurrent postuploadpack process we canary slowdown event should revise number canary gitaly node help promote well experience node load what concurrency
1037,34461510,3.0,observability team sprint planning issues planning we use issue means discuss priority observability team milestone references epics roadmap
1038,34438422,1.0,the testbe environment appear route alert production destination alert testbe environment send production slack channel lead false alert example img cc oncall
1039,34398667,5.0,make archive delayed replica work postgre upgrade after upgrade patroni cluster postgre need archive delayed replicas work
1040,34367388,3.0,update delete project restore runbook while project restore i encounter issue need update accord runbook basically case project restore work anymore describe runbook multiple reason need add note runbook
1041,34289578,1.0,ca use symbolic link bin tf have establish symbolic link bin tf directory one local env path order support easy inclusion bin tf path have explicitly add project path path bin tf script fail code reference actual directory link link target execution
1042,34274020,3.0,change low urgency cpu bind machine type as low urgency cpu bind shard receive traffic change machine type standard see
1043,34268356,3.0,add low urgency cpu bind node accommodate cpu saturation we low urgency cpu bind cpu saturation alert probably relate spike job we add node
1044,34234798,1.0,add zendesk external service our testing environment currently zendesk set external service for sake parity i think add production status page event zendesk support outage need report look like admin privilege require add i unable would mind take care
1045,34234548,2.0,request new subdomain this template gitlab team members seek support sre existing template available please fill detail this issue require new subdomain create advisory landing page the project use new subdomain custom domain page detail point contact request if need propose date time date time additional detail format type additional detail sre support need support request details edit
1046,34163906,3.0,generate alert manager config runbook pipeline currently different source truth alert manager route configuration these an encrypted file manually store gcs regularly update see detail the relatively check effort need unify route config proposal add pipeline runbook repo generate config store encrypt gcs then adapt chef repo use gcs file erb directly placeholder follow relate cc
1047,34162364,2.0,provision new hdd base gitaly node production detail point contact request if need propose date time necessary additional detail format type n sre support need as migrate archived repository hdd base storage need provision new node production back hdd this simiar order safeguard production let provision class machine adjust observe behavior system edit
1048,34153471,2.0,export version database load warehouse runbook
1049,34140342,1.0,stage console permission break people rail console access data bag able login anymore week it typo gstg base console node chef role remove ssh allowgroup node
1050,34119354,2.0,investigate chart monitoring dashboard miss datum follow chart dashboard datum error
1051,34001861,1.0,elasticcloud watcher trigger project name the alert trigger post slack long include project project file server average gitaly wall time second average rate invocation second sec example slack message gitlab internal currently sre help direct right place cc
1052,33961346,5.0,setup grafana image renderer service grafana recommend move away phantomjs adopt grafana image renderer plugin definition done the image renderer deploy gke instance slackline verify work new image rendering
1053,33958578,1.0,document runbook gitaly use housekeeping button lot upload pack process relate
1054,33955237,4.0,document process import pgbouncer log analysis count i like approach incident yesterday i like note step proceed collect log pgbouncer table structure create import datum query execute analysis please let know add process perhaps enable connection log pgbouncer i like implement future automate extract support analysis metric pgbouncer also break like summarize number connection status pool utc log stat xact s query s b s b s xact query wait utc log stat xact s query s b s b s xact query wait utc log stat xact s query s b s b s xact query wait utc log stat xact s query s b s b s xact query wait utc log stat xact s query s b s b s xact query wait utc log stat xact s query s b s b s xact query wait utc log stat xact s query s b s b s xact query wait and retention time database we partition table
1055,33945354,1.0,use shard label distinguish pgbouncer sidekiq pgbouncer node currently possible distinguish pgbouncer main pool metric pgbouncer sidekiq pool nod hardcode database connection name regular expression match host name we use shard label distinguish metric possibly pgbouncer node minor corrective action cc
1056,33886329,1.0,configure chef client gitlab server cookbook default we currently configure chef client role file this lead copy n paste x add configuration recipe gitlab server x rollout new recipe gitlab
1057,33871579,2.0,geo replication break staging postgres update attempt after postgres update attempt geo team report replication break staging although affect prospect production attempt geo enable production fix unblock geo team
1058,33871203,1.0,delete stale branch op chef repo there thousand stale branch should clean
1059,33837283,1.0,reenable usage ping gitlab admin interface overview reenable usage ping gitlab admin interface we disable usage ping great fixing we reenable again next step x go admin area setting metric profiling usage statistic check usage ping x please post screenshot usage ping setting
1060,33763049,1.0,license database extract hi create issue request license db extract hand book command require owner acl se on extension g can share file slack time thank
1061,33762548,2.0,export version database load warehouse runbook
1062,33753070,1.0,add runbook note remove haproxy machine gcp lb follow
1063,33692496,1.0,set variable gitlab live environment we get close have merge with allow run test formless login mechanism live environment staging pre prod canary production need set environment variable gitlab environment you find variable value team gitlab qa access token could help cc
1065,33657695,1.0,reduce disk space contention ci runner vms create gsrm as corrective action reduce number job serially handle runner vm each time runner vm run job accumulate residual disk space docker container volume sometimes unlucky vm run combination job fill disk this start happen fairly start affect efficiency development work for background summary note this cheap easy option if turn insufficient revisit option
1066,33650500,8.0,upgrade patroni the goal upgrade patroni downtime failover release note the initial step plan chef client database node execute patroni node ack new version chef mr i patroni new process launch check positive resume log database patroni new step comment disable chef node update patroni python package pip pause patroni cluster maintenance mode gitlab patronictl pause restart patroni cluster maintenance mode gitlab patronictl restart resume maintenance mode patroni cluster gitlab patronictl resume merge chef new version patroni restore chef node acceptance criterion x create runbook document process execute x rollout stage new patroni version execute failover verify integration work properly traffic routing enable checksum staging pause strategy x rollout new patroni version production
1067,33629473,1.0,enable praefect gstg gprd this new setting allow replication job store postgresql it expect impact normal operation praefect outside replication see detail
1068,33616634,1.0,incident review lack observability incident summary prometheus alertmanager operational thanos completely unavailable grafana alert trigger use elastic watcher http input perform simple check prometheus alertmanager thanos grafana operational alert monitor stack external step take troubleshoot improve time detection dashboard monitoring component however lot chart besides monitor external system we thano send trace info elastic apm the log available kibana prometheus prevent happen change review simply miss fact result circular dependency affect team attribution minute downtime degradation customer impact who impact incident all employee attempt use infrastructure department monitoring system what customer experience incident none how customer affect none if precise customer impact number unknown estimate potential impact n incident response analysis how event detect grafana dashboard time how detection time improve monitoring grafana system latency thanos queriie alert how reach point know mitigate impact how time mitigation improve post incident analysis how root cause diagnose how time diagnosis improve do exist backlog item prevent greatly reduce impact incident yes be incident trigger change deployment code change infrastructure yes link issue represent change yes timeline all time utc change rule result circular dependency thanos transaction duration start request duration go roof start hit timeout add monitor label rule server add filter record rule skip rule server improve route why s this section mean dig lesson learn corrective action limit consider dive deeply example customer experience inability create new project a code change deploy contain escaped bug why bug notice staging the integration test use case miss why integration test use case miss it inadvertently remove refactoring test suite why test suite refactore as effort decrease mttp why hour notice issue production the initial alert supresse false alarm why alert suppress the system dedupe alert inadvertently suppress alarm duplicate why hour resolve issue production the change carry escape bug contain database schema change roll change impossible engineering engage immediately oncall sre conduct forward fix lessons learn be explicit lesson learn carry forward these usually inform corrective action example the result refactoring activite integration test review test refactor our tooling dedupe alarm integration test ensure work exist newly add alarm corrective actions investigate fix monitor relate metric unavailable grafana issue add alertmanager alert thanos latency matter define slo threshold epic add alertmnager alert component monitoring stack add elastic watch alert monitoring stack start jaeger elastic apm trace prometheus alertmanager grafana support jaeger prometheus come release send log thanos alertmanager grafana kibana simply log lot set staging environment monitoring stack test change production i think overkill focus well monitoring alert instead migration kubernetes creation staging env easy chef help migration guideline blameless rca guideline
1069,33612884,3.0,re enable indexing gke log reduce schema detailed error expansion match field limit get background config option adjust api persistent setting dynamically updateable need bring number field index here example request response detail version true size sort order desc boolean source exclude aggs field utc script source lang painless field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format field format query bool filter type query externaldiffuploader lenient true query range format gte lte highlight field response detail take false shard total successful skip fail failure shard index pubsub rail inf node reason type reason fail create query index pubsub rail inf type reason field expansion match field limit get shard index pubsub rail inf node reason type reason fail create query index pubsub rail inf type reason field expansion match field limit get hit total hit
1070,33565099,1.0,thanos storage enable testbed the testbe environment thano storage enable sidecar setup thano component this cuse prometheus send datum gcs useful
1072,33532275,2.0,thanos alert rule configure suggest alert monitor thanos setup at present alert configure cluster we consider add for example suggest alert point frequent rule group failure thanos ruler node cc
1073,33529835,1.0,investigate dip redis cache latency apdex we previously investigate apdex spikiness during investigation find apdex dip correlate cpu saturation the cpu saturation address way upgrade instance type apply rate limit user produce traffic burst this drop baseline cpu utilization smoother that class apdex dip long occur however appear new class latency apdex dip correlate cpu saturation from redi cache overview dashboard these event frequent previously dip high amplitude get amplify look apdex long period time here day please note dashboard use aggregation amplify low dip make thing look bad actually look long time frame the change behaviour appear correlate exact date upgrade instance march this change include change underlying instance type change number cpus change kernel version gcp gcp it prove server issue since apdex measure client possible happen client some possible step gather cpu profile redis process perf validate claim cpu burst event gather datum process run redis host time event validate process contribute undo variable change start instance type change redis host
1074,33518307,3.0,provision new hdd base gitaly node staging as migrate archived repository hdd base storage need test process staging need hdd base gitaly node provision
1075,33517848,1.0,rename alert slack channel alert channel need rename adjust alertmanager config actual name configure default attribute alertmanager cookbook partially overwrite chef role try channel id instead name am config
1076,33514849,2.0,certificate expire sslmate certificate expiring day related email detail recently expire x x x upcoming expiry x x set auto renew x x x x x x cc
1077,33509653,2.0,rollout thanos thano release number memory performance improvement
1078,33509628,4.0,point implement postgresql upgrade ansible playbook we need support sre implement ansible playbook upgrade execution mr apply change chef make snapshot database rollback scenario
1079,33498871,1.0,install postgre debug symbol host run postgre to aid profile postgre stack add debug symbol package main postgre package this apply patroni manage host host run package postgre apt repo the postgre binary strip symbol add package profiler like perf useful shell apt cache search dbg debug symbol
1080,33430869,4.0,create new gitaly storage shard node replace nfs gitaly storage shard nfs usage at our usage target specify try maintain usage new project creation quickly cause usage nfs there currently gitaly shard nodes configure accept new project maintain level availability important avoid shard fill quickly to remove single node new project storage rotation cluster prevent usage acceleration new gitaly node create add list shard configure gitlab application store new project repository create production change issue template
1081,33430844,4.0,create new gitaly storage shard node replace nfs gitaly storage shard nfs usage at our usage target specify try maintain usage new project creation quickly cause usage nfs there currently gitaly shard nodes configure accept new project maintain level availability important avoid shard fill quickly to remove single node new project storage rotation cluster prevent usage acceleration new gitaly node create add list shard configure gitlab application store new project repository create production change issue template
1082,33430820,4.0,create new gitaly storage shard node replace nfs gitaly storage shard nfs usage our usage target specify try maintain usage new project creation quickly cause usage nfs there currently gitaly shard nodes configure accept new project maintain level availability important avoid shard fill quickly to remove single node new project storage rotation cluster prevent usage acceleration new gitaly node create add list shard configure gitlab application store new project repository create production change issue template
1083,33420813,2.0,increase login session duration sentry sentry log minute feel bit excessive can increase acceptable
1084,33419602,2.0,review deadmans snitch infrastructure look deadman snitch infrastructure wonder work expect the dashboard reference runbook pretty vacant deadman snitch integrate slack pagerduty differ runbook most alert configure alertmanager receive datum snitchheartbeat appear fire should snitch alertmanager environment gprd alertmanager ping month op alertmanager active my big concern alertmanager stop send alert present slack message alert certainly pagerduty alert afaic be i miss expectation x if snitch get ping alert pagerduty minute x be alert redundant actually useful x be date cc
1085,33401295,5.0,log shard failure try search pubsub rail inf gprd the content tag structure text search necessary there currently lot failure query like type reason fail create query bool filter query successful login field type phrase operator or slop lenient true none true true boost query slop none boost range null null true true boost true boost index uuid kiwqjulqrg index pubsub rail inf cause type cause reason field expansion match field limit get this hour reasonable able search hour worth datum time can limit raise adjustment improve
1086,33308198,2.0,execute testing postgresql upgrade data volume similar production we execute test postgresql upgrade dataset staging database it require execute test datum set production evaluate time consume process please consider follow step execute consistent gcp snapshot read replica connect postgresql execute command select execute gcp snapshot in postgresql execute select we need attach snapshot follow host after ongres check execute setup environment test playbook execute postgresql upgrade
1087,33287406,5.0,incident practice support cmoc emea basic summary this mean simple problem solve table scenario first test incident response basic group host interaction practice environment location staging practice test env scenario service stop haproxy lb current status haproxy run door closed incident start eoc execute stop lb gstg start incident handle eoc use declare incident management skip check page verify test cmoc rotation set validate x creation incident gdoc x creation incident issue x cmoc imoc find incident issue once manager cmoc join imoc cmoc talk comment understand issue cmoc log talk create incident link test status page cmoc talk update resolution action eoc talk action load balancer restart manager talk escalate engineer verify incident resolve cmoc confirm resolution talk update follow action item create incident review issue how escalate action item infradev
1088,33261764,1.0,increase prometheus sample owe query need run directly prometheus run thanos result counter reset bug lead incorrect result however query prometheus directly frequently hit sample limit set m sample this mean run query prometheus thanos proposal increase sample default value m currently limit query roughly mb sample datum assume byte sample the prometheus default m item mb query our prometheus fleet use available memory like reasonable change thanos reset bug address cc
1089,33260605,3.0,poc redis cluster potential infra improvement scalability redis gdk connect redi cluster run minikube
1090,33138978,3.0,improve stackdriver exporter metric we currently collect lot stackdriver metric header header ci prd gprd gstg op pre testbe this lead number problem large storage need metric system slow scrape time timeout lost scrape lot stackdriver api traffic duplication metric datum stackdriver monitoring component for example pull entire metric subsystem this metric lot overlap datum propose todo x filter metric specific metric filter ie split scrape job granular instance
1091,33106785,2.0,export version database load warehouse runbook
1092,33100615,3.0,create new gitaly storage shard node replace nfs gitaly storage shard nfs currently usage a new gitaly node create add list shard configure include consideration store new project repository that way nfs remove rotation concern node removal configuration additional burden remain node it important avoid acceleration usage growth remain node accept new repository
1093,33098053,3.0,chef break postgre dr db gprd no file directory chef
1094,33088089,1.0,enable javascript source fetch sentry project customersgitlabcom customersstggitlabcom this issue enable setting source error trigger we need setting enable project
1095,32963394,1.0,add chef client enable script chef client disabler per discussion addition intercept call chef client disabled like able easily check disabled add chef client enabled script gitlab client disabl recipe expect behavior if chef client enable script exit if chef client disabled script exit non zero reason output chef client run intercept
1096,32693292,1.0,fix permission contain credential the file world readable contain postgre superuser credential shell msmiley ls opt gitlab patroni r gitlab psql gitlab psql mar opt gitlab patroni presumably patroni daemon need read file file owner unix account run patroni daemon shell msmiley pgrep patroni xargs ps uid user args uid user command patroni bin python patroni bin patroni opt gitlab patroni msmiley d psql psql psql therefore safe change file read write owner chmod
1097,32637793,3.0,chef break download cloudflare break chef client break gprd node download chef anymore captcha protect new
1098,32632884,3.0,cert expire unnoticed the cert expire noticing we get inform utc we replace cert new utc update chef vault we get expiry warning email sslmate question why notice what process
1099,32579198,3.0,mark certificate resource sensitive true mark certificate resource sensitive true
1100,29678381,2.0,setup secondary database flag patroni staging in staging add secondary database primary add flag receive traffic config patroni node tag nofailover true noloadbalance true
1101,29670681,8.0,get elastic cluster operation metric prometheus as elastic cloud provide way time series datum internal cluster metric current state thing like storage api limited possibility elastic monitoring cluster view operation production cluster satisfying consider setup like elastic cluster metric prometheus this alert easy standard compare create watch elastic dashboard grafana
1102,29611372,1.0,add txt record domain verification drift we set drift sso okta enable need perform domain verification can add follow txt record domain drift domain verification further information request available
1103,29544814,3.0,upgrade dr haproxy staging prod preprod haproxy ubuntu lts end lbs it nice dr unify configuration example hard stop option config this mean exception dr environment ideal
1104,29543252,5.0,research log volume rate in order calculate proper size cost elastic logging cluster need research current log volume rate we datum index environment decision index want exclude elastic
1105,29496485,2.0,thanos process successfully restart thano upload bucket storage this issue document finding form thanos crash loop action item add like follow note troubleshooting prometheus runbook reference alert detect frequent thano restart file bug report thanos community potentially work patch time allow problem statement thanos appear race condition thanos instance upload new block directory bucket storage thano instance restart reason instance fail startup upload complete this race condition thano bug automatic restart thanos service good mitigation bug fix special case if thano upload bucket abort uncleanly leave incomplete directory bucket cause crash loop continue indefinitely fix require manually delete incomplete block directory object storage bucket background part thanos job use object storage bucket primary place store metric datum use local filesystem persistent cache data recently notice thanos store process temporarily enter crash loop unhandled exception scan bucket new file example log excerpt block fail load meta download file storage object exist this log message indicate object storage bucket block directory name contain file in example miss file create minute later thanos compact finish upload datum block chunk subdirectory in general new block directory create object storage bucket dir file create step upload datum file index file block our current version thano late release treat absence file block directory error if thano start fatal error crash loop in contrast thano periodic refreshe catalog bucket content error treat non fatal warning it minute create new directory create file depend datum upload available network bandwidth this main window opportunity thano hit race condition enter crash loop potential solution thanos call syncblocks startup periodically only initialsync method treat absence file fatal in contrast periodic call syncblocks log warning stop process bucket remain dir if syncblocks skip dir exist file prevent initialsync fail cause thano process exit shortly startup avoid crash loop allow periodic syncblocks call scan viable bucket dir instead abort prematurely this result fresh datum reliably available slow compaction run
1106,29484939,2.0,followup remove prefix review app after remove transitional prefix review app after exist mr review app server src file storage rebase late master x cookbook change x chef repo cookbook bump x change
1107,29437383,2.0,update database diagram update roll change pgbouncer database diagram focus change pgbouncer rw
1108,29383410,2.0,create new file module node serve praefect this allow differentiate file storage access directly rail access praefect x gitlab com infrastructure change x chef repo change
1109,29285464,1.0,grant select access analytic user db i add newly create table data team etl i look somebody grant select access analytic user public table db for context mr i block mr time happen other relevant issue
1110,29277728,3.0,incident review spammers cause large mailer sidekiq queue please note incident relate sensitive datum security related consider label issue mark confidential incident relate rca summary we experience incident spam attack cause large mailer sidekiq queue affect team attribution minute downtime degradation we set sla mail queue purpose great minute latency job minute minute minute minute minute for calculate duration event use platform metrics dashboard look appdex slo violation impact metrics start following what impact incident service outage sub service brown exposure sensitive datum who impact incident external customer internal customer specific team how incident impact customer prevent x incorrect display y how attempt access impact service feature how customer affect how customer try access impact service feature include additional metric relevance provide relevant graph help understand impact incident dynamic detection response start following how incident detect do alarming work expect how long start incident detection how long detection remediation be issue response incident bastion host access service available relevant team memeber page able root cause analysis the purpose document understand reason cause incident create mechanism prevent recur future a root cause person way writing refer system context specific actor follow s blameless manner core root cause analysis for necessary start incident question happen keep iterate ask time while hard rule time help question deep find actual root cause keep min come answer consider follow different branch example usage s spam campaign why the battery dead why the alternator function why the alternator belt break why the alternator belt useful service life replace why the vehicle maintain accord recommend service schedule fifth root cause what go start follow identify thing work expect any additional out go particularly what improve we hard create new account spam campaign we strict limit issue note creation while rate limit good thing need careful collateral effect exist use case enable we well tool clean sidekiq queue make easy safe execute prevent overload redis start follow use root cause analysis explain improve prevent happen be improve detection time detection be improve response time response be exist issue prevent incident reduce impact do indication knowledge incident place corrective action list issue create corrective action incident for issue include follow bare issue link issue label action include estimate date completion corrective action incldue name individual own delivery corrective action hard create bogus account abusive operation rate limit issue creation easy identify kill bad sidekiq job identify add troubleshooting runbook deal issue spam disable mail send clean queue issue block spammer prevent overload redis master affect queue clean mailer queue easy stop send mail consider move mailer queue separate cluster test idea improve protection spam campaign discussion guideline blameless rca guideline s
1111,29240913,3.0,fix sidekiq log index mapping since pubsubbeat warning alert maybe relate deployment day we fix index mapping
1112,29022082,1.0,gitaly error rate exceed slo canary stage the alert m pagerduty event this alert self resolve minute dashboard show canary stage briefly spike slo time today
1113,28918188,2.0,add detailed monitoring raise alert case network traffic patroni fleet low average create alert network traffic reach average evaluate correct value detect metric this alert raise severity
1114,28866342,4.0,usage patroni investigate usage patroni config resilient short network glitch with patroni ensure use value ttl instead serfcheck we need proceed test staging
1115,28862059,1.0,gitter beta dns change gitter beta environment recently change we create new elb asg webapp server we like point dns a record i sure usually outside aws account additional note the beta webapp elb elb gitter aws account right dns record point directly instance gitter
1116,28838617,1.0,remove residual firewall rule definition apply reboot recently iptable drop rule unintentionally apply patroni host cause connectivity loss database server incident review the immediate recovery step include unassigning chef role gitlab iptable manually remove iptable rule affect host especially patroni host that leave residual unmanaged configuration host as incident follow corrective action residual config discover find probably dangerous the gitlab iptable cookbook default recipe use cookbook iptable ng define iptable rule that iptable ng cookbook install deb package iptable persistent implicitly netfilter persistent deb package job persist iptable rule reboot the iptable ng cookbook store version rule far i know chef client support cookbook so file harmless in contrast content iptable actively load reboot netfilter persistent systemd service for host unintentionally gitlab iptable cookbook apply cookbook remove reapplie apply clean step include following option a disable systemd unit option b remove file iptable rule option c remove deb package iptable persistent optionally netfilter persistent
1117,28838043,3.0,many pagesdomainsslrenewalworker sidekiq exception queue today i notice instance pagesdomainsslrenewalworker sidekiq exception lately here grafana chart past hour here log past day hit error day the error log tend following rate limit reach connection reset peer jws anti replay nonce directory include
1118,28833806,2.0,check replication check status stream replication need replica date receive traffic able primary cluster please evaluate need create alert node replicate properly please check replication slot node
1119,28823296,5.0,evaluate elastic apm distribute trace solution elastic apm opentracing bridge easy plugin solution send trace datum we evaluate use distribute tracing technical requirement miss feature feature opentracing support miss correlation log estimate datum volume cost factor
1120,28779954,1.0,cookbook license gitlab com need fix deploy cookbook customer gitlab com need review license cookbook incorporate customer change break ruby update
1121,28770014,5.0,help onboarding buddy as onboarde buddy i want process smooth possible x introductory coffee chat x make sure access onboarding issue x clarify new account need create x provide resource x relevant slack channel x schedule follow meeting week new year x make sure sre onboarding issue x help sre onboarde see meet person co working day improve onboarding issue handbook necessary
1122,28643682,3.0,remove deprecate digital ocean instance since move kubernetes cluster manage project auto devops configuration time remove previous digital ocean instance application this droplet gitlab production project digital ocean console once instance delete chef configuration remove chef server
1123,28643658,2.0,remove deprecate aws instance since move kubernetes cluster manage project auto devops configuration time remove previous aws instance application once instance delete chef configuration remove chef server the instance name
1124,28634530,2.0,setup log praefect gprd most work set log gstg create pubsub host x gitlab com infrastructure mr
1125,28499784,5.0,cloudflare fix haproxy acls check cloudflare header from the when configure like know haproxy check additional ip address associate request extra x forward for cloudflare we need fix acls src look cloudflare header src cloudflare ip
1126,28419151,2.0,investigate rca production rca deep dive from what happen a network glitch cause quorum loss trigger failover routine as wrong current leader leader change patroni behave expect trigger failover complete detect healthy evidence prometheus graph show network interruption at dec consul report miss contact current leader follow quick join dec db gprd info serf eventmemberfaile db gprd dec db gprd serf eventmemberfaile db gprd dec db gprd info serf eventmemberjoin db gprd dec db gprd serf eventmemberjoin db gprd at gmt network failure cause error walfile upload gmt user gitlab replicator db log disconnection session time user gitlab replicator info msg begin archive file detail upload gprd postgre backup pitr wale structured action push wal key gprd postgre backup pitr wale prefix pitr wale state begin traceback recent file wal e lib site package google cloud storage line client size file wal e lib site package google cloud storage line client stream size file wal e lib site package google cloud storage line response file wal e lib site package google request line file wal e lib site package google line callback file wal e lib site package google line invalidresponse request fail status code expect httpstatus ok during handling exception exception occur traceback recent file src gevent line file wal e lib site package worker line file wal e lib site package worker line k url tf file wal e lib site package blobstore gs line size size file wal e lib site package google cloud storage line file wal e lib site package google cloud storage line raise message response response googleapicallerror put request fail status code expect httpstatus ok greenlet waluploader object walsegment object fail googleapicallerror critical msg an unprocessed exception avoid error handle detail traceback recent file wal e lib site package google cloud storage line client size file wal e lib site package google cloud storage line client stream size file wal e lib site package google cloud storage line response file wal e lib site package google request line file wal e lib site package google line callback file wal e lib site package google line invalidresponse request fail status code expect httpstatus ok during handling exception exception occur traceback recent file wal e lib site package line main concurrency file wal e lib site package operator line file wal e lib site package worker pg line join raise val file src gevent line file wal e lib site package worker line file wal e lib site package worker line k url tf file wal e lib site package blobstore gs line size size file wal e lib site package google cloud storage line file wal e lib site package google cloud storage line raise message response response googleapicallerror put request fail status code expect httpstatus ok structured start leader election next replica peer wal position ahead downvote db gprd info wal position ahead wal position db gprd info wal position ahead wal position db gprd info wal position ahead wal position db gprd info wal position ahead wal position db gprd info wal position ahead wal position db gprd info wal position ahead wal position db gprd info wal position ahead wal position db gprd info wal position ahead wal position db gprd info wal position ahead wal position db gprd info wal position ahead wal position leave good candidate new ledear log db gprd info got response role replica true timeline patroni scope pg ha cluster version gmt xlog gmt pause false state run db gprd info get response run timeline role master replication state stream usename gitlab replicator async state stream usename gitlab replicator async state stream usename gitlab replicator async state stream usename gitlab replicator async state stream usename gitlab replicator async state stream usename gitlab replicator async state stream usename gitlab replicator async state stream usename gitlab replicator async state stream usename gitlab replicator db gprd async state stream usename gitlab replicator async state stream usename gitlab replicator async gmt patroni scope pg ha cluster version xlog location false db gprd info got response gmt true timeline xlog gmt pause false state run role replica patroni version scope pg ha cluster db gprd info got response version scope pg ha cluster state run role replica true timeline gmt xlog pause false gmt db gprd info get response run true xlog gmt pause false role replica gmt patroni scope pg ha cluster version timeline db gprd info got response timeline state run true xlog pause false gmt gmt role replica patroni scope pg ha cluster version db gprd info get response true timeline state run gmt patroni scope pg ha cluster version xlog gmt pause false role replica db gprd info get response replica true patroni scope pg ha cluster version gmt timeline state run xlog gmt pause false db gprd info got response version scope pg ha cluster state run timeline true role replica xlog gmt pause false gmt db gprd info get response scope pg ha cluster version gmt true timeline xlog gmt pause false role replica state run db gprd info get response replica state run gmt xlog gmt pause false timeline true patroni scope pg ha cluster version db gprd warning master alive db gprd info follow different leader healthy node db gprd info lock owner i db gprd info change restart progress so wrong online leader role preserve timeline cluster member host role state tl lag mb pg ha cluster run pg ha cluster run pg ha cluster run pg ha cluster run pg ha cluster run pg ha cluster leader run pg ha cluster run pg ha cluster run pg ha cluster run pg ha cluster run pg ha cluster run pg ha cluster run consecuences minute downtime replicas restarting corrective action investigate usage patroni config resilient ti short network glitch
1127,28331221,1.0,host discourse forum poc testing readiness review once poc environment provision accessible recently restore backup authentication configure need perform thorough validation this include work readiness review
1128,28318751,1.0,transparency api request ip project id as sre i need able quickly identify problem user project unicorn workhorse specifically respect api call
1129,28314510,8.0,learn elastic search gain knowledge elastic search v able setup support log search solution find learn material share finding evaluate possible training option certification read stuff try thing improve doc
1130,28310903,2.0,verification step postgre gprd fail relate the actual verification script fail return successfully example
1131,28242485,1.0,turn azure do blackbox prober we alert azure anymore try clean infra remove blackbox exporter server job azure do production
1132,28082432,1.0,a disk space leave alert get fire resolve immediately a disk space leave alert get fire i page pagerduty i look disk system appear normal nelsnelson df filesystem size use avail mount udev g g tmpfs g m g g g g tmpfs g k g shm tmpfs m m lock tmpfs g g fs cgroup sdb g m g log stor opt gitlab gitlab rail share artifact t t t opt gitlab gitlab rail share artifact stor opt gitlab gitlab ci build t t t opt gitlab gitlab ci build stor opt gitlab gitlab rail share lfs object t t t opt gitlab gitlab rail share lfs object stor opt gitlab gitlab rail share page t t t opt gitlab gitlab rail share page stor opt gitlab gitlab rail upload t t t opt gitlab gitlab rail upload tmpf g g i suppose disk capacity dip brief i run df check quickly
1133,28070585,1.0,fail connect port operation time it deadman switch host operational reachable normal port can help understand it hardcode gitlab restore destination final signal successful restoration look like available anymore fail connect port operation time so verification staging backup fail any recent change address change unavailable and
1134,28027466,1.0,weird wobbling web haproxy ratio notice happen today the onset wobbling correspond natural increase traffic europe americas come online
1135,27996584,5.0,upgrade terraform gcp provider the recent major version upgrade google cloud terraform provider release include change want need near future note impact manage gcp project note require fair review testing ensure issue module roll deployment environment this issue update accord upgrade guide split task require
1136,27995476,2.0,match exist chef server config gitlab chef server cookbook once base cookbook automate chef server complete need document replicate exist chef server configuration cookbook attribute validate chefspec inspec
1137,27995407,2.0,automate chef server install follow breakout terraform code place instance central bootstrap need improve our current chef server build manually little automation installation configuration chef server there chef server cookbook add new role base prior with configuration ability test independently deploy change role chef server issue track creation container build test deployment pipeline possible standard wrapper cookbook chef server correspond test kitchen chefspec inspec config validate
1138,27994920,2.0,investigate application start failure know read replica responsive summary in incident review meeting early today december corrective action item skip favor depth discussion we need investigate lose patroni node affect the meeting nearly i state follow meeting schedule discuss concern most individual assemble shortly separate d end discuss matter it evident quickly information need uncover fruitful conversation place definition done x outline technical condition failure occur x provide step reproduce behavior document scenario appropriate section runbook x determine priority level bring attention development team performance availability meeting label combination
1140,27986142,2.0,create metric catalog praefect to establish apdex error rate metric alerting graphana dashboard
1141,27981854,2.0,use custom instance type patroni node we currently use gb memory postgresql server instance this lead utilize cpu this currently waste gcp cpu quota not mention cost reserve core use we easily cut custom node type base bad case cpu utilization peak core use we safely cut allocate core cpu proposal switch allocation
1142,27947632,1.0,problem license db extraction we unable read info info connect server connection time info info server run host info info ip connection port full log read local file local airflow log license db info dependencies meet taskinstance db incremental queue info dependencies meet taskinstance db incremental queue info info starting attempt info info executing license db incremental info running airflow run license db incremental extract info job subtask license db incremental info use pool setting info job subtask license db incremental info use executor localexecutor info job subtask license db incremental info fill dagbag local airflow analytic dag extract info job subtask license db incremental info running taskinstance db incremental run host airflow deployment info info event license db incremental event type pende info info event license db incremental event type pende info info event license db incremental event type pende info info event license db incremental event type pende info info event license db incremental event type pende info info event license db incremental event type pende info info event license db incremental event type pende info info event license db incremental event type pende info info event license db incremental event type pende info info event license db incremental event type pende info info event license db incremental event type running info info info info root read manifest location info info root create database engine info info root info info root info info root process table info info start new https connection info info start new http connection info info recent info info b file local lib site package sqlalchemy engine line info info b return info info b file local lib site package sqlalchemy line info info b return info info b file local lib site package sqlalchemy line info info b fairy info info b file local lib site package sqlalchemy line info info b rec info info b file local lib site package sqlalchemy line info info b info info b file local lib site package sqlalchemy util line info info b info info b file local lib site package sqlalchemy util line info info b raise info info b file local lib site package sqlalchemy line info info b return info info b file local lib site package sqlalchemy line info info b return info info b file local lib site package sqlalchemy line info info b info info b file local lib site package sqlalchemy line info info b connection info info b file local lib site package sqlalchemy engine line info info b return info info b file local lib site package sqlalchemy engine line info info b return info info b file local lib site package line info info b conn info info connect server connection time info info server run host info info ip connection port info info info info info info exception direct cause follow info info info info recent info info b file line info info b info info b file local lib site package fire line info info b args context info info b file local lib site package fire line info info b component info info b file local lib site package fire line info info b result info info b file line info info b info info b file extract line info info b con info info b file local lib site package panda io line info info b chunksize info info b file local lib site package panda io line info info b result info info b file local lib site package panda io line info info b return info info b file local lib site package sqlalchemy engine line info info b connection info info b file local lib site package sqlalchemy engine line info info b info info b file local lib site package sqlalchemy engine line info info b e dialect info info b file local lib site package sqlalchemy engine line info info b info info b file local lib site package sqlalchemy util line info info b exception tb cause info info b file local lib site package sqlalchemy util line info info b raise info info b file local lib site package sqlalchemy engine line info info b return info info b file local lib site package sqlalchemy line info info b return info info b file local lib site package sqlalchemy line info info b fairy info info b file local lib site package sqlalchemy line info info b rec info info b file local lib site package sqlalchemy line info info b info info b file local lib site package sqlalchemy util line info info b info info b file local lib site package sqlalchemy util line info info b raise info info b file local lib site package sqlalchemy line info info b return info info b file local lib site package sqlalchemy line info info b return info info b file local lib site package sqlalchemy line info info b info info b file local lib site package sqlalchemy line info info b connection info info b file local lib site package sqlalchemy engine line info info b return info info b file local lib site package sqlalchemy engine line info info b return info info b file local lib site package line info info b conn info info operationalerror connect server connection time info info server run host info info ip connection port info info b background error info info find io module state interpreter info info recent info info b file local lib site package snowflake connector line info info b auth info info b file local lib site package botocore vendored request line info info b prep info info b file local lib site package botocore vendored request line info info b hook info info b file local lib site package botocore vendored request line info info b file info info b file local lib site package botocore vendored request line info info b length info info b file local lib site package botocore vendored request line info info b fileno info info find io module state interpreter info info info info handling exception exception info info info info recent info info b file local lib site package snowflake connector line info info b info info b file local lib site package snowflake connector line info info b raise info info retryrequest find io module state interpreter info info event license db incremental event type fail info info event job d license db incremental fail info info event license db incremental event type fail info info event job d license db incremental fail error pod launching fail pod return failure fail traceback recent file local lib site package airflow contrib operator line execute pod return failure airflowexception pod return failure fail during handling exception exception occur traceback recent file local lib site package airflow line result context file local lib site package airflow contrib operator line execute raise launching fail ex airflowexception pod launching fail pod return failure fail info all retrie fail mark task fail info job subtask license db incremental traceback recent info job subtask license db incremental file local lib site package airflow contrib operator line execute info job subtask license db incremental pod return failure info job subtask license db incremental airflowexception pod return failure fail info job subtask license db incremental info job subtask license db incremental during handling exception exception occur info job subtask license db incremental info job subtask license db incremental traceback recent info job subtask license db incremental file local bin airflow line module info job subtask license db incremental info job subtask license db incremental file local lib site package airflow util line wrapper info job subtask license db incremental return kwargs info job subtask license db incremental file local lib site package airflow bin line run info job subtask license db incremental dag ti info job subtask license db incremental file local lib site package airflow bin line run info job subtask license db incremental pool info job subtask license db incremental file local lib site package airflow util line wrapper info job subtask license db incremental return kwargs info job subtask license db incremental file local lib site package airflow line info job subtask license db incremental result context info job subtask license db incremental file local lib site package airflow contrib operator line execute info job subtask license db incremental raise launching fail ex info job subtask license db incremental airflowexception pod launching fail pod return failure fail info info task exit return code
1143,27939523,4.0,gitlab restore implement cleanup procedure avoid quota event it common case gitlab restore project reach quota instance stall if restoration error occur case lead hard failure auto cleanup we need job clean instance periodically base certain mask instance rule like if backup verification start day ago finish instance protect deletion time consider backup verification fail send signal destroy instance to discuss organize cronjob ci cd pipeline special task anyhow
1144,27914122,1.0,adjust praefect storage node config gstg new format after change need adjust gstg configuration
1145,27912638,1.0,setup sentry praefect gstg as readiness review praefect let setup sentry reporting initially gstg later replicate gprd
1146,27906899,1.0,the view alertmanager link pagerduty incident instead i expect but instead example view gitlab alertmanager link go this i expect
1147,27904748,2.0,document troubleshooting restore backup failure please document step troubleshoot database backup restore we like involve sre troubleshoot please write runbook explain step execute usually fix alert
1148,27892387,2.0,node incomplete chef run there lot node miss chef client metric datum gprd gstg etc some chef client disable some complete client run
1149,27752364,4.0,replace datasource data analytic team we like propose change datasource data analytic team datawarehouse actual mechanism restore database apply wal optimise we like provide extra replica follow database stay update stream replication license version customer
1150,27752138,8.0,onboard datawarehouse database it need onboard datawarehouse database license version customer for need execute follow step check hardware available database recommend upgrade need check number replicas create replica primary exist create dashboard add metric database suggestion create overview dashboard specific verify backup create restore project add deadmansnith verify alert place
1151,27701789,3.0,grant access ongre the ongre team access chef repo we like check possibility grant access ongres saml could configure multiple omniauth provider
1152,27561703,1.0,improve handover label look feedback follow board label the premise incident label on incident template automatically board any work triage on call work tag way the eoc triage item remove on label region at end shift item hand region eoc see fit this work well on label ongoing work ongoing work need hand that mean label action on we consider scope tag ensure it well use tag example possibly action open discussion comment opinion
1153,27479514,2.0,monitor autovacuum queue postgres primary the query monitor autovacuum queue list table need process autovacuum process size list goal able table need processing wait use metric similar load average alert n worker table wait process time trigger alert here need know size queue think easy add prometheus case troubleshooting need great detail log i usually achieve cronjob plpgsql form query run n minute log detail what good option case infrastructure i think split achieving goal have item important however have item extremely helpful troubleshooting autovacuum behavior run query make sense postgres master so wrap check run replicas cc
1154,27423355,8.0,test postgres database restoration gcp snapshot staging i go test gcp snapshot restoration postgres database manually staging instance prefix manually create i find way use snapshot stage restore instance gitlab restore additionally open question need create snapshot cc
1155,27419624,3.0,add alert monitoring while work step migrate op i notice op type monitoring alert exception blackbox rule check sign endpoint run gprd prometheus we enable node gitlab exporter add alert
1156,27398783,2.0,terraform env gprd possibly plan ci see example local planning work appear service account permission issue error message say api question appear enable
1157,27342916,1.0,miss permission gitlab op terraform service account while apply change gitlab com apply job return permission error fail apply update op gke cluster need add cluster admin privilege environment utilize gke
1158,27271403,3.0,cloudflare prototype log shipping as discuss create prototype cloudflare logpull push filebeat bonus point metric extraction
1159,27176545,1.0,staging add extra pgbouncer node primary database rebalance connection pool add extra pgbouncer node pool primary database staging cluster setup application use node web api node sideqik
1160,27152618,3.0,rca daily utc latency spike please note incident relate sensitive datum security related consider label issue mark confidential incident summary we daily spike latency morning utc this issue serve collect evidence investigate mitigate root cause update this duplicate affect web team attribution minutes downtime degradation for calculate duration event use platform metrics dashboard look appdex slo violation impact metrics start following what impact incident service outage sub service brown exposure sensitive datum who impact incident external customer internal customer specific team how incident impact customer prevent x incorrect display y how attempt access impact service feature how customer affect how customer try access impact service feature include additional metric relevance provide relevant graph help understand impact incident dynamic detection response start following how incident detect do alarming work expect how long start incident detection how long detection remediation be issue response incident bastion host access service available relevant team memeber page able root cause analysis the purpose document understand reason cause incident create mechanism prevent recur future a root cause person way writing refer system context specific actor follow s blameless manner core root cause analysis for necessary start incident question happen keep iterate ask time while hard rule time help question deep find actual root cause keep min come answer consider follow different branch example usage s the vehicle start problem why the battery dead why the alternator function why the alternator belt break why the alternator belt useful service life replace why the vehicle maintain accord recommend service schedule fifth root cause what go start follow identify thing work expect any additional out go particularly what improve start follow use root cause analysis explain improve prevent happen be improve detection time detection be improve response time response be exist issue prevent incident reduce impact do indication knowledge incident place corrective action list issue create corrective action incident for issue include follow bare issue link issue label action include estimate date completion corrective action incldue name individual own delivery corrective action guideline blameless rca guideline s
1161,27173904,2.0,create change separate pool pgbouncer production read write as plan execute read database split traffic pool pgbouncer level sideqik web please test staging create change request apply production we need extra node organize change
1162,27015529,1.0,deploy thanos sidecar time flag after roll thanos time this limit sidecar lookback prometheus thanos query frontend this allow reduce load prometheus maintain long history prometheus
1163,27013533,1.0,rca latency apdex score degradation pgbouncer saturation please note incident relate sensitive datum security related consider label issue mark confidential incident gitlab com gl infra summary a failover pgbouncer node lead imbalance connection distribution active pgbouncer instance turn lead saturate pgbouncer connection cause apdex degradation affect web team attribution minutes downtime degradation for calculate duration event use platform metrics dashboard look appdex slo violation impact metrics start following what impact incident service outage sub service brown exposure sensitive datum who impact incident external customer internal customer specific team how incident impact customer prevent x incorrect display y how attempt access impact service feature how customer affect how customer try access impact service feature include additional metric relevance provide relevant graph help understand impact incident dynamic detection response start following how incident detect do alarming work expect how long start incident detection how long detection remediation be issue response incident bastion host access service available relevant team memeber page able root cause analysis the purpose document understand reason cause incident create mechanism prevent recur future a root cause person way writing refer system context specific actor follow s blameless manner core root cause analysis for necessary start incident question happen keep iterate ask time while hard rule time help question deep find actual root cause keep min come answer consider follow different branch example usage s the vehicle start problem why the battery dead why the alternator function why the alternator belt break why the alternator belt useful service life replace why the vehicle maintain accord recommend service schedule fifth root cause what go start follow identify thing work expect any additional out go particularly what improve start follow use root cause analysis explain improve prevent happen be improve detection time detection be improve response time response be exist issue prevent incident reduce impact do indication knowledge incident place corrective action list issue create corrective action incident for issue include follow bare issue link issue label action include estimate date completion corrective action incldue name individual own delivery corrective action guideline blameless rca guideline s
1164,27002656,2.0,large number fail blackbox probe currently blackbox probe fail continuously x update fix host header redirect x fix break blackbox probe x add alert blackbox probe fail
1165,26986385,2.0,cleanup stage tier type label there lot prometheus monitor target miss stage tier type label these necessary correct identification generic service alert rule correct routing alertmanager
1166,26981123,1.0,fix sidekiq error ratio metric the sidekiq error ration panel grafana show error ratio high true cause alert
1167,26974433,3.0,sync folder public grafana dashboard the current grafana sync script support update folder this cause dashboard squash flat namespace
1168,26958032,2.0,handle dns entry host zone terraform this come play regardless
1169,26947132,1.0,rca gcp service disruption please note incident relate sensitive datum security related consider label issue mark confidential incident summary a brief summary happen try executive friendly possible affect team attribution minute downtime degradation for calculate duration event use platform metrics dashboard look appdex slo violation impact metrics start following what impact incident service outage sub service brown exposure sensitive datum who impact incident external customer internal customer specific team how incident impact customer prevent x incorrect display y how attempt access impact service feature how customer affect how customer try access impact service feature include additional metric relevance provide relevant graph help understand impact incident dynamic detection response start following how incident detect do alarming work expect how long start incident detection how long detection remediation be issue response incident bastion host access service available relevant team memeber page able root cause analysis the purpose document understand reason cause incident create mechanism prevent recur future a root cause person way writing refer system context specific actor follow s blameless manner core root cause analysis for necessary start incident question happen keep iterate ask time while hard rule time help question deep find actual root cause keep min come answer consider follow different branch example usage s the vehicle start problem why the battery dead why the alternator function why the alternator belt break why the alternator belt useful service life replace why the vehicle maintain accord recommend service schedule fifth root cause what go start follow identify thing work expect any additional out go particularly what improve start follow use root cause analysis explain improve prevent happen be improve detection time detection be improve response time response be exist issue prevent incident reduce impact do indication knowledge incident place corrective action list issue create corrective action incident for issue include follow bare issue link issue label action include estimate date completion corrective action incldue name individual own delivery corrective action guideline blameless rca guideline s
1170,26909720,1.0,corrupt wal gke prometheus one prometheus server gitlab production gke cluster corrupt wal reload block head truncate fail create checkpoint read segment corruption segment unexpected record afaik correct way fix kick prometheus repair
1171,26909449,1.0,prometheus inf gprd fail upload object store fire thanos compaction run hour thanos compact upload block hour
1172,26908797,1.0,inf prometheus not sure long currently gb expand gb
1173,26833795,1.0,fix path ppa gitlab sentry cookbook the gitlab sentry cookbook add ppa redi point old trusty path this lead add apt repository run chef run time add additional comment deb src line lead multi thousand line file chef client timeout run busy time add apt repository rewrite file repeatedly reason i care investigate
1174,26824818,1.0,provision aws iam account customer cross account role access for request gitlab com access need provision account use assume customer own cross account iam role permission provision eks cluster account at start i think aws account terraform configuration gitlab com gitlab com infrastructure likely place resource adjust necessary later date the account assign iam policy allow sts assumerole condition prevent user assume role account if need enable eks provisioning need stipulate host current production account policy adjustment ensure provision account assume role grant unwanted elevated permission as example negate access role production account like following need change accordingly see additional context actually desirable case json version statement effect allow action sts assumerole notresource arn aw far require external id cross account role access able limit account assume role external d provide like null condition sts externalid condition key ensure permission grant condition key present json version statement effect allow action sts assumerole notresource arn aw
1175,26820785,1.0,add redirect rule problem crawlers report duplicate content page end url example solution add redirect rule url end concern know template require url end tag prioritization
1176,26784630,1.0,tweak alert disk use log patroni server alert soon the current alert log fill come space remain when log fill negative impact wal e shipping node performance we alert little aggressively log disk fill perhaps remain space
1177,26776728,1.0,update runbook provide good documentation associate sidekiq job project user update runbook describe find sidekiq queue job project user review sidekiq queue look problematic project user fyi
1178,26756847,5.0,make cloudflare dns terraformable since keep seperate zone cloudflare cloudflare configuration purpose go split right this mean extend dns environment terraform allow manage zone cloudflare provider impact production this resolve diversion production underlying dns different difference manage we able structure configure json file
1179,26739422,1.0,rail console auto devops deployments currently rail console run vm code run deploy node in non core application run auto devops way rail console moment when connect directly application pod result kubectl exec bash root cd bin root console traceback recent main config require lib rubygem require lib rubygem require load file bundler setup loaderror root rail console also kubectl exec bash root bundle exec rail c bash bundle command find root cd root exec rail c traceback recent main lib lib could find bundler require to update last version instal system run bundle update to install miss version run gem install if simply deploy vm manually set mechanism auto devops deploy code update vm if deploy pod access cluster kubectl need run kubectl exec rail console neither ideal let discuss option
1181,26670540,2.0,deploy prototype dynamic image rescaling cloudflare worker staging context something like work javascript event async function let url new let fetchparam cf image fit scale let width width return width when need x header anymore return fetchparam let response await fetchparam response new response cloudflare worker return response accord these require route user avatar url project avatar url group avatar url pattern good
1182,26670291,1.0,update runbook documentation use manage command as sre i need know create install trigger ansible base command help manage action large number node the runbook update include section accomplish task this original issue detail
1183,26670201,3.0,make cloudflare worker terraformable in order deploy prototype need way deploy cloudflare worker scrip route vie terraform this issue goal way deploy worker script worker route
1184,26669985,3.0,use metric staging production the bad citizen run puma it effectively block worker process time require render metric increase latency processing time fleet substantially we run request minute mean time anything escape native interruptible default ruby gvl effectively block request process give period time look actual implementation actually inefficient we list file escape native present we implement separate endpoint fully test i assume prefer switch we validate switch staging production use endpoint
1185,26572236,1.0,environment new op environment when provision new chef server infrastructure receive follow error bootstrap sv op central startup script info startup script can load environment ops m overload variable term at glance i inclined set value op environment op central think intention lift shift current op environment new region destroy old infrastructure update terraform config directory state file op let new primary however currently area op central hard code problematic prevent approach notably gcp network subsequent reference downstream dependent resource in order facilitate migration i anticipate limit option name use effectively run environment parallel op central likely good choice term length region specificity start i sure encounter issue continue op aborted migration receive pushback somewhat flippant since restart effort op away region i suspect time figure reasonably easily destroy recreate infrastructure impact running service if use entirely new mean duplication chef config impact shall new op environment address name issue
1186,26542980,5.0,add aw govcloud account okta this issue primarily placeholder track effort milestone planning infrastructure issue board all note update follow log issue directly
1187,26542132,1.0,thanos compact disk fill the prometheus disk this similar issue the alert clear follow runbook the follow command execute df sudo systemctl stop thano compact sudo systemctl start thano compact df
1188,26534975,2.0,validate scrape praefect gstg staging praefect enable i find metric prometheus one metric i look specifically that create regardless input give miss i think scrape target expose metric target
1189,26511361,3.0,intermittent http failure ingress controller we see intermittent http failure try connect ingress controller the instance site issue that resolve upgrade cluster provision new ingress deploy app we try provision new ingress resolve the second instance suspicious failure app we get pingdom check failure return intermittent timeout these relate issue place capture information explore possibility ping connect byte ms connect byte ms connect byte ms connect byte ms connect byte ms connect byte ms timeout receive reply header host timeout receive reply header host connect byte ms connect byte ms connect byte ms connect byte ms connect byte ms connect byte ms ping statistic connect ok fail time round trip min avg max ms
1190,26503320,3.0,documentation new chef infra server break infrastructure provision conjunction complete need document change infrastructure conduct readiness review proceed production change migration this issue include discussion center documentation lead production readiness review
1191,26503275,2.0,implement update monitoring chef server gcp break infrastructure provision conjunction complete need implement monitoring chef infra server application definition check port hook node exporter basic jsonnet def dashboard
1192,26482037,3.0,update cookbook gitlab runner use new docker package cookbook gitlab runner obsolete docker engine package repo this update use new docker ce package repo cc
1193,26390744,1.0,quality test fail praefect staging see we see error message praefect allow
1194,26338837,3.0,add runbook i find runbook instruction diagnose repair see
1195,26338071,3.0,blackbox exporter close connection blackbox exporter close connection target respond that lead exhaustion file descriptor current limit fd also alert target see incident
1196,26329956,3.0,cloudflare accept forward original client ip with current configuration ip cloudflare report application ip session log
1197,26310715,2.0,audit kubernete workload ensure scrape scrape split
1198,26283490,3.0,rca elevated ci job queue duration please note incident relate sensitive datum security related consider label issue mark confidential incident gitlab com gl infra summary due project import bug job option attribute wrong data type lead failure assign job queue endless loop the fair usage job scheduling algorithm prefer job belong short pipeline job get chance run share runner this increase overall queue time number pende job rise affect ci runners team attribution minute downtime degradation utc utc m for calculate duration event use platform metrics dashboard look appdex slo violation impact metrics start following what impact incident job pende long time who impact incident user run job how incident impact customer customer need wait long time job schedule how attempt access impact service feature how customer affect how customer try access impact service feature job duration percentile job duration percentile provide relevant graph help understand impact incident dynamic detection response start following how incident detect report customer support user see pende job do alarming work expect get alert slo apdex ci runner latency define alert percentile hour percentile severely affect how long start incident detection m support report customer issue how long detection remediation m be issue response incident bastion host access service available relevant team member page able it take long time identify root cause understand impact root cause analysis jobs get stick pende state why they low chance get assign share runner why the shared runner occupy job corrupt option retry indefinitely why the attribute string instead hash why the job come import project importer bug work why what go customer support escalate infra team dev infra work debug tricky issue what improve alert pende job rise job queue well understanding job scheduling impact elevated job queue time start follow use root cause analysis explain improve prevent happen be improve detection time detection be improve response time response be exist issue prevent incident reduce impact do indication knowledge incident place corrective action list issue create corrective action incident for issue include follow bare issue link issue label action include estimate date completion corrective action incldue name individual own delivery corrective action x prevent corrupt job option x prevent job reschedule indefinitely improve alert elevated job queue time make easy identify job pick project runner guideline blameless rca guideline s
1199,26256776,1.0,mirror move infra vault op pende outcome need mirror entirely mirror
1200,75706089,8.0,facilitate failover test staging use geo with learnings test apply as deliverable disaster recovery working group like conduct single node test geo enable failover this test plan failover staging scheduled time fail geo base secondary test work secondary fail original infrastructure what need happen facilitate apply change learning single node test build change management issue track tactical piece failover coordinate consumer staging help define testing schedule need place event stage extended period time
1201,75565488,1.0,modify fastly header accommodate origin content frame this template gitlab team members seek support sre existing template available please fill detail detail point contact request if need propose date time n additional detail format type n sre support need please modify fastly header allow object tag origin content allow clickable svgs remove x frame options header add csp frame ancestor directive whitelist this proposal discuss edit
1202,75498455,2.0,instal python fail the reason fail decompress datum zlib available i install zlib i root access bastion system bash nelsnelson asdf install python download python build cloning plugin python pyenv remote enumerating object remote total delta reuse delta pack reuse receiving object mib byte s resolve delta check connectivity python build install download instal build failed ubuntu python build inspect clean work tree python result log python last log line true file python lib line code file python lib line return error file python lib line zipimport zipimporterror decompress datum zlib available recipe target install fail install error manually run install yield similar error bash traceback recent file nelsnelson tmp lib line main file nelsnelson tmp lib line file nelsnelson tmp lib line module file nelsnelson tmp lib line main file nelsnelson tmp lib line bootstrap return p projects file nelsnelson tmp lib line true file nelsnelson tmp lib line code file nelsnelson tmp lib line return error file nelsnelson tmp lib line zipimport zipimporterror decompress datum zlib available recipe target install fail install error
1203,75423510,5.0,update alert runbook wal make sure alert wal g wal archiving backup work intend alert point date runbook troubleshooting
1205,75059477,3.0,disable wal e gprd after run wal g successfully parallel wal e gprd gstg fully switch wal g disable wal e gprd make sure switch wal e backup consumer gprd x switch wal e backup consumer wal g x add alert silence wal e x switch primary use wal g disable wal g wal push secondary merge mr disable wal e check primary switch need postgre restart disable cronjob node x disable wal e backup push cronjob node cleanup chef role update runbook for execution copy cr issue gstg work acceptance criterion x backup wal file consumer configure read wal g archive x wal e disabled gprd
1206,74934845,5.0,switch gprd db replicas restore wal g archive we work test daily db backup wal g gprd gstg wal e completely disabled to able disable wal e gprd need find current consumer wal e archive switch use wal g archive for x archive replica x delay replica x database lab joe x active gitlab restore instance x wal g archive default path new restore instance x acceptance criterion x all backup consumer use wal g archive default
1207,74835216,3.0,disable wal e gstg after run wal g successfully parallel wal e gstg gprd disable wal e gstg make sure disable wal e backup consumer gstg x gstg disable wal e backup consumer x gstg add alert silence wal e x gstg switch primary use wal g disable wal g wal push secondary x merge mr x check primary x switch need postgre restart x disable cronjob node x disable wal e backup push cronjob node
1208,74767597,2.0,investigate failed build gitlab note much initial discussion link fail job contain todo any relevant necessary info move issue close duplicate report couple fail job clone fetch object repository i pretty sure i find error job job fail system failure error response daemon error set label mount source lib docker volume runner bad message link document its duration i presume nanosecond translate minute roughly correlate error message job error job fail execution take long second there relationship couple go issue track separately this issue block issue gather correct metric
1209,74358395,2.0,help redirect meta issue tracking
1210,74279059,3.0,identify deliverable beta launch macos shared runners the macos build cloud runner close beta currently progress the goal milestone wrap work autoscaler transition solution open beta the target open beta launch milestone march in addition autoscale development learn windows shared runners rollout significant work require infrastructure team pre requisite open beta launch this issue start discussion planning need infrastructure component place
1211,74253429,5.0,switch gstg db replicas restore wal g archive we work test daily db backup wal g gstg to able disable wal e gstg need find current consumer wal e archive switch use wal g archive for x archive replica x delay replica x geo secondary db x database lab joe x active gitlab restore instance
1212,74171455,4.0,update cirepom leverage new api update this issue close appear viable option give ostensibly short term impending application base dog foode solution the i look modify cirepom project i concerned status quo align state art gitlab project repository storage api i think i need develop well understanding code base operate integrate gitlab currently appear cirepom initiate repository migration manner ruby def target exec d end source and subsequently monitor track state quarantine project migration exhibit error this problematic reason the gitlab application track migration state expose api monitor state give migration the gitlab application project repository storage api significantly reduce error rate migration increase stability point quarantine feature necessary understanding design prevent repeat attempt migrate repository migration fail new api it appear defacto gitlab client ruby gem support api as result but maybe naive query it unclear firestore datastore feature surround migration state management quarantining use update
1213,74091608,3.0,chef recipe instal custom wal g binary as release wal g version need chef recipe install custom binary
1214,74049552,1.0,renew renew domain year we complete acquisition fuzzit past year transfer domain gitlab the website deprecate want add year registration prevent register domain benefit brand equity please add year domain
1215,73810238,2.0,enable new patroni gitlab pgchecksum chef recipe run patroni node enable new patroni gitlab pgchecksum chef recipe instal gitlab pgchecksum package patroni node
1216,73762705,3.0,include statement postgresql elastic log well debugging currently analyse slow query need log db host search local log redact query statement send log elastic security reason instead consider include statement log redact value this allow aggregate class query easy debug db performance issue slow query action
1217,73762129,3.0,alert small set query dominate postgresql normally db workload spread different query if postgre workload dominate slow query miss index statistic lead bad query plan severe impact overall db performance downtime see incident hard detect symptomatic alert alert directly point slow query we alert small set query dominate total query time suggest action
1218,73759743,1.0,upgrade thano inf op a simple apt upgrade box follow restart see
1219,73750484,5.0,define slo alert gcs storage in case increase gcs latency error rate alert symptom difficult find real cause we define slo alert gcs request action
1220,73746129,4.0,research converting runbook database credential rotation ansible play figure possible convert credential rotation runbook db op play
1222,73589795,5.0,create terraform repo transient import project we request vms customer project import ex as manually provision infra hand support professional service engineer perform import i think efficient terraform repo allow engineer self service creation server i propose use gcp module dev resource possible this module write support create vms similar way interview testing purpose the main thing need modify remove chef provision add firewall rule module allow support engineer access ssh since terraform long infra support efficient request
1223,73533796,1.0,install debian package patroni node to support postgre checksum enablement install debian package patroni node gitlab aptly package host package it upload aptly i attach case gitlab pg gitlab pg the goal issue implement package aptly roll chef database node
1224,73257732,2.0,help create temporary access gcp bucket package team work registry we open help team work registry well understand current development work from ar justification access hayley i maintainer gitlab container registry as upcoming registry upgrade need create inventory repository exist current registry bucket we need examine upgrade for reason need able list scan bucket gcs api this require service account read access viewer role i believe have access useful debug customer issue general decide want service account view role developer temporarily
1225,73108670,5.0,readiness review jaeger this checklist source link jaeger runbook cover majority point list the scope review deploy jaeger production environment elasticsearch instance deploy elastic cloud storage backend jaeger service labkit instrumentation summary x short overview mention purpose service dependency owner x explain scope review explicitly scope architecture x runbook contain architecture overview provide link x runbook contain logical architecture diagram x runbook contain physical architecture diagram optional x runbook provide information reviewer understanding service component dependency interaction documentation x blue print design doc provide link x runbook provide link x runbook date x documentation service locate service catalog entry provide link x service catalog list dependency service catalog link exist documentation service catalog link readiness review performance x runbook section performance characteristic cover follow consideration provide link x current request s min max average latency characteristic saturation x throtteling limit x bottleneck cpu bind memory bind x documentation set certain config option affect performance scalability x runbook section scalability information cover follow consideration provide link x expect load future x scale expected load x scale availability zone region x scalability limitation x performance test availability x runbook section cover availability consideration cover follow topic provide link x failure mode service blast radius long recover x happen outage service depend x availability zone az outage x split brain azs x region outage x external dependency affect availability x service affect outage service x exist recovery time objective rto document how plan achieve x error budget x disaster recovery test x failover procedure do runbook instruction durability x runbook section cover durability consideration cover follow topic provide link x possible failure mode recover x deletion accident x disk failure x datum corruption x gcp outage x exist recovery point objective rpo document how plan achieve backups x test backup replay x monitor backup x backup retention policy x backup different logical physical environment security compliance runbook section cover security consideration cover follow topic provide link x list access role x who role x how protect access x auditability access x which entrypoint need protection x how apply security update os service regulations policy apply pii sox x protect customer datum x encryption rest x customer datum leak log x long log security include readiness review monitoring x runbook section cover monitoring cover follow topic provide link x list key sli be monitor x list slo be monitor alert x list relevant alert x alert actionable link runbook x metric catalog entry service provide link x list relevant dashboard x list relevant log
1226,72672050,1.0,estimate quickly tb disk allocate new project repository gitaly shard fill new project estimate quickly tb disk allocate new project repository gitaly shard fill new project this slightly tricky constant number shard participate round robin arrangement
1227,72668091,1.0,move sidekiq shard detail dashboard use recording rule the shard detail dashboard slow load wo produce datum specific time frame we query dashboard record rule pre compute query
1228,72590774,1.0,create new gitaly storage shard node replace nfs gitaly storage shard nfs usage our usage target specify try maintain usage new project creation quickly cause usage nfs there currently gitaly shard nodes configure accept new project nfs maintain shard sufficient capacity new user repository creation target level availability prefer important help avoid scenario shard fill quickly to remove single node new project storage rotation cluster prevent acceleration capacity consumption new gitaly shard node create add list shard configure gitlab application store new project repository create production change issue template
1229,72269394,2.0,dns please review ip this template gitlab team members seek support sre existing template available please fill detail detail point contact request if need propose date time date time additional detail format type additional detail sre support need we work party security scanner bit sight they identify follow ip associate gitlab dns record we find give dns record update my ask review list if ip long indicate google doc then validate dns record properly remove update if indicate production pre prodcution user managed i ip remove bit sight portal please note time i say necessarily vulnerable wrong ip however need validate ip remove misattributed ip negatively impact score if easy sync let know edit
1230,72259382,1.0,automate workload sidekiq configuration update instead create mr workload gitlab com project yaml file revert mr order execute noop change trigger pipeline run click button pipeline web page switch programmatic method trigger pipeline build include execution pipeline stage change merge main branch chef repo project
1231,72241340,8.0,container registry define postgresql setup prod we like consider issue production setup container registry database we chat issue i like summarize actual idea setup postgresql production hardware spec n gb memory architecture use production consul cluster entry patroni cluster we node initially cluster primary read write traffic secondary read traffic secondary receive traffic snapshot backup have pgbouncer node primary database node read write traffic have pgbouncer node read server let enable datum checksum database initial setup postgresql setup
1232,72063955,2.0,work delivery team initial container registry new architecture rollout process staging work delivery team change need deployment tooling work team check connectivity database monitor execute database migration
1233,71893025,1.0,create new gitaly storage shard node replace nfs gitaly storage shard nfs usage our usage target specify try maintain usage new project creation quickly cause usage nfs there currently gitaly shard nodes configure accept new project nfs maintain shard sufficient capacity new user repository creation target level availability prefer important help avoid scenario shard fill quickly note i remove nfs configuration receive new repository past threshold remove to remove single node new project storage rotation cluster prevent acceleration capacity consumption new gitaly shard node create add list shard configure gitlab application store new project repository create production change issue template
1234,71885097,1.0,silence alert the queue main stage queue latency outside slo silence alert the queue main stage queue latency outside slo duration day request silence feature category
1235,71712100,3.0,add zone check firewall rule cloudflare audit log problem the cloudflare interface easy quick overview current rule apply zone compare consistency there variation configuration require production necessary staging proposal cloudflare firewall rule describe issue gitlab com gl infra cloudflare firewall label zone applicable check cloudflare audit log the current propose label markdown zone gitlab com zone staging gitlab com zone gitlab net potential tests x add necessary label exist open ticket update cloudflare audit check rule zone update cloudflare audit check matching rule zone identical for rule exist multiple zone verify order cc feedback discussion
1236,71707213,3.0,create new gitaly storage shard node replace nfs gitaly storage shard nfs usage our usage target specify try maintain usage new project creation quickly cause usage nfs there currently gitaly shard nodes configure accept new project nfs maintain shard sufficient capacity new user repository creation target level availability prefer important help avoid scenario shard fill quickly note i remove nfs configuration receive new repository past threshold remove to remove single node new project storage rotation cluster prevent acceleration capacity consumption new gitaly shard node create add list shard configure gitlab application store new project repository create production change issue template
1237,71575839,8.0,container registry set backup delay replication snapshot production we define registry database cluster staging backup pipeline restore pipeline delay replica disk snapshot replicas we consider have replica traffic activity
1239,71575296,2.0,work delivery team initial rollout process work delivery team change need deployment tooling work team check connectivity monitor execute database migration
1240,71574618,8.0,create new monitoring container registry db production we review monitoring set generic method deploy monitor cluster impact cluster the metric alert customize cluster business requirement different also review tool script ansible playbook test refactor need different cluster
1241,71456812,8.0,connect smartle brand digital design team repository this template gitlab team members seek support sre existing template available please fill detail detail point contact request if need propose date time date time additional detail format type additional detail documentation verify follow system prerequisite instal repo user create repo connector the application host server continuously available publicly addressable java version high disk space requirement it space clone git repository mb installation repository connector link relevant repos public example resource file gettext file json file etc help center article repo connector sre support need dsmith note need create smartling gcp project marketing folder setup vm new project ubuntu setup auto patcthing gb disk start do basic hardening set way maintain ssh key marketing team access config file install connector doc open question set simple ansible repo hardening key management okta install relate mr
1242,71319088,2.0,investigate turn cloudflare e mail obfuscation performance reason current js load automatically cloudflare responsible feature call e mail obfuscation protect e mail adresse scrape it deliberately turn default would great investigate turn feature gain performance js inject page cloudflare automatically example page load cloudflare info
1243,71178779,2.0,create test plan container registry team test new cluster q a devs staging work progress parent epic
1244,71177367,8.0,new readiness review container registry new pg db cluster the new infra readiness review process template create validate we need follow process template operational readiness production readiness critical point fully cover readiness review complete db runbooks taking account new db cluster update monitoring a robust backup setting this issue work progress
1245,71163919,3.0,cloudflare waf rules graphql this template gitlab team members seek support sre existing template available please fill detail detail point contact request if need propose date time date time additional detail format type additional detail sre support need support request details the follow path exclude waf rule post graphql exact match post path contain post path end base recent traffic analysis exception i think add enable waf block mode detail edit
1246,71022724,3.0,gitlab restore backup restore fail excessive duration reduce backup size enable wal compression we deadman snitch alert miss backup restore
1248,70924882,8.0,review runbook database ecosystem attend different database need review database relate runbook attend possibility different database some point review hardcode value different database also consider certain level abstraction cluster future
1249,70885114,8.0,provision new database cluster production container register service we need provision postgresql cluster production staging test complete this issue address change design preparation design validation session change testing create documentation requirement requirement install postgresql version the hardware spec evaluate traffic expect create cluster primary node read configure new patroni cluster create new cluster consul patroni use provision pgbouncer receive traffic read write receive traffic read
1250,70815983,1.0,memory troubleshooting guide come incident diagnose memory issue notoriously hard there method largely dependent type process profile it good guide go diagnose high memory usage it sense separate guide process type go we pprof potentially continuous profiling ruby stackprof heap dump objspace gc stat jemalloc stat generic pmap smap core dump heap dump gdb novel heaptrack tcmalloc poireau some need work tooling end but pprof continuous profiling good document ref
1252,70643594,5.0,add patroni cluster add wal g wal fetch patroni cluster help catch big replication delay load wal file gcs close switch stream replication transparently postgre this reduce load primary enable recover wal file miss primary we wal g binary instal patroni node gstg gprd but gstg work box use wal g wal push need point wal g configuration gprd wal e gcs bucket enable wal g wal push this corrective action
1253,70642475,1.0,improve replication lag runbook instruction the runbook point replication lag alert cover related issue wal file clean replica lag instruction deal unused replication slot this corrective action
1254,70636194,3.0,disable chef client preserve reboot in see disable chef client disable chef client preserve reboot unexpectedly bring db rotation high replication delay we need figure preserve reboot node unhealthy state maybe fail write change disk case work general need test fix
1255,70624806,1.0,adjust backup alert threshold meaningful value we currently alert successful basebackup ago this problematic randomly alert backup fail backup take slightly long successful alert backup take time successful we set like hour tolerance backup take unexpectedly long previous reason sunday backup finish slightly fast monday backup traffic look thano variance backup time
1256,70422575,5.0,patroni fail datum disk we incident staging datum disk primary run make db unavailable patroni fail node the patroni log contain exception time db gstg error db gstg traceback recent db gstg file patroni lib site package patroni line query db gstg cursor db gstg file patroni lib site package patroni line connection db gstg return db gstg file patroni lib site package patroni postgresql line db gstg db gstg file patroni lib site package line connect db gstg connect server connection refuse db gstg server run host localhost accept db gstg ip connection port db gstg during handling exception exception occur db gstg traceback recent db gstg file patroni lib site package patroni line db gstg retry db gstg file patroni lib site package patroni line query db gstg return param db gstg file patroni lib site package patroni line query db gstg raise problem db gstg postgresconnectionexception connection problem we need sure patroni able fail primary unavailable space leave data disk
1257,70420669,3.0,prevent db wal file fill data disk fail unused replication slot fail logical replication prevent wal file clean we need measure prevent fill datum partition lead problem data loss
1258,70413585,3.0,alert reboot while alert symptom preferable class node certain reboot cause noticeable issue it helpful alert reboot case look symptom easy deduce reboot cause symptom threshold miss frequent reboot we alert probably page reboot gitaly patroni nod we alert high reboot frequency node there reboot dashboard
1259,70226196,1.0,gitlab connection troubleshoot instruction customer when customer report problem connect hard debug cause miss information we provide troubleshooting instruction customer help debug problem sure support documentation hand point customer a basic aid kit like traceroute curl curl curl null for future work enable nel
1260,69812143,2.0,runbook update permanent maintenance mode section patroni management runbook during simulation demo notice area improve update permanent maintenance mode section patroni management runbook step include explanation
1261,69679310,1.0,improve postgre runbook i recently receive page increase number dead tuple run dead end follow runbook investigate as follow investigation work ongre ensure runbook lead well actionable step resolution
1262,69552071,1.0,add pagerless quieter symlinkable pipeable version exist gkms utility it nice pagerless quieter symlinkable pipeable version exist gkms utility chef repo
1263,69510779,2.0,install observability tool patroni fleet current situation current situation intend iterate work describe currently order inspect activity database open psql session sudo gitlab psql particular node run query select from page result find look desire outcome intend outcome work represent issue how improve current situation this change propose install tool project source homepage this support easy method examine database activity wide range option sorting filtering keystroke away instead craft copypasta query database psql session acceptance criteria work item necessary arrive desire outcome atomically x the json chef role staging patroni fleet modify include x the installation pg activity pypi package x the invocation script automatically use credential repeat step gstg gprd environment specific role
1264,69494139,3.0,praefect database staging constantly near cpu for example note service change chart variable link need change cc
1265,69421064,1.0,revamp storage dashboard redo include stat time
1266,69383079,2.0,add support remote file management chef role add support remote file management chef role see
1267,69382125,4.0,change system wide default branch main change system wide default branch main see discussion
1268,69370978,2.0,add database user management tool script patroni fleet add database user management tool script patroni fleet
1269,69370790,2.0,create new gitaly storage shard node replace nfs gitaly storage shard nfs usage update utc our usage target specify try maintain usage new project creation quickly cause usage nfs there currently gitaly shard nodes configure accept new project nfs maintain level availability important avoid shard fill quickly to remove single node new project storage rotation cluster prevent acceleration capacity consumption new gitaly shard node create add list shard configure gitlab application store new project repository create production change issue template
1270,69370738,2.0,create new gitaly storage shard node replace nfs gitaly storage shard nfs usage our usage target specify try maintain usage new project creation quickly cause usage nfs there currently gitaly shard nodes configure accept new project nfs maintain level availability important avoid shard fill quickly to remove single node new project storage rotation cluster prevent acceleration capacity consumption new gitaly shard node create add list shard configure gitlab application store new project repository create production change issue template
1271,69308903,1.0,proxy fastly currently proxie fastly review app host serve directly src server this mean fastly dependent feature like edge redirect test currently review app domain require use separate domain proxye review app domain allow simplify infrastructure
1272,69304900,1.0,delete provision vm gl a vm provision customer migration delete month
1273,69299269,8.0,fix wal g gcs upload wal g gcs upload reliable see
1274,69299009,5.0,fix wal g backup list sorting wal g sort backup modification timestamp instead creation timestamp lead take wrong backup backup fetch latest we sort creation timestamp instead see
1275,57456373,3.0,repostor migration repos block user hdd after conversation i learn repos user block remove good target hdd
1276,57053756,1.0,use general public splashscreen default start page i like switch default dashboard public grafana instance why key ares apdex request second error rate saturation metric key public face service service status description service healthy warning degraded maintain git wdyt
1277,55804436,2.0,demonstrate runbook add pgbouncer instance demonstrate create pgbouncer read write node create pgbouncer read only instance
1278,55752141,1.0,create elasticsearch lab slack bot elasticsearch api the idea replicate success database lab slack channel instead elasticsearch index instead database what look like there new app slack read access elasticsearch index grpd profile validate query send redact match document the bot issue query upload redacted response channel file upload some metadata like query time result count etc show directly bot response why useful as usage advanced search ramp diagnose slow query test improvement implementation do security review elasticsearch profile api ensure datum safety create slack application manage elasticsearch call add application slack workplace references elasticsearch validate api elasticsearch explain api elasticsearch profile api slack file api
1279,54313901,1.0,revert mr remove repack extension package production patroni fleet this mr remove repack extension package production patroni fleet revert way directive useless post removal long unnecessarily codify go forward
1280,54312183,1.0,revert mr remove repack extension package staging patroni fleet this mr remove repack extension package staging patroni fleet revert way directive useless post removal long unnecessarily codify go forward
1281,54163469,2.0,set gcs bucket serve gitlab docs review apps similarly like host documentation review apps gcs bucket serve cloudflare the setup review apps the domain manage the gcs bucket probably marketing documentation similarly the ci config update similarly relate work
1282,54077923,3.0,monitor cloudsql performance as show probably need monitor cloudsql performance praefect db possibly extend cpu usage high limit likely sign something be wrong need attention save lot investigation time delay metric alert early want look
1283,53868182,1.0,runbook need trigger alertmanager update the runbook repo update alertmanager config push file gcs this need trigger update process update config gke some option include trigger deploy reload kind directly push config map change
1284,50152574,1.0,create new gitaly storage shard node stor gprd replace stor gprd configure rotation store new project gitaly storage shard nfs usage our usage target specify try maintain usage new project creation quickly cause usage nfs there currently gitaly shard nodes configure accept new project maintain level availability important avoid shard fill quickly to remove single node new project storage rotation cluster prevent usage acceleration new gitaly node create add list shard configure gitlab application store new project repository create production change issue template
1285,50122720,8.0,implement lock mechanism database backup we want run wal g backup push patroni replicas but manually designate node running backup problematic it need manually adjust case failover designate special node elegant backups stop work special node go proposal the replica regularly try acquire lock ttl consul lock holder run backup push this lock run wal push replica transition period wal e wal cron job replica try acquire lock minute lock holder create local lock file process like need query consul time second lock cronjob backup push script check run primary release lock lock file case minute time window race condition case failover wal push run node parallel minute wal g issue test
1286,50113426,2.0,decommission op gitlab net es cluster op gitlab net cluster eol upgrade search indexing we disable feature safely reomve op gitlab net deployment elastic cloud account
1287,50082769,2.0,investigate initial dns setting node gcp eliminate gap setting reference incident one hypothesis incident pgbouncer time cache zone record wo read dns information upstream provider use resolution this mean gcp node start specifie example default google dns information dhcp dnsmasq information place pgbouncer try query google dns instead local dnsmasq service acceptance criterea be hypothesis correct if prevent dns gap occur
1288,50031592,5.0,wal g backup fail op walg backup fail may see thanos datum
1289,49918227,2.0,dial sidekiq fleet concurrency a request dial sidekiq fleet concurrency elasticsearch advanced global search cluster overload
1290,49866678,1.0,deploy break it appear mr merge master deploy the commit deploy it appear stop work late friday june i know specific deploy work i think problem chef client there lot chef relate warning error log syslog maybe snippet helpful jun chef m jun chef compile error chef cache cookbooks recipe m jun chef m jun chef m jun chef m jun chef expect process exit receive jun chef begin output gsutil cp op secret gitlab walg gcloud kms decrypt gitlab secret op global jun chef jun chef serviceexception anonymous caller access google cloud storage object jun chef the require property project currently set jun chef set current workspace run jun chef m jun chef m gcloud config set project value jun chef m jun chef set temporarily environment variable jun chef end output gsutil cp op secret gitlab walg gcloud kms decrypt gitlab secret op global jun chef gsutil cp op secret gitlab walg gcloud kms decrypt gitlab secret op global return m jun chef m jun chef m jun chef chef cache cookbook library jun chef m chef cache cookbook library jun chef m chef cache cookbook library block jun chef m chef cache cookbook library jun chef m chef cache cookbook library jun chef m chef cache cookbooks recipe jun chef m chef cache cookbooks recipe m
1291,49856734,1.0,ca redis cluster gitlab miss instance op environment this issue track work corrective action redis cluster gitlab miss instance op environment alert it recommend remove alert entirely alert definition exist instance go incident issue mr remove alert rule
1292,49310256,3.0,write runbook project export we runbook manual project export while work find way export fail document possible solution
1293,47057779,5.0,enable access log gitlab runner custom fargate download bucket the runner group love able gauge interest new aws fargate driver the simple way track download bucket this issue request enable access request log parse datum download driver fyi cc
1294,46811018,1.0,license database extract hi create issue request license db extract hand book command require owner acl se on extension g thank thumbsup
1295,46808291,2.0,export version database load warehouse runbook
1296,44840332,3.0,review late switchover experience consider possible improvement future mention switchover restart replicas perform successfully weekend start utc cause increase load minute error rate significantly increase high cpu period from i monitoring look like significant stress system i think additional analysis need result improvement procedure future
1297,43848773,3.0,use correct timestamp elastic search postgre log the timestamp kibana log postgre reflect time ingestion actual time log event this make minute wrong order we use field log timestamp
1298,43416308,4.0,configure flipper http adapter summary in term use gitlab feature flag system development epic todo tbd
1299,43414649,4.0,configure flipper http adapter summary in term use gitlab feature flag system development epic todo tbd
1300,24515650,5.0,upgrade patroni the note copy the patroni release note cite bug fix allow health check patroni block patroni potentially slow stall consul agent promptly talk consul server reduce lock time take method alexander due lock hold dcs slowness affect rest api health check cause false positive this bug fix help avoid case unnecessary patroni failover the patroni agent slow respond incoming health check request hold internal lock run rest consul agent slow this bug fix avoid contention time wait lock rest consul finish cache result ttl second this address concern slow call consul lead patroni cluster lock expire probably increase patroni dcs ttl set addition upgrade be sure review release note current patroni release target release confirm patroni cluster run safely mixed mode member run functional testing plan deploy gstg gprd
1301,24515597,3.0,increase patroni patience talk consul increase patroni dcs setting currently second ttl currently second the rationale describe and remember ttl silently halve patroni send consul the ttl probably time large note the set currently second ok leave increase setting roughly long patroni wait passively notification consul cluster state change start round push status update consul renew consul session ttl the configure ttl significantly large
1302,24515528,1.0,stop abort patroni failover the setting apply postgres user account this consistently abort conversion old primary db replica failover make configuration change avoid significant toil replace rebuild old primary node fresh replica
1303,24505303,5.0,why patroni failover occur problem statement patroni failover event expensive occur frequently expect while failover progress read replica database remain available writable primary database unavailable this cause upstream client fail task require interaction primary database for purpose effectively unavailable time patroni failover mechanism crucial maintain high availability writable postgres database provide efficient reliable return service writable instance fail unreachable client however unnecessary failover event harm availability typically cause minute downtime require hour manual clean analysis goal reduce rate unnecessary failover event improve availability avoid toil discover trigger recent patroni failover event propose option avoid sacrifice ability detect respond event necessitate failover non goal reduce toil associate failover event separate desirable goal address point the setting apply postgres user account this consistently abort conversion old primary replica failover make configuration change avoid significant toil replace rebuild old primary node fresh replica reduce duration downtime failover event separate desirable goal tune expect yield significant improvement the downtime duration consist phase failure detection time time actual failure detection mainly affect health check frequency timeout scope tune failure detection aggressive lead high false positive rate that appear case currently reduce currently high false positive rate require increase time detect actual failure to good knowledge currently patroni failure detection time second ttl leader election patroni leader election process include mandatory delay let replica apply old primary transaction possible wal stream then freshest healthy replica elect new primary the postgres timeline fork replica ask switch new timeline start consume new transaction new primary reconvergence client reconnect new postgre primary db this time small client connect writable primary postgre instance proxy pgbouncer only handful pgbouncer instance actually reconnect patroni new primary postgres db background in couple month patroni time initiate failover writable primary postgres node most failover appear unnecessary judge availability metric prior failover prior work for reference necessarily patroni failover event investigate failover rca issue failover rca issue failover rca issue failover rca issue several people independently observe patroni failover trigger timeout patroni agent local consul agent those timeout observe patroni method make patroni loop rest call local consul agent what cause timeout clear idea propose include limit ephemeral network packet loss general path consul agent connection consul server kernel memory pressure delay tcp receive consul server undergo leader election at failover show time failover current patroni lose cluster lock consul session mutex invalidated this effect brief interruption consul agent consul server consul session implement patroni cluster lock automatically expire unlock cluster renew second patroni attempt renew session second second delay lead lock expiry
1304,24505297,5.0,we lose custom metric rule gke migration container registry since migration gke metric long work recording rule invalid impact metric slo min ratio ratio this situation cause loss visibility panel dashboard and trigger alert look datum utilize issue find new recording rule metric ensure panel aforementioned dashboard work
1305,24497368,2.0,create database gke grafana service currently store grafana dashboard configuration locally sqlite database x create cloud host database sync datum old grafana new
1306,24497209,1.0,move grafana dashboard service kubernetes top level tracking move grafana dashboard gke
1307,24486926,1.0,cut line this issue functional hack board see feature request real cut line
1308,24476239,5.0,proposal use image provisioning there discussion topic i like bring formally decision i like propose provision database node chef directly pre create image the main driver reproducibility current dynamic provisioning lead different version potentially software component like kernel happen intermediate library main component in case have consistent reproducible image lead problem like uneven performance difficult impossible debugging cause different versioning bug cause component version happen possible security issue data affect issue postgresql example prone index corruption issue replicate instance run different version glibc alter collation reproducible build reproducible image generally consider good pattern obviously mean chef current provisioning system start base image install software generate final image actually image layer start base ubuntu lts layer add basic common utility package image add specific gitlab package software image add postgresql pgbouncer software image while option exist packer tool generate image other reproducible build technique offer advantage make provisioning stage error prone provisioning external software repository fail lead provisioning fail fast provisioning normally important buy help instance new replica need bring asap well security since image immutable statically analyze security issue dynamic provisioning install potentially different typically new version package analyze security perspective it come drawback image management image combination point kind exponential with proper scripting layered approach big deal high storage cost image minor cost factor software package update but fix create periodic issue review check software package update cc
1310,24475284,1.0,unset gitlab superuser postgresql configuration parameter currently set while reason setting global scale avoid possibly idle connection take account exist postgresql parameter similar cause trouble administrative command for instance need disable run long run operation like vacuum analyze most importantly cause recent failure trigger patroni see while patch postgresql imho subject need work i propose workaround among i propose implement follow alter user gitlab superuser set equivalent command disable statement timeout gitlab superuser user bad thing application connect user hopefully case the main reason prevent fail avoid have unset operation this need add ddl code require action check application code connect database gitlab superuser case need check add ddl statement ddl creation process prepare ddl change script apply production if proposal act comment
1311,24475245,1.0,make sure meltano cert upgrade happen your sslmate certificate expire september the renewal certificate await approval follow domain please visit please visit for information approve certificate visit cc
1312,24470177,2.0,update gitlab exporter gitlab monitor enable ci metric scrape we disable scrape ci metric query real heavy the issue now improve query add date range filter this require new index go ship once index find way production update gitlab exporter archive dr replica revert enable scraping
1313,24459604,1.0,alert toil hpa unable scale this particular alert fire i think simply need tune the alert relatively basic scale operation complete successfully hit minimum number pods bad thing the time i see alert fire staging environment legit fire base current alert rule undesire mean json type scalinglimited status true lasttransitiontime reason toofewreplicas desire replica count increase fast maximum scale rate utilize issue figure tune alert fire situation scale minimum number pods in situation completely fine happen hpa want scale allow we want pod run sake redundancy
1314,24458199,1.0,read permission analytic new table when create new table assign read permission analytic user this lead an alternative update permission manually add analytic user gitlab group gitlab user own table read write access write need analytic archive replica read anyways we want sure analytic user patroni instance example cc
1315,24457541,3.0,cleanup unused postgre resource as long postgre epic work remove unused postgre resource clean unapplied terraform config
1316,24436488,2.0,create share cloudsql module terraform since google provide cloudsql module suit use case need implement module extend the primary issue the module implement private ip support we connect instance private ip gitlab instance app run gke cluster add private network functionality vpc module make cloudsql work currently cause problem api permission google module support count mean enable disable ci variable when build project kubernetes cluster attach project group auto devops want able use cloudsql production instance disable staging review app the module terraform support conditionally include module the new module
1317,24417128,5.0,improve alert job get stick sidekiq queue we notice soon job stick sidekiq queue long run job block we well prompt alert
1318,24416205,2.0,add runbook analyze gitaly pprof datum in case incident performance profiling helpful analyze go pprof datum we runbook instruction
1319,24411696,2.0,declare gitlab ci gcp firewall rule terraform currently gitlab ci firewall rule manually manage we declare terraform manage reviewable version control way somewhat relatedly chef role declare iptable rule we consider remove redundant gcp gcp firewall careful checking need ensure remove iptable rule gcp equivalent enforce cc fact checking
1320,24398833,1.0,fix terraform ci failure aws snowplow environment while work fix error mr tflint start work properly surface follow error aws snowplow terraform configuration tflint error fail load configuration attribute redefine the argument set each argument set several lifecycle block generate error relate attribute name like false positive tflint error fail load configuration attribute require dot follow attribute once lifecycle block comment enable tflint run finish final output tflint character high match valid pattern character high match valid pattern result issue error warning notice
1321,24393429,1.0,performance insight week query see query see query see
1322,24334739,2.0,why postgre replication lag grow significantly the dr environment postgre replica host recently lag hour trigger pagerduty alert the prometheus metric suggest chronic problem potentially affect replica why replication fall what
1323,24320338,2.0,alert run nat port after roll change describe machine usually public ip access internet google cloud nat to determine ip need use follow formula m p where m number machine region multiply generous number account future growth p minimum nat port vm default there tcp udp port available nat ip context introduce cloud nat create resource saturate assume constant min nat port vm risk run nat port number machine router grow we alert sre provision extra nat ip advance
1324,24312315,2.0,gitlab monitor scrape cause replication lag archive replica background we disable gitlab monitor patroni host suspect query amplify load database i move heavy query endpoint archive replica we observe unsustainable replication lag archive replica a single scrape take query offender query plan there direct way speed query expect scan lot datum gb buffer query this issue track infra change
1325,24304846,2.0,create new dr unix group allow ssh sudo access we like new dr unix group create require allow ssh sudo access dr specific node relate cc this issue move
1326,24195544,2.0,create diagram prod create diagram prod setup port visibility team also table host
1327,24186594,1.0,error report slack question
1328,24176931,1.0,add env zero environment page handbook we include information env zero bootstrap infrastructure project
1329,24138265,1.0,dashboard sync lib error unauthorized lib value main this fetch dashboard list public server probably break public dashboard server get rebuild i suspect create update api key
1330,24105643,1.0,look rail console issue dr environment test ssh rail console dr i like create i start test access ash access request hope geo team well access look issue dr environment davids mbp user ssh dsmith rail starting console wait gitlab ee gitlab shell traceback recent bin main bin require gitlab embed lib ruby gem lib rail require gitlab embed lib ruby gem lib rail invoke gitlab embed lib ruby gem lib rail command perform gitlab embed lib ruby gem lib dispatch gitlab embed lib ruby gem lib thor gitlab embed lib ruby gem lib thor run gitlab embed lib ruby gem lib rail command console perform gitlab embed lib ruby gem lib rail command gitlab embed lib ruby gem lib rail command gitlab embed lib ruby gem lib rail gitlab embed lib ruby gem lib require gitlab embed lib ruby gem lib gitlab embed lib ruby gem lib block require gitlab embed lib ruby gem lib require gitlab embed service gitlab rail config require gitlab embed lib ruby gem lib rail initialize gitlab embed lib ruby gem lib rail gitlab embed lib gitlab embed lib gitlab embed lib gitlab embed lib gitlab embed lib gitlab embed lib block gitlab embed lib gitlab embed lib gitlab embed lib ruby gem lib rail gitlab embed lib ruby gem lib rail gitlab embed lib block gitlab embed lib gitlab embed lib block level gitlab embed lib block level gitlab embed lib block gitlab embed lib ruby gem lib rail block gitlab embed lib ruby gem lib rail run gitlab embed lib ruby gem lib rail gitlab embed lib ruby gem lib rail block class engine gitlab embed lib ruby gem lib rail gitlab embed lib ruby gem lib rail block level class engine gitlab embed lib ruby gem lib rail gitlab embed lib ruby gem lib instrument gitlab embed lib ruby gem lib rail block gitlab embed lib ruby gem lib load gitlab embed lib ruby gem lib gitlab embed lib ruby gem lib block load gitlab embed lib ruby gem lib load gitlab embed service gitlab rail config initializer require gitlab embed service gitlab rail lib gitlab version gitlab embed service gitlab rail lib gitlab gitlab embed service gitlab rail lib gitlab connection gitlab embed lib ruby gem lib connection gitlab embed lib ruby gem lib gitlab embed lib ruby gem lib abstract gitlab embed lib ruby gem lib abstract connection gitlab embed lib ruby gem lib abstract checkout gitlab embed lib ruby gem lib abstract gitlab embed lib ruby gem lib abstract gitlab embed lib ruby gem lib abstract gitlab embed lib ruby gem lib abstract gitlab embed lib ruby gem lib gitlab embed lib ruby gem lib new gitlab embed lib ruby gem lib initialize gitlab embed lib ruby gem lib connect gitlab embed lib ruby gem lib connect gitlab embed lib ruby gem lib new gitlab embed lib ruby gem lib initialize error pgbouncer connect server
1331,24080762,1.0,use letsencrypt similarly i think time switch docs site let encrypt i hope work correctly lot visitor crucial work properly
1332,24071878,1.0,use letsencrypt host gitlab pages cert expire day ago notice it require manual effort find update cert sslmate autorenewe give letsencrypt le support build pages beta presumably generally work dog foode exercise reduce irregular cognitively expensive toil work easily miss
1333,24053089,2.0,some public dashboard break for example most panel render error message query time expression evaluation continuously refresh page occasionally yield panel load update work need remove iptable firewall rule runner prometheus instance scrape gprd infra prometheus public network interface the gprd prometheus ip whiteliste this mr get ball roll i drop floor ago it imperative gcp firewall carefully check ensure open security hole remove box firewall add rule terraform need note interact cause ip gprd prometheus change currently cloud nat
1334,24009928,3.0,gke gprd evict registry pod occasional check cluster traffic canary shift result evict pods course time we alert set type situation to handle utilize issue track prevent evicted pods the reasoning currently diagnose memory overuse compare configure memory request message the node low resource memory container registry exceed request this particular pod example mb ram request mb the request configuration yaml registry image port host port request cpu m memory utilize issue track good handle situation it need figure appropriate baseline manage request appropriately kubernetes scale node need reference
1335,24007535,1.0,use helm diff dry run mention coffeebreak helm diff think useful would nice addition exist dry run pipeline branch right
1336,23976412,2.0,slow database vm different region wip issue leave op state slow see timeout error chatop to wrap following delete op create new instance new mount nfs setup use memorystore connect cloudsql repos point shut box rsync datum old new server change dns or worried introduce change file local disk
1337,23972334,1.0,run clearsharedrunnersminutesworker production in fix issue pipeline minute usage show incorrect value the problem clearsharedrunnersminutesworker time try reset number project give fix need run production reset pipeline minute quota user correct statistic we decide despite give free minute user the worker run automatically cron schedule month the ask possible run following production ruby report status operation issue this long result project namespace statistic sync month august
1338,23957253,3.0,route slo alert pagerduty route latency slo alert pagerduty accurate indicate real production issue we send slo alert pagerduty trim false alert rate error ratio alert saturation alert service availability alert operation rate alert probably pagerduty manifest high latency error ratio affect production
1339,23942446,2.0,shut this machine zoom sync replace schedule pipeline x chef repo change x gitlab com infrastructure change
1340,23928490,5.0,bolster alerting gke cluster component we lack alert page issue gke clusters component instal inside let improve stature consider following x the stable prometheus operator helm chart alert choose implement configuration alert consider port suit need x alert number evict pods remain x alert unable scrape metric extended period time x custom rule need capture run pods x validate rule place reach limit hpa scale configuration x alert bump maximum allow node node pool x node specific metric disk memory cpu usage x container throttling
1341,23927256,1.0,change kubernetes monitor services use json log output component instal monitoring standard test log format let swap json log format easy search inside kibana
1343,23846368,1.0,consolidate bastion howto page runbook we currently following gitlab com runbook howto gprd howto gstg howto op howto dr howto pre at minimum consolidate i prefer instead file repository onboarding directory we update regularly
1344,23845288,1.0,when deployment kubernetes fail master branch inconsistent deploy our deployment procedure kubernetes assume deployment succeed in time helm automatically roll desire change this present issue master branch long accurrate representation run cluster utilize issue track ensure master reflection exist cluster thought we want setup failure job perform work potentially revert change issue link mr need form communication know deploy unsuccessful auto close need reopen
1345,23845203,1.0,gprd kubernetes cluster inadvertently create preemptible instance the cluster create node type mistake utilize issue switch node pool change non preemptible node
1346,23845090,3.0,gprd kubernetes cluster generate error utilize issue track error learn discuss address log filter ignorable info daemonset fluentd gcp change possible yes rm remove ssl no file directory possible yes warning contain exactly certificate crl skip possible yes unable fetch pod metric pod metric know pod not advise no watch end old resource version possible possibly caller new block not advise yes prometheus info level message no advise yes level info server listen not advise yes actionable items
1347,23812123,3.0,enable json log registry currently structured log registry gke enable
1348,23794489,2.0,change onprem testbe env gcp filestore gitaly node request thread
1349,23789919,3.0,investigate saturation read replica pgbouncer lack load distribution during observe follow behavior at utc experience network issue cause patroni fail end corrupted state offline new master master replicas immediately overloaded server connection maxe configure active client drop wait connection increase replicas register failover blip manage stabilize nearly inmediately the behavior expect as experiment take cluster temporarily what observe crater excess load when finally restore add cluster read replica attempe experiement pull rotor as cratere check internal list host screenshot datum save simply entropy from memory database replica list after add cluster list with sample observation combination see post rejoin possible unlikely we expect spot expect it worth note replica start see slow recovery thus clearly capacity component riddle as precaution add database replica hope addition buy runway case have replica fail
1350,23788247,2.0,communication incident communication lack effectiveness neglect update time notify support fair know try people join incident
1351,23787912,3.0,client read replica list observability during observe unexpected behavior application read replica load balancing when node fail failover cause significant uneven load shift when remove cluster load shift when undo load come when add restore try remove overloaded it replica work let time stabilize see platform nominal operation one question ask incident list look like fleet it turn easily answer magic able watch node it useful expose datum able site wide be randomize think expose log step require flight processing a possible visual approach replica client position array we cap position start regardless number replicas in event able incident like today imperative
1352,23787703,3.0,patroni documentation training post realize ee need review patroni documentation runbook training database cluster configure patroni work perform common operation add node remove node check status build node etc everyone eoc imocs familiar dna like good setting run training
1353,23787506,3.0,patroni runbook tool during apparent patroni runbook lack detail work date for instance create new replica run bunch issue question validity chef terraform state disk sizing patroni startup etc also require deep knowledge flag we think abstract operation tool hide complexity i want able type like foo create replica foo disable replica target foo likely gittab ctl patroni the way today risky craft command fly execute host production
1354,23787489,1.0,patroni runbook add enable statistic during mention statistic enable new master failover cover runbook
1355,23787387,1.0,runbook handbook update page ongres during clear eoc page ongre add runbook handbook change chatop necessary reduce friction
1356,23756624,1.0,set redirect catchall haproxy domain per request gitlab com marketing corporate need following also work
1357,23753683,1.0,build automation calculation storage mtbf user face service sub issue starting point discuss question track mtbf currently spreadsheet manual question discussion where go datum currently production issue what mtbf metric want track mtbf sev incident issue mtbf incident issue further breakdown tag eg mtbf service where want store end result prometheus periscope add start discussion
1358,23722603,2.0,cert manager version old need upgrade nov letsencrypt say we work jetstack author cert manager series fix client cert manager fall traffic pattern send excessive traffic let encrypt server continuously to mitigate plan start block traffic cert manager version current semver minor release november please upgrade cert manager instance some old kubernetes cluster fall category probably autodevop setup run cert manager pod
1359,23717031,2.0,rca consul ssl issue incident gitlab com gl infra summary a brief summary happen try executive friendly possible we discover expire self sign certificate consul server these certificate renew usual way signing key cerficate authority long available exist tls connection service pass traffic change network interruption cause disconnect able reconnect this problem database high availability setup use connection service location if exist connection web node api node interrupt able find database if connection database node interrupt database fail able decide primary each situation bad render entire site unusable fix the problem case go machine address problem time work even roll push change chef leave individual node non functional minute all change need exactly simultaneously allow database fail this leave lot individual risk lot unknown test validate there possible solution work walk decide turn validation certificate remove risk allow time come proper solution certificate management all option require similar effort importantly risk process simultaneously restart service other solution explore replace certificate ca self sign cert switching single custom ca system ca store sslmate switch letsencrypt cert metadata affect consul database pgbouncer patroni web api team attribution sre minutes downtime degradation second consul minute patroni impact metrics start following what impact incident service outage sub service brown exposure sensitive datum the impact elevated risk interruption establish tcp connection cause partial total outage depend involve who impact incident external customer internal customer specific team everyone impact in end impact notice involve activity how incident impact customer prevent x incorrect display y there impact customer detection response start following how incident detect this detect restart database server staging it connect consul do alarming work expect no we alert expiration certificate intend production how long start incident detection day certificate expiration notice how long detection remediation about day troubleshooting planning maintenance window remediate be issue response incident bastion host access service available relevant team memeber page able sshguard consul server lock bastion host behavior test staging match behavior test dr root cause analysis the ssl certificate expire consul host self sign certificate use ca key long exist no production readiness review these server originally proof concept later promote production move fast rush switch database high availability technology patroni what go start following the process restart consul server cause outage go exactly plan the team amazing job cover possible risk plan the handover time zone extremely helpful what improve start follow use root cause analysis explain improve prevent happen our method manage certificate optimal certificate automatically renew case be improve detection time detection all certificate monitor especially case auto renew a production readiness review catch usage self sign certificate associate ca be improve response time response we hand planning response apac shift europe shift troubleshooting finish we decide instead set emergency procedure people troubleshooting testing one plan execute response in retrospect right decision situation urgent reduce time be exist issue prevent incident reduce impact yes do indication knowledge incident place since alert indication corrective action list issue create corrective action incident sshguard block consul server guideline blameless rca guideline s
1360,23683431,1.0,deep dive drive deep dive incident perform walkthrough incident dna meeting august please familiarize incident rca this issue process troubleshoot
1361,23679500,1.0,replace ssl cert ssl certificate expire m echo notafter aug gmt sslmate list dv active no key file certificates store manage gcloud upload entire new mean need rekey sslmate
1362,23678289,4.0,project repository migration epic
1363,23674684,1.0,cut line this issue functional hack board see feature request real cut line
1364,23674652,1.0,cut line this issue functional hack board see feature request real cut line
1365,23674367,1.0,cut line this issue functional hack board see feature request real cut line
1367,23661438,3.0,mttp infrastructure measure mean time production infrastructure change elapse time commit master have change apply production chef terraform etc
1368,23646435,2.0,clean production board the production board number item open state like incident change delta we need clean move infrastructure project close additionally bunch change change change point unfinished as general rule change clearly define start end state let clean finish move infra issue
1369,23632177,4.0,certificate runbook ensure certificate renewal installation runbook certificate use as good certificate inventory capability good place start gather inventory chef epic
1370,23631223,5.0,certificate monitoring ensure expiration monitoring certificate use as good certificate inventory capability good place start gather inventory chef
1371,23621412,1.0,sshguard block access consul server sshguard block ssh access consul server it get request bastion interpret hostile add iptable rule block bastion ssh port this effect lock we set timeout allow traffic use different method determine hostile traffic sudo iptable result line like folowing bastion server list chain sshguard reference num pkts bytes target prot opt source destination k drop the line remove sudo iptable sshguard we determine exactly traffic trigger behavior if cause access ssh setup misconfigure cause malicious get access bastion
1372,23618000,1.0,rca merge request get close inadvertently please note incident relate sensitive datum security related consider label issue mark confidential incident gitlab com gl infra summary because issue note mr source branch report miss refresh service close mr find report missing source branch affect sidekiq team attribution minute downtime degradation for calculate duration event use platform metrics dashboard look appdex slo violation impact metrics start following what impact incident small number mr get close inadvertently who impact incident small number user internal external how incident impact customer mrs close stay open how attempt access impact service feature how customer affect how customer try access impact service feature include additional metric relevance provide relevant graph help understand impact incident dynamic detection response start following how incident detect report user do alarming work expect alarming kind event how long start incident detection how long detection remediation be issue response incident bastion host access service available relevant team memeber page able root cause analysis merge request get close unexpectedly why close mr why because branch miss why the branch cache invalidated why we kill job incident prevent branch invalidation case why because possible race condition branch invalidation what go start follow identify thing work expect any additional out go particularly what improve start follow use root cause analysis explain improve prevent happen be improve detection time detection be improve response time response be exist issue prevent incident reduce impact do indication knowledge incident place corrective action x fix mergerequestworker race condition guideline blameless rca guideline s
1374,23611966,1.0,rca gitaly call cause bad latency sidekiq queue grow please note incident relate sensitive datum security related consider label issue mark confidential incident gitlab com gl infra rapid action issue summary some commit massive tag cause job gitaly call lead high gitaly latency grow sidekiq queue for timeline incident issue affect gitaly sidekiq web team attribution minute downtime degradation m m for calculate duration event use platform metrics dashboard look appdex slo violation impact metrics start following what impact incident high gitaly latency sidekiq queue grow high web latency who impact incident user wait job trigger finish web hook how incident impact customer how attempt access impact service feature how customer affect how customer try access impact service feature include additional metric relevance provide relevant graph help understand impact incident dynamic detection response start following how incident detect utc gitaly latency apdex alert alert general notice eoc do alarming work expect alarming queue size fire early gitaly latency alert pagerduty how long start incident detection queue start grow get detect eoc m how long detection remediation m m be issue response incident root cause analysis sidekiq job pile hour why jobs take long process why gitaly latency get bad why there gitaly call job why some job process massive amount tag cause problem gitaly why commits user contain tag what go start follow identify thing work expect any additional out go particularly what improve add limit thing like tag improve performance handle tag improve sidekiq architecture alert grow queue size fire early gitaly latency apdex alert pagerduty page cmoc mgr slack command work change severity label incident ticket time reflect current rating incident severity update meaningful customer start following corrective action x add chef config sidekiq change manual x create rpc implement rpc x postreceive bound change process x add timeout gitaly call sidekiq x possible kill running sidekiq job architect queue implementation x page gitaly slo alert x identify limit prevent platform incident x add runbook analyze gitaly pprof datum x document infra escalation channel oncall runbook guideline blameless rca guideline s
1375,23606406,3.0,terraform gke module failure apply network policy when network policy disable desire attempt configure provider fail error occur error occur googleapi error the network policy addon enable update node badrequest
1376,23606383,3.0,terraform gke module unable upgrade kubernetes version when attempt upgrade cluster terraform fail error occur error occur update default pool find to update version non default pool use version attribute pool
1377,23580291,2.0,service observability review with team level ownership service urgent task need perform depth review service observability highlight recent consul certificate expiration incident please create issue service say review execute high priority
1378,23573661,3.0,create production gke clusters utilize issue discuss want spec production gke decide cluster production cluster cny cluster handle after decision spin new issue reuse complete work our design document desire detail complete implementation
1379,23564824,1.0,provision short live console terraform chef chef converge special case console instance console see i investigate
1380,23553317,2.0,error deploy sudo chef client warn resource client override resource cookbook please upgrade cookbook remove cookbook warn resource client override resource cookbook please upgrade cookbook remove cookbook warn resource client override resource cookbook please upgrade cookbook remove cookbook warn resource client override resource cookbook please upgrade cookbook remove cookbook warn resource client override resource cookbook please upgrade cookbook remove cookbook warn resource client override resource cookbook please upgrade cookbook remove cookbook recipe gitlab client action install date chef handler action create date chef handler action create date action enable date recipe compile error chef cache cookbooks cookbook license gitlab com recipe nomethoderror undefined method support cookbook trace chef cache cookbooks cookbook license gitlab com recipe block chef cache cookbooks cookbook license gitlab com recipe chef cache cookbooks cookbook license gitlab com recipe relevant file content chef cache cookbooks cookbook license gitlab com recipe copyright gitlab user gitlab license shell false system true support true end directory gitlab license recursive true owner gitlab license group gitlab license end system info platform ubuntu ruby ruby revision linux bin chef client chef bin chef client running handler error run exception handler prometheushandler run handler complete error exception handler complete chef client fail resource update second fatal stacktrace dump chef cache chef fatal please provide content file file bug report fatal nomethoderror undefined method support
1381,23551782,3.0,improve error handling postgres graceful failover script goal improve error handling graceful failover script the script appear kill error handling call function the deadline second appear long complete pgbouncer pause operation consider tune deadline background the graceful failover script automate step switching postgre instance currently primary writable node identify current primary member given environment patroni cluster issue pause command pgbouncer instance db client need write access wait second pause complete pgbouncer instance if wait kill active idle transaction db session primary postgres node wait second pause complete pgbouncer instance if wait try abort kill knife command try issue pause command issue resume command pgbouncers exit script at point script assume pause succeed tell patroni run switchover immediately prefer specify node new primary unpause pgbouncer instance during today production maintenance script problem the pause command slow reach script deadline that abnormal look way reduce chance happen note pgbouncer pause command block operation duration partially dependent client behavior the error handle routine fail reach line un pause pgbouncer instance that leave pgbouncers paused state cause extended downtime detail today automation failure log production change issue
1382,23551529,2.0,automate terraform env project update gitlab com gitlab com infrastructure automatically plan apply configuration environment env project application centric environment this environment need handle little differently inherent circular dependency there multiple approach handle bootstrappe remote state genesis project time example sample code
1383,23551395,1.0,add non core gcp project env project add infra vault configuration core project environment env project gitlab com gitlab com infrastructure consistent
1384,23551276,2.0,use consistent method manage secret bucket we add env project environment gitlab com gitlab com infrastructure order start manage gcp project service account provision resource project infrastructure code terraform module as follow step need sort handle secret bucket conflicting bucket create multiple different module project module likely good place some create project module create gitlab storage module permutation attribute case
1385,23551198,1.0,document bootstrappe gcp project terraform we add env project environment gitlab com gitlab com infrastructure order start manage gcp project service account provision resource project infrastructure code terraform module as follow step need add update documentation project bootstrap management process especially handle circular dependency manage env zero terraform possible
1386,23551045,1.0,add iam permission terraform service account we add env project environment gitlab com gitlab com infrastructure order start manage gcp project service account provision resource project infrastructure code terraform module as follow step need add import update relevant iam permission service account
1387,23549558,1.0,understand postgres failover cause brief outage when fail master node postgre cause brief outage this issue understand
1388,23549530,5.0,manage registry application grafana dashboard libsonnet these need convert current manual creation method libsonnet
1389,23521299,5.0,why pgbouncer host uneven distribution client connection problem a large majority database client connection handle single pgbouncer instance despite fact pgbouncer instance active backend google internal tcp load balancer this imbalance defeat attempt alleviate bottleneck pgbouncer cpu saturation goal learn traffic roughly equally distribute active member google internal tcp load balancer ilb this research task result recommendation alternative if fix imbalance instead use haproxy load balancer pgbouncer instance this optionally let run multiple pgbouncer instance host however unlike google ilb haproxy additional tcp endpoint add network latency component manage background recently week ago response performance problem change database client connect primary postgres instance patroni cluster previously client need write access postgres connect single pgbouncer instance run host primary postgres instance pgbouncer single thread process use cpu worth compute cycle that pgbouncer instance workload saturate cpu ceiling peak daily workload to spread workload cpu need add pgbouncer instance treat pool we reconfigure db clients connect primary postgres database google internal tcp load balancer ilb point pool currently active pgbouncer instance the ilb proxy set network routing rule let client talk directly backend service instance pgbouncer host the ilb backend service actually contain active host inactive host only active host matter analysis surprisingly workload evenly distribute pgbouncer host the number establish connection client pgbouncer unevenly distribute active pgbouncer host this unevenness defeat goal spread workload multiple pgbouncer instance client hostname start web git strongly affect bias prefer connect host
1390,23520099,2.0,rca note creation commit api calls halted sidekiq queue please note incident relate sensitive datum security related consider label issue mark confidential incident gitlab com gl infra summary a user generate note single commit api call slow sidekiq queue block send notification issue mr comment customer affect sidekiq team attribution minute downtime degradation for calculate duration event use platform metrics dashboard look appdex slo violation impact metrics start following what impact incident block send pende mr comment notification who impact incident user suppose notification how incident impact customer notification arrive delay m how attempt access impact service feature how customer affect how customer try access impact service feature include additional metric relevance up notification queue provide relevant graph help understand impact incident dynamic detection response start following how incident detect pagerduty alert queue size do alarming work expect alarming queue size work alert threshold reach hour incident start how long start incident detection m how long detection remediation m be issue response incident easy hold backend engineer beginning root cause analysis notification delay why they pile sidekiq queue why the queue process job considerably slow why many long run job add queue why a commit huge comment get thousand new comment process commit comment slow why apparently automation user create comment api inadvertently what go our monitoring detect issue page eoc backend engineer great job find mitigate root cause thank jump incident quickly what improve we alert soon note stick queue alert queue size job duration limit note user create time commit make process efficient commit note we kind incident year ago improve alert protection note corrective action x improve alert guideline blameless rca guideline s
1391,23517805,2.0,update dbre section on call handbook section the dbre section on call handbook read for database relate issue dbre page respond page good effort basis expect response time alert trigger automation all escalation dbre initiate human sre manager this need update give expect response time also i believe automate alert
1392,23509114,8.0,infrastructure iacv okr raise availability consistent key result mttd mttr mtbf mttp pis ok health minimum level maturity comment key result service fully define observable service level error budget capacity planning comment key result service run kubernetes comment
1394,23508038,1.0,ci cd enablement drive user visible service mtbf day day current mtbf calculation spreadsheet day day base calculation mtbf incident
1395,23501053,8.0,okr drive user visible service mttd unknown min key result devops lifecycle stage sli dashboard development monitor comment key result team own service fully define observable service level error budget capacity planning comment key result team own service run kubernetes comment key result perform weekly load testing stage team own service comment
1396,23499956,8.0,dev ops okr drive user visible service mttr day key result incident severity rca error budget account comment key result team own service fully define observable service level error budget capacity planning comment key result migrate postgresql ecosystem use stackgre kubernete staging comment
1397,23451215,1.0,error deploy gitlab mitigate sackpanic chef client gitlab prometheus systemd windows gitlab server gitlab exporter run handler error run exception handler run handler complete error exception handler complete chef client fail resource update second fatal stacktrace dump chef cache chef fatal please provide content file file bug report fatal cookbook version depend chef version run chef version
1398,23439311,5.0,we need way test kubernete application configuration change locally right test application change happen environment gstg pre this safe way test thing multiple person test it nice utilize docker kubernetes integration locally minikube tool test application configuration change necessary recently use docker desktop toy minikube we need ensure tool utilize backward compatible linux osx use issue track progress make reality
1399,23439198,1.0,migration method gke registry configure set weight script our current script set weight haproxy backend currently work properly registry frontend make necessary modification set weight kubernetes back container registry backend allow transition vm kuberenetes nice easy
1400,124902511,2.0,create new gitaly storage shard nodes gprd store new project production change change summary create new gitaly storage shard nodes gprd store new project why increase capacity new project repository storage more detail detailed step step change build new vm new vm instance ensure creation storage creation storage directory configure gitlab application aware new gitlab application aware new node add new gitaly node kubernetes container new gitaly node kubernete container configuration test new new node enable new node new node gitlab change details services impact change technician change reviewer time tracking hour downtime component zero downtime meta x replace occurrence xx new gitaly shard node number execute command x set title production change issue create new gitaly storage shard node stor gprd store new project detailed step change the follow detailed step change build new vm instance pre condition execution step x create new mr the commit increment default multizone stor variable setting number new gitlay shard add line file environment gprd here example title description use mr x use new value multizone stor field change mr title increment multi zone storage node number new gitaly shard new total x link x have mr review colleague execution command step x optionally check quota apply terraform change you check bash gcloud production compute region describe json jq metric limit usage x merge mr x click apply gprd pipeline stage play button post execution validation step x examine gprd apply pipeline stage output confirm absence relevant error rollback step revert mr ensure creation storage directory once gitaly node create minute chef run system immediately available pre condition execution step x make sure chef client run error bash export stor bundle exec knife ssh sudo grep chef client finish log syslog tail execution command step if chef converge minute invoke manually if chef refuse run wrong procedure roll bash bundle exec knife ssh sudo chef client x confirm storage directory opt gitlab git data repository exist file system new node bash bundle exec knife ssh sudo df opt gitlab git datum repository sudo ls opt gitlab git sudo ls opt gitlab git datum repository head post execution validation step x confirm gitaly service run bash bundle exec knife ssh sudo gitlab ctl status gitaly x confirm relevant error log bash bundle exec knife ssh sudo grep error log gitlab gitaly current tail rollback step no rollback procedure step necessary this step confirm verifie step take far configure gitlab application aware new node configure gitlab application include new node note the gitlab application consider new node disable default pre condition execution step x create new mr chef repo project here example title description use mr the commit consist follow change x update list item file role gprd base stor gitaly add similar json nfs path opt gitlab git data repository x update map entry file role gprd add entry similar json nfs path opt gitlab git data x link x have mr review colleague execution command step x notify engineer on planned change x create silence gitalyservicegoservertrafficabsentsinglenode alert raise new gitaly receive traffic minute reference alert raise past x merge mr x examine pipeline stage output job pipeline verify change apply successfully error post execution validation step x verify chef role check change bash bundle exec knife role gprd base stor gitaly common grep nfs nfs path opt gitlab git data repository x wait minute node converge naturally in normal circumstance chef client periodically run plus upto minute verify check node status ignore patroni postgre server list bash bundle exec knife status role gprd base stor gitaly common list optionally case run patience think explicit run force chef client run relevant node it excruciatingly long time well wait natural convergence bash bundle exec knife ssh role gprd base stor gitaly common sudo chef client rollback step revert mr check pipeline change successfully apply re run command post execution validation step add new gitaly node kubernetes container configuration pre condition execution step x create new mr gl infra workload gitlab com project here example title description use mr x in mr want update file release gitlab add new node yaml list typically datum look like hostname default port tlsenabled false x link x have mr review colleague delivery execution command step x notify engineer on plan change seek approval ensure deployment from announcement ongoing time x merge mr x examine pipeline stage output verify error rollback step revert mr re run execution step roll test new node confirm new storage node operational pre condition execution step export user auth token environment variable shell session bash export also export admin user auth token environment variable shell session bash export execution command step x create new project bash export test rm curl error post private token export jq export jq x clone project bash git clone x add commit push readme file project repository bash echo pushd git add git commit add readme git push origin main popd x use api new storage server bash export export error post private token content type application json jq x optionally poll api monitor state bash curl error private token jq x optionally confirm new location bash curl error private token jq x once project finish move new shard proceed add commit push update readme bash echo pushd git add git commit update readme test nfs git push origin main popd x verify change persist expect bash rm git clone grep test enable new node gitlab enable new node gitlab admin console require admin account change new project store in admin area setting repository repository storage expand list storage node the one check one receive new project for information gitlab docs execution command step x open private browser window tab navigate x click expand button repository storage x click save change button i know change trust process click button x click play production gitaly shard weight assigner job assign weight post execution validation step x take count project create new shard bash export stor bundle exec knife ssh sudo find opt gitlab git data wc x observe number go time post execution validation step take count project create old shard bash export yy stor bundle exec knife ssh sudo find opt gitlab git data wc observe number go time either go change x delete silence create gitalyservicegoservertrafficabsentsinglenode alert step gitlab application aware new node
1401,123233985,4.0,the script specify secret database client connection this template gitlab team members seek support sre existing template available please fill detail detail point contact request if need propose date time date time additional detail format type additional detail sre support need the script require specify client certificate connection this verify sh ssh cat gitlab perl m print sudo env gitlab embed bin psql sslmode verify gitlab ssl praefect database server gitlab ssl praefect database client gitlab ssl praefect database client user praefect dbname the template recipe unfortunately scope cookbook update accordingly edit
1402,118261655,1.0,container registry review adjust sli manifest route context relate task now phase registry upgrade migration complete exist sli likely loose operation execute metadata database fast storage backend slow registry self serve issue
1403,116351647,3.0,establish method monitor health success disk backup snapshot creation gcp summary give context problem issue try prevent happen provide brief assessment risk chance impact problem corrective action fix assist triage prioritization establish method monitor health success disk backup snapshot creation gcp there presently know risk disk backup snapshot begin fail aware situation go try use snapshot fail fail disk backup snapshot result extensive loss customer datum virtual server disk fail reason period fail snapshot creation related note originate link know related incident issue the relation happen automatically create issue incident uncomment follow line originating desire outcome acceptance criteria how know issue complete if initial thought implementation detail gotchas edge case etc share fresh mind an alert generate ultimately sre on page certain number disk backup snapshot fail create certain period time schedule initialization associated services apply appropriate service associate corrective action applicable corrective action issue checklist x link corrective action arise x give context problem corrective action try prevent occur x assign severity label high sev relate incident default x assign priority default
1404,113483221,1.0,demo run pager pause demo run pager pause scenario for site wide outage eoc get nearly continuous stream page need acknowledge distract try focus mitigate say outage for production pagerduty service send page eoc gitlab production slo alerts gprd main stage slo alert gprd cny stage manually create maintenance window waste time incident see action detail moderator note since steve azzopardi excellent work create relationship hierarchy service alert burdensome problem severe incident nevertheless feature value directly effect way engineer page important socialize operational information see runbook documentation meeting format demo presentation x moderator note taker tbd acceptance criteria all item complete prior closure issue google doc create todo meeting schedule agenda include google doc link scenario meeting record recording upload youtube apply video follow playlist infrastructure fire drills infrastructure group mark video private yellow classified datum share review google doc video potential follow issue need resolve
1405,112951247,3.0,for project authorization refresh job use worker base applicable urgency context base discussion grant access insert low priority accord mean delay maximum minute user actually access project gain access but revoke access block bad actor want immediate this mean opportunity different type worker thing differ urgency and base context eg addition member project vs removal member project use worker base urgency user action
1407,112170477,1.0,increase capacity large runner machine offering after test new runner manager work different machine type need update chef configure production level capacity with configuration ready official start the step announce new runner unpause gitlab available user
1408,112170462,1.0,create terraform plan large machine runner manager add new runner manager terraform repository work checklist x merge x request quota limit upgrade cpus x apply change create runner manager nodes x apply fix x manually verify configuration x finalize preparation step create docker machine vm node manually node sure configuration work generate docker machine tls auth certificate
1409,112170426,1.0,create chef role large runner machine type for new runner manager need define chef role use configuration plan checklist x merge
1412,112170229,4.0,create new gcp project new runner manager as define new shard saas linux runner saas linux medium saas linux large for working runner mean gcp project shard we need register unique cidr ephemera runner ephemeral runner subnetwork ephemeral vms later base create project create runner manager connect plan checklist x register unique cidr ephemeral runner project documentation x define new project terraform x create new project manually x new environment definition x configure new project terraform rebase merge
1413,112057532,1.0,adjust slo urgent authorize project sidekiq queue summary give context problem issue try prevent happen provide brief assessment risk chance impact problem corrective action fix assist triage prioritization newly common alert the sli sidekiq service main stage apdex violate slo because require slo adjustment should move throttle look slos call throttle look queue time execution time the urgent authorize project set urgent high look queue time affect apdex execution time look day like job complete second source zoom week bad case scenario source queue time look queue time right urgent high slo source zoom week get source conclusion the urgent authorize project create throttle number job execute concurrently saturate db it unfortunate add urgent shard urgent throttle related note originate link know related incident issue the relation happen automatically create issue incident uncomment follow line originating gitlab com gl infra desire outcome acceptance criteria how know issue complete if initial thought implementation detail gotchas edge case etc share fresh mind this urgent classified activity long complete expectation urgent work deliberately throttle low concurrency level replica it desire reclassify workload instead simply adjust slo particular queue long alert eoc what x change queue throttle inside runbook x update label urgent high urgent throttle throttle slo take consideration x gstg x gprd x create follow issue clean shard shard delete fix associated services apply appropriate service associate corrective action applicable corrective action issue checklist x link corrective action arise x give context problem corrective action try prevent occur x assign severity label high sev relate incident default x assign priority default
1414,112047375,1.0,update home page url staging this template gitlab team members seek support sre existing template available please fill detail detail point contact request if need propose date time utc additional detail format type no require sre support need update home page url staging navigate sign restriction change field to new value click save change edit
1415,102941271,1.0,bring camoproxy service cluster staging bring camoproxy service cluster staging part for detail
1416,102882061,1.0,increase machine type stage camoproxy increase machine type staging camoproxy so configuration osqueryd appear cause lead performance issue small instance this issue track work increaste machine type instance staging environment part
1417,94803334,3.0,metric dashboard improvement context this work upgrade migrate container registry new version back metadata database online garbage collection this achieve follow gradual migration plan detail task go list prometheus metric grafana dashboard registry ensure work expect display accurately this good opportunity fix generic improvement
1418,93417473,2.0,clean consul node snapshot os upgrade after upgrade console node snapshot want day probably week we want able close upgrade cr finish open pende delete snapshot this issue reminder clean snapshot reasonable time pass
1419,92826617,1.0,increase gb increase gb see
1420,92222481,2.0,upgrade gitlab service terraform the late major version terraform hit ga start plan upgrade avoid unnecessary tech debt we push change follow auto devops environment x service staging x service prod x license stg x license prd x gs staging x gs production related issue we use close procedure reference gitlab com infrastructure terraform upgrade
1421,89844435,2.0,remove un need key staging omnibus gkms vault per incident issue short term corrective action fix immediate problem omnibus gkms vault file large encrypt there go effort compress json file encryption per able remove key praefect tls help save space thing work short term specifically remove key omnibus gitlab ssl
1422,89669363,8.0,validate exist node switch boot script because update boot script require reboot stick the script introduce force node use kernel receive ubuntu advantage kernel livepatches we validate switch node rebuild reboot ac x in gstg create new instance generic stor boot script version prior ubuntu start version debatable check tf low version use x make sure run usual chef client work successfully x bulk update script version tf x reboot machine x check machine run x make sure run usual chef client work successfully checklist note work replace ok note require table thead tr th script version th xenial th bionic th focal body tr td td x ok td x ok td x ok td td x ok td x ok td x ok td td x ok td x ok td x ok td td x ok td x ok td x ok td test dev td x ok td x ok td x ok tr any failure regard epbf exporter ignore long chef client finish this know issue
1423,89033504,5.0,determine gitaly deduplication efficiency to determine use project migration api need know bad case size project ac x build script size project reference object structure x run script multiple shard gather output
1424,88756621,8.0,ubuntu advantage aware bootstrap script this issue split scope creep as rework kernel management install late kernel node bootstrap time hardcoding version sure apt holding work case force rollback kernel use livepatch security update use livepatch status security update apply reboot require the mr link add this issue document rollout bootstrap script
1425,87529364,2.0,teleport approval work remote command line the tctl command work remote session it work fine server approver laptop work the error look like client can connect auth server fail direct dial auth server get certificate valid get certificate valid fail dial auth server reverse tunnel get dial tcp connect operation time get dial tcp connect operation time be auth server run
1426,87295112,2.0,container registry define custom sli blob upload route context as break single container registry sli multiple sli api route proposal separate blob upload route sli sli route patch put delete get comment get unused see additional detail
1427,87036899,2.0,investigate setup restart policy teleport discover staging today teleport staging dead dsmith systemctl status teleport gravitational teleport access control server loaded load systemd system enable vendor preset enable active inactive dead fri utc week day ago doc main pid code exit success oddly code exit success i wonder happen look like systemd unit restart failure work case success
1428,86945122,1.0,disk space usage nfs we need repository shuffle away nfs unless trigger automatically home git repository specific threshold cross this scenario epic intend address implementation complete fully test staging test production obviously manually easily i think worth discuss good time proceed production testing automation
1429,85776055,3.0,setup ansible user fleet in order support test ansible node gcp project need target user setup connection control node ci runner to establish access exist host create data bag user chef add pre generate ssh key save for long term access remain migration long chef long perform user provisioning chef need update bootstrap module sure user exist compute instance initially provision terraform
1430,84947722,2.0,developer evangelism group project group project request project group name character start group community project administrator email jcoghlan provide brief overview reason project need long will primarily developer evangelism team testing demos content creation security provide list datum corresponding classification project access group project access checklist make sure follow criterion meet understand project administrator if database copy datum process pseudonymization script regular security update apply node project unused instance remove timely manner the project administrator responsible user additional administrator add project the project administrator responsible justify cloud spend project group project intend development test demo work everything project consider temporary infrastructure tasks x create new branch group character long for example add telemetry group x create file name copy exist file change administrator group name variable x once pipeline succeed review change correct stop review activate job x merge change master x create branch master name group push x verify pipeline complete successfully optional if group start gitlab add newly create branch protect branch
1431,82056775,5.0,poc migrate get collection format as note shift ansible code gitlab environment toolkit publish package galaxy enable use upstream dependency build specific code ideally begin shift functionality exist chef cookbook enable focus general update improvement get downstream user benefit thing highly specific deployment gcp com collection this idea fully flesh feasible incorporate current process requirement manage role fleet infrastructure
1432,81962137,3.0,update csp directive embed iframe many team create curate learn path partner content edcast platform gitlab learn impartner ssot content gitlab doc site handbook but user click web ide button handbook page iframe iframe go blank we need support embed content the definition ensure party site list embed content handbook allow csp frame ancestor directive fastly cloudflare webide ideally able test staging change production config for additional context highlight update csp directive handbook enable clickable svg object update csp inadvertently block embed iframe external party site gitlab team provide educational content partner resource etc this issue past
1433,81448025,1.0,sre onboarding template assume engineer ssh access chef step as onboarding engineer step configure chef knife require ssh user configure chef datum bag allow ssh chef server the order step update reflect dependency caveat add chef step create chef user understand ssh step need complete
1434,81337239,1.0,increase frequency analyze operation namespace table minute increase frequency analyze operation namespace table minute alvaro hernandex write current incident start cron ed analyze so probably frequent i suggest run frequently min minute it lightweight operation cause bit i o last weekend non incident profile second probably peak load week day lightweight write base let min action production incident
1435,80939112,3.0,replace customerdot zuora integration user remediate sox gap the customerdot integration user currently tie specific engineer email address the follow issue contain additinoal context provide advice email associate customerdot integration user fulfillment the ask work infrastructure team complete switch customerdot integration user fulfillment
1436,80821930,1.0,disable cloudflare onion routing setting current situation current situation intend iterate work describe the onion routing setting enable this cause cloudflare issue alt svc header tor browser it presume behaviour cause issue cloudflare spectrum trigger this make site unusable tor disable header manually client desire outcome intend outcome work represent issue how improve current situation disable onion routing setting that cause tor traffic run usual tor relay short circuit cloudflare hidden service customer challenge page afterward work normally hit issue this fix easy apply site usable albeit inconvenience user set set this upstream cloudflare use cloudflare spectrum https ingress we raise case contribute problem hidden service interfere affect behavior acceptance criteria work item necessary arrive desire outcome atomically onion routing turn gstg onion routing turn gprd
1437,80675908,2.0,hook gcp instance maintenance event during discover react imminent maintenance event hook gcp api i write little script register listen event if okay run executable file hook problem maintenance run hook maint every script event up okay event those symlink shm contain state event up if metadata api unavailable maintenance assume pass event if state save current execution instead only change state cause hook run we seek deploy we add cheap script trigger expect hook production service bash bin env bash set shm hook clean state other consume want know happen touch state truncate state local local event we return fi echo get new event event find executable hook execute time pass event or ing true prevent xargs qutte error find xargs sh event true event local echo event state cat state null return return fi metadata flavor google true response request fail echo request fail cat state null echo state register assume maintenance mode maint fi do wait change fetch value asap metadata flavor google sleep continue elif none response clear poll up response maint fi sleep for subsequent request poll metadata flavor google
1438,80645953,3.0,expose container registry database relate metric grafana dashboard relate as approach deployment new registry metadata database online garbage collection pre production need extend exist grafana dashboard registry database relate metric application metric database cluster metric the container registry metadata database enable emit follow set prometheus metric connection pool statistic count open idle use etc connection sql stat statement duration the duration single statement execute database record histogram online gc statistic statistic online gc number process task recover space etc the team try self serve change need guidance
1440,80044697,4.0,rebuilt org ci increase ip address space in effort roll discover ip space upgrade we need change cidr rebuild cluster the suggestion update gitlab gke pod cidr i estimate wo time tolerance downtime customer impact pretty simple change we need sure helm config push new cluster we need update runbook new range
1441,80016011,1.0,verify gitlab domain one trust this template gitlab team members seek support sre existing template available please fill detail detail point contact request if need propose date time date time additional detail format type additional detail sre support need support request details new txt record verifying domain one trust sso setup here link verify domain attach screenshot case link the txt token one trust onetrust domain verification alias blank edit
1442,79744643,2.0,transfer io net org domain meltano aws account as separate business unit company meltano team manage asset example website account google workspace slack zendesk sendgrid domain name bill meltano specific corporate credit card budget meltano department the exception io net org domain register gitlab aws account to meltano team control asset responsible sure expense take correct budget i think appropriate domain transfer meltano aws account id since request reject year ago meltano team independent manage infra asset ownership domain remnant early day team gitlab
1443,79703641,3.0,add deployment version info dropdown menu service grafana use case we want information deploy version service grafana right pod info dashboard service deployment version sidekiq but easy find add kubernetes detail dropdown menu service overview dashboard add deployment version information directly kube deployment detail dashboard link kubernetes detail dropdown menu i prefer option
1444,79643924,1.0,review update license version customers runbook documentation per incident as sre i able open runbooks find basic information find change customers license version service
1445,79643555,3.0,review update services base readme per incident the services base project review update well reflect intend workflow make change promote change staging production environment clean remain environment
1446,79434974,2.0,tls renewal certificate this originally set implement describe handbook look like need access route renew certificate the data team work track
1447,78997103,3.0,grow production api fleet node the api vm fleet start load place vms load core value peak use time this couple deploy lead poor apdex until api move kubernetes increase fleet size minimize customer face performance issue the outcome work deploy peak time spike single node puma worker component
1448,78727798,5.0,update packagecloud late version this template gitlab team members seek support sre existing template available please fill detail detail point contact request if need propose date time likely need additional detail format type sre support need we like upgrade late version packagecloud bring support additional package version distribution team like provide customer we like upgrade available use gitlab release omnibus issue edit
1449,78681464,8.0,deploy ephemeral sessions read only console server use virtual machines provision create push chef configuration ephemeral systemd session staging production environment
1450,78681460,8.0,deploy teleport read only console server use virtual machines provision create push chef configuration install teleport staging production environment
1451,78668132,8.0,provision read console server write terraform provision read console server include teleport ephemeral session poc once complete deploy teleport read only console server deploy ephemeral sessions read only console server
1453,78440319,2.0,fluentd pod restart cause old index log line send elasticsearch i file issue observability sre team please write clear description problem issue description acceptance criterion give issue terse descriptive title do forget put issue epic fit assign weight base perceive complexity problem space unknown see description workflow remember epic order priority necessity indicate team lead engineering manager like insert backlog problem it observe restart fluentd reprovisione daemonset pod cause log file read beginning enter old duplicated line elasticsearch this skews log base aggregation put strain elasticsearch node desire outcome fluentd make good effort attempt pick leave the feature tail input plugin configure mean allow fluentd resume previous position the pos file keep log daemonset pod mountpoint host mount namespace file write persist pod restart acceptance criterion as possible duplicate line enter elastic fluentd pod restart scenario
1454,78415198,1.0,ilb thano query frontend bypass iap grafana i file issue observability sre team please write clear description problem issue description acceptance criterion give issue terse descriptive title do forget put issue epic fit assign weight base perceive complexity problem space unknown see description workflow remember epic order priority necessity indicate team lead engineering manager like insert backlog problem we need access thano query frontend kubernete internally inside outside gke migrate grafana desire outcome a dns entry resolve private ip allow unauthenticated access thano qfe inside vpc acceptance criterion x a dns entry resolve private ip allow unauthenticated access thano qfe inside vpc fyi
1455,78093399,8.0,ci cleaner script stop work for reason ci clean script stop work error i sure error nonetheless fix my initial suspicion change naming scheme gcp api i minimal investigation far link pipeline
1456,78091731,1.0,delete extra server entry okta asa during course fix additional server entry okta asa create this unnecessary delete i access the person i know access delete server need delete
1457,78079775,2.0,revise chef server lego cron certificate renewal the chef server reload nginx new certificate instal the lego command need following add bin chef server ctl hup nginx sure happen renewal we evaluate lego schedule run day
1458,78011323,1.0,support attribute patroni service configuration support attribute patroni service configuration why the present situation production patroni host follow the command sudo systemctl stop patroni invoke production wait long postgresql postgresql database server terminate send sigkill signal force process halt untidy way this happen patroni host instruct shutdown reboot systemd service manager attempt stop unit daemon include patroni service expectation this experiment bad consequence the experiment place configuration attribute allow adjust time patroni wait postgresql database server cleanly shutdown patroni give send postgresql sigkill it not know long postgresql database server require shut cleanly production we conduct test determine interval actually possible outcome the bad possible outcome patroni take long default timeout second postgresql database server shut cleanly if postgresql database server shut end new timeout second bad but place mechanism control timeout interval configuration adjust upwards encounter new situational information host system patroni service shutdown command presently method control mechanism configuration management tooling material reference discussion discovery problem issue intend iteratively solve alternative proposal this strictly alternative option combine current approach reduce anxiety system service stop instruction it possible disable patroni systemd unit configuration use sigkill method
1459,77826560,8.0,core infra team vision discussion i discuss idea clarify expand update team vision i want kickstart discussion team sure incorporate perspective change here section note i struggle long time try figure team vision north star lie this statement continue deliver tangible value indirectly self manage customer run large scale it mention time improvement scale challenge self manage customer wo deal resonate strongly the area core infra own include area grant bit rarely clear way incorporate link self host customer outside occasional sale answer question manage incorporate focus reusability self host gitlab admin significantly help answer question clarify priority provide impetus improve quality work product chef ansible relate cookbook playbook pipeline testing deployment standardize publicly available distribute terraform module helper script pipeline definition etc frontend traffic management allow blocklist waf etc kubernetes cluster monitoring management in addition ensure focus repeatability reusability code artifact common theme i come discussion center shift mindset think service platform instead tool technology so instead attack problem manage infrastructure think terraform code update custom script i like think standard use case abstract implementation detail bit we leverage good technology give purpose backend effectively enable self service team gitlab codify mean deploy infrastructure environment narrow focused abstraction the idea enable scalability saas platform maturity distil complex codebase diverse technology simple consumable interface one example define frontend service comprise manage allow blocklist path base routing acls dns ssl certificate deal mountain option raw gcp gke api console cli option present service api cli allow user create vm project etc ensure result action properly tag follow naming convention appropriate security measure place subject lifecycle policy reduce bloat idle infrastructure for gitlab engineer look engage infrastructure intimidating present curate selection common use case web frontend internal api custom cli utility tell oh yeah check thousand line terraform chef ansible code need the easier safe people productively contribute growth management well life as thing stand operate bit service team deal fractured ad hoc request different direction simply wo scale effectively these couple point perspective what think since brainstorm thousand idea feasible important urgent try focus thing critical alleviate pain provide great value etc as concrete exercise way frame consider end goal particular timeframe work backwards fill broad stroke ex it february past year implement core webservice api associated cli use holistically manage frontend service so far manage allow blocklist cloudflare page rule waf rule path base routing emergency use band ssh credential sre successfully migrate infrastructure authn authz okta oauth integration service allow create manage vm base image report utilization environment visibility facilitate update os upgrade removal outdate image catalog establish baseline definition standard require gke cluster plan develop cli create updating validate foundational cluster configuration environment
1461,77621595,8.0,cleanup residue cloudflare cleanup terraform diff cause this involve touch module allow orange cloud domain import new page rule fix order terraform
1462,77589454,1.0,woodhouse unrelated people incident issue example it appear thought woodhouse incorrect gitlab handle incident opener issue description lead situation open issue tag we case subsequent description edit this incorrectly assume relationship user slack handle gitlab handle
1463,77573036,3.0,woodhouse mention wrong gitlab user i notice i create initial description populate woodhouse mention cbarrett username i sure identity manage slack handle pull my email cbarrett craig depend look
1464,77561597,5.0,manage repository residency audit tooling package runbook gitaly shard repository residency audit tool script ruby gem host like rubygem or roll chef recipe include gitaly specific role
1465,77490807,2.0,web pages haproxy log need visibility as eoc i able examine access request come web page service help troubleshoot incident involve web page inspire incident
1466,77485561,1.0,bump cloud native gitlab cng redis this sister issue bump redis version cng
1467,77439656,3.0,upgrade gke cluster we need upgrade gke cluster kubernetes minimum release highlight incorrect sysctl setting node potentially lead issue look upgrade note i highlight important thing note i think affect kube apiserver follow deprecate api long serve all resource app app use app instead daemonset deployment replicaset resource extension use app instead networkpolicie resource extension use instead podsecuritypolicie resource extension use policy instead sig api machinery apps cluster lifecycle instrumentation testing we need audit gitlab chart service deploy sure deprecate interface resource metric endpoint resource metric endpoint deprecate please convert follow metric emit endpoint resource sig node we need confirm rely ingress replace deprecate annotation allow associate ingress object particular controller path definition add pathtype field allow indicate specify path match incoming request valid value exact prefix implementationspecific sig apps cluster lifecycle network we check ingress object sure ok i think fine backwards compatibility bit confirmation investigation worthwhile metric change document review checklist pre upgrade check x upgrade kubectl client ci x confirm resource workload gitlab com use non deprecated apis x confirm resource workload gitlab helmfiles use non deprecated apis x confirm resource workload tanka deployment use non deprecated apis x confirm use resource metric endpoint resource migrate new endpoint x confirm ingress resource rely annotation x confirm metric change document affect upgrade x op upgrade x op nodes upgrade x pre upgrade x pre node upgrade x gstg upgrade x gstg nodes upgrade x gprd upgrade x gprd node upgrade x org ci upgrade x org ci node upgrade
1468,77506531,5.0,update wal g info runbooks this runbook information outdate update result successful implementation wal g production retire wal e we want iterate different phase result asap clean wrong info give size info runbook acceptance criteria all mention wal e remove state clearly one valid anymore update mention wal g runbook represent production moment clean section need pointer external link
1469,77227259,2.0,bump omnibus redis we want sure allow new thread io setting configure from annotate threaded i o redis single thread certain thread operation unlink slow i o access thing perform thread now possible handle redis client socket read write different i o thread since especially writing slow normally redis user use pipeline order speed redis performance core spawn multiple instance order scale use i o thread possible easily speedup time redis resort pipeline sharding instance by default threading disabled suggest enable machine core leave spare core use thread unlikely help we recommend thread i o actually performance problem redis instance able use big percentage cpu time point feature so instance core box try use i o thread core try use thread in order enable i o thread use follow configuration directive io thread set io thread use main thread usual when i o thread enable use thread write thread syscall transfer client buffer socket however possible enable threading read protocol parse follow configuration directive set yes io thread read usually thread read help note this configuration directive change runtime config set aso feature currently work ssl enable note if want test redis speedup redis benchmark sure run benchmark thread mode option match number redis thread able notice improvement we need review changelog potential bc break
1470,77160393,2.0,create lifecycle docker image container inf op the inf op runner lot disk space consume docker image container volume etc we add cron job perform routine clean up resource node operate plenty disk space new image etc minimum dod a weekly cron job clean dangle image docker image prune nice dod a elaborate cron job clean image attach container old container old volume old network consider move lib docker filesystem prevent disk usage endanger node root filesystem
1472,76948932,1.0,wrap gsm epic milestone the project launch google secrets initial set relate process effectively need wrap the milestone need update closed reference leftover follow task address individually subsequent effort fix instance level service account
1473,76890801,1.0,create new gitaly storage shard node replace nfs nfs gitaly storage shard nfs usage gitaly storage shard nfs usage our usage target specify try maintain usage new project creation quickly cause usage nfs nfs the automatic weight assignment tool soon eliminate nfs nfs candidate shard receiver new project residency note this partially corrective action incident gitaly nod remove weight pool soon it long case node remove soon initial additional node reduce instead
1474,76869540,2.0,wal g backup push log day we wal g backup push log day instead simply overwrite log file backup
1475,76838440,3.0,evaluate infracost inclusion ci terraform repository the infracost utility like useful addition standard terraform workflow template provide useful starting place incorporate addition terraform project workflow gitlab com infrastructure gitlab service environment
1476,76837913,8.0,high level planning discussion os upgrade the follow initial rough draft pro con method i sure pro con i like open input i continue consider option this list eventually determine proceed we mainly consider patroni i include i think pros cons upgrade method i consider pet service patroni gitaly other node git web ephemeral likely go easiest rebuild patroni high availability configuration upgrade bit downtime failover primary gitaly nod hand single point failure unlikely able upgrade downtime delete rebuild server i expect process terraform update base image give module apply target fashion the disk delete apply terraform reattache newly build server this preferred method redi patroni ideally gitaly pro we completely fresh server unlikely end state server come boot will able prove viable process stateful server future by rebuild implement require change gsm an excellent time upgrade machine type cons a rebuild time bootstrap available gitaly single point failure downtime method terraform dirty long take rebuild server there gitaly server probably it unnerving delete server base decision simply want express in place update with path run place upgrade time node reboot we set storage terraform module allow new server build new image try force rebuild old one this preferred method ci nodes pro do require rebuild server shorter downtime service like gitaly downtime reboot would avoid rebuild ci node fairly manual setup cons potential strange behavior upgrade go awry reason if upgrade awry recover difficult likely fast fallback rebuild plan we update machine type time bit difficult rebuild involve terraform change apply unrelated upgrade sd gitaly specific option subscribe ubuntu advantage this option subscribe advantage software security update there yearly fee associate option a subscription vm year support if want support year base want day access support respectively this software security update ideally plenty time this conjunction ignore boot disk image change storage module allow build new server new image process this preferred method gitaly pro we bit runway necessary try figure good way update potentially zero downtime we focus node need upgrade quarter the cost expensive con it cost it reasonable cost we kick road minor downside give benefit extra time we want stop build gitlab package problem upgrade switch newly build server this method involve build additional server run configure desire switch datum disk old new node these server likely come form new terraform module definition i think particularly great idea case i believe reasonable use method gitaly pro we likely gitaly update short downtime way able solve problem con we method switch traffic old node new node currently server reference dns different new server the dns change server create roll config change update location gitaly shard go long painful process high risk if want try mitigate risk create sort haproxy system order able swap traffic node this method likely substantial extra work validate planning execute switchover build new server migrate repos node new node this method involve repo migration api repos gitaly shard new i recommend method pro we upgrade gitaly nodes downtime con the migration api good undoubtedly decent number failure require investigation validation repair this age the migration repos shard slow process shard
1477,76836574,8.0,discussion high level plan chef ansible transition with chef server eol limited option community alternative overall pessimistic outlook future product community sale chef software decide begin transition ansible quarter this issue kickstart discussion gather initial requirement generate high level plan effort feed basic plan here roll high level plan additional detailed note this work progress likely continue evolve proceed implementation begin iterate roll specific portion infrastructure reorganize gitlab com gitlab com infrastructure update readme extend pipeline config gitlab com gitlab com infrastructure support ansible get implement tagging standard addition need project leverage tag filter gcp inventory this necessarily deployment consul service discovery well option distinguish new unconfigured node successfully bootstrappe develop bootstrap process plan implement corresponding terraform change support chef ansible design equivalent level playbook structure exist chef role begin migrate common base configuration role design document implement gkms gsm equivalent chef vault item rollout sequence wip leave role migrate bastion gitaly praefect patroni redis console consul develop play current configuration chef cookbook role cleanup unused chef stuff role cookbook datum bag archive project gitlab decommission remove monitoring backup audit cleanup documentation note inventory early i consider consul inventory source later realize encounter circular dependency bootstrap node register consul visible ansible target run playbook register consul while expand use consul service discovery useful area appropriate case use consul pivot point successfully bootstrappe node appropriate inventory source deployment which bring google cloud compute engine inventory source this plugin allow dynamically build inventory base filter match resource gcp environment we currently rely chef inventory source deployment deployment tooling new ansible infrastructure need new source truth inventory it clearly make sense shift gcp filter tag base approach bootstrappe value assign terraform provide clean method handoff terraform ansible we plan implement formal tagging standard leverage for deployment leverage consul high level inventory source appropriate node register service catalog subject health liveness readiness check indicate deployment specific node appropriate newly bootstrappe node piece fully configure subsequent ansible run bridge gap new node process bootstrappe deployment incorporate functionality prevent situation deployment workflow implement tagging standard facilitate update instantiate inventory new source truth gcp thing the current chef inventory keep duration project ie minimal bootstrap run list ultimately new inventory need understand fully integrate end project ability easily disable remove chef shift ansible similarly develop play register service consul develop corresponding health check follow immediately framework place adopt shift chef chef inventory ansible pull vs push model from architecture standpoint fundamental difference chef ansible communication paradigm while ansible support pull model certainly consider encounter scaling challenge default start assumption documentation example tutorial small scale example rely push model ssh this deployment conduct simple alignment overwhelming mass documentation example probably start in addition intrinsically relate effort benefit area leverage ansible build pipeline golden image dynamically scale infrastructure mind work project because agentless nature ansible lend approach likely project initial switch management orchestration as i start read ansible question i need want leverage awx tower after skim main product page i compelling need case especially base success gitlab primary feature list deployment recently codify automate database operation task feature gitlab equivalent ansible dashboard metrics export prometheus grafana dashboard real time job status update gitlab ci jobs ansible tower workflows gitlab ci pipelines activity stream auditing gitlab ci pipelines repository history ansible tower cluster n a integration gitlab ci pipelines webhook integration ansible tower smart inventories n a suffice need ad hoc job gitlab ci jobs remote command execution gitlab ci jobs teleport rest api gitlab api secret with recent completion initial setup google secrets manager begin coordinate shift begin service i dig sufficient detail initial work update instance service account need coordinate effort os upgrade in addition update instance service account need functionally replace exist tooling manage secret gkms apply chef vault i explore depth definitely case dragon inherently vague project structure as see time break similar codebase modular asset multiple repository enable flexibility consumer modular code inherently bring certain add complexity management pain i thought address project first recently announce gitlab environment toolkit develop qa team effectively provision gitlab environment demand testing we need evaluate include ansible code project clear overwhelming overlap effort absent insurmountable complication try avoid fork effective use share resource consider fork maintain descendent project absolutely necessary alternatively attempt cherry pick portion ansible code apply situation work separate project get infrastructure tooling inherit in case i hopeful close examination show effective mean share infrastructure provision code gitlab broad community second possibly independent i think ansible tooling need completely separate current terraform code consolidate project gitlab com infrastructure bring multiple benefit it provide reduce complexity ease management monorepo easy coordination change ansible terraform code bootstrappe instance manage instance group provide single place core infrastructure code tooling store manage document consolidation terraform code monorepo adherence feature flag code change necessarily include approach benefit improved consistency infrastructure code certainly desirable implicit approach in good case consolidation easy find portion codebase easy thing consistent definitely reach this idea completely flesh implication thing like ephemeral gitlab service environment conceivably consolidate reason fair bit difficulty differ deployment pattern additionally centralize significant privilege security risk single project maintain good code hygiene proper access control critical while i favor second point significant decision i fully fully flesh note benefit feedback broad team if decide share infrastructure repo right approach i strongly favor keep ansible code tool monorepo regardless if clear need distribute code consumable format outside team outside gitlab revisit choice later team immediate usage i think benefit far outweigh drawback disaster recovery when i approach conceptualize i think far centralized model chef lean implementation awx tower once i work table easy need model with mind need structure project locate correspond ci pipeline job way chef terraform today ops instance mirror thus need look dr capability underlie gitlab instance host project ci pipeline other outstanding questions topics there aspect i think i touch service ness so far squarely realm implement tool correspond technology stack as central server define interface interact tool directly access code automation trigger run ci i hesitate push standard production readiness review time reference template ensure check box relevant bit apply documentation architecture diagram monitoring log alerting etc observability this area interesting challenge compare current system with ansible agent system monitoring likely need bake ci job handle hook external service runtime there central server collect emit log traditional log infrastructure likewise metric
1478,76834477,1.0,verify work database backup it unclear functional backup database
1479,76829474,8.0,improve wal g backup job alert the wal g backup job metric cause bogus alert backup run random node time metric look like backup day single node interested successful backup job env day this track incident issue the fix add issue track milestone acceptance criterion x do cause wal g backup delay false alert single patroni nodes x runbook doc use central pushgateway job metric environment focus
1480,76824777,8.0,migrate new subscriptions management application production fleet this issue likely spawn production change issue migrate datum database switch dns
1484,76824713,5.0,create new deploy process ansible chef a new recipe customer cookbook set right credential ansible a new project mirror customer code base perform stage deploy production deploy master
1485,76824698,3.0,create new subscription stage terraform environment tcp load balancer health check static ip start single node begin bastion host restrict outgoing incoming traffic
1486,76824677,1.0,create new chef role new staging fleet stg subscription start role basic possible no customer cookbook start
1487,76824670,2.0,create new google cloud project stage gitlab subscription staging a new project subscription management app create the intent have project build strong security barrier environment customers infrastructure consider module manage new project
1488,76795255,2.0,set slack integration access request workflow use instruction gravitational need install teleport slack application this app relay access request production access channel slack authorize user able review approve currently proof concept stage consider development minimal change production ready
1489,76531461,1.0,current gitlab org dockerhub plan expire jan this template gitlab team members seek support sre existing template available please fill detail from docker docker team plan start user month provide access advanced feature capability help team automate development workflow increase productivity include unlimited private public repository unlimite authenticated image pull parallel auto build role base access control unlimited team unlimited hub image vulnerability scan and you receive email legacy organizational repository plan expire january billing cycle date read faq cc i think legacy thing dockerhub i owner cc case input organization edit
1490,76476214,2.0,adjust slo calculation apple apple time window comparison because i propose adjust slo calculation compare similar time window error request otherwise wind see alert underlie omni present error rps reduce
1491,76196529,3.0,gitlab hosted version codesandbox sandpack fork overview as need upstream contribution codesandbox sandpack package smooshpack unfortunately change land upstream package upstream availability after receive approval fork package apply change gitlab project publish new npm package gitlab smooshpack for use feature flag need gitlab fork package asset host separate bucket currently codesandbox package asset host proposal similar step original package specific asset need host download package tarball wget npm view gitlab smooshpack extract sandpack folder contain pre build asset tar package sandpack the asset host sandpack then drop file sandpack asset host bucket requirement suggested domain ssl enable links references legal issue fork
1492,76158340,1.0,export version license db import data warehouse runbook hopefully time
1493,76148299,5.0,geo secondary staging send email after promote geo secondary node staging run test trigger send email email receive we check email configuration geo node fix document necessary setting
1494,76148097,3.0,check google auth config geo staging during staging failover test promotion secondary skarbek able log geo node google auth error we need check google auth setting sure google auth work failover
1495,76147615,4.0,make sure work geo failover during stage geo failover test step promotion fail geo attempt authenticate postgre connection match line host gmt fatal password authentication fail user gitlab gmt detail password match user gitlab it temporarily fix add host trust line mention error opt gitlab postgresql data gitlab ctl restart postgresql get overridden time gitlab ctl reconfigure we need fix work primary secondary site failover maybe need change db password gitlab role
1496,76099203,1.0,ensure accurate domain contacts gandi i work domain gandi recently notice contact old people long instance we ensure accurate
1497,75967749,1.0,license app datum export credential configuration issue description base similar issue request relate issue for automate export table license gitlab com project pipeline schedule need configuration parameter able run properly configuration screen credential variable value gcp service account auth json file gcp project d api enable project the region zone gcp compute bucket the gcs bucket shall upload sql export instance cloud sql instance database cloud sql database
1498,75826476,8.0,fix multiple wal g backup push failure tonight gcs failure show multiple problem wal g backup push gprd backup run fail tonight reminder we set currently wal push block long error backup fail miss cleanup composite part apparently gcs fail moment upload compose wal g try delete composite part dec info finish write dec info start dec info finish write dec info start dec info finish write dec error fail run retriable func err storage object exist retry attempt dec error fail run retriable func err storage object exist retry attempt dec error fail run retriable func err storage object exist retry attempt dec error fail run retriable func err storage object exist retry attempt dec error fail run retriable func err storage object exist retry attempt dec error fail run retriable func err storage object exist retry attempt dec error fail run retriable func err storage object exist retry attempt dec error fail run retriable func err storage object exist retry attempt dec error fail run retriable func err storage object exist retry attempt dec error fail run retriable func err storage object exist retry attempt dec error fail run retriable func err storage object exist retry attempt dec error fail run retriable func err storage object exist retry attempt dec error fail run retriable func err storage object exist retry attempt dec error gcs error unable delete temporary chunk gcs error unable delete temporary chunk context deadline exceed dec error upload upload dec error gcs error unable delete temporary chunk gcs error unable delete temporary chunk context deadline exceed dec error unable continue backup process loss i think wal g manage delete chunk gcs maybe return failure delete chunk chunk actually delete wal g try delete chunk get object exist deadline kick and backup fail i check actually chunk delete henris gsutil ls gprd postgre backup pitr walg gprd postgre backup pitr walg total object byte mib henris gsutil ls gprd postgre backup pitr walg gprd postgre backup pitr walg gprd postgre backup pitr walg gprd postgre backup pitr walg total object byte mib suggestion when clean fail object exist error print warning fail clean chunk fatal error stop backup backup fail gcs outage short gcs context timeout the backup restart eoc fail multiple part time gcs problem maybe m context timeout able retry long survive so fault wal g case dec info start dec warning unable copy object chunk err googleapi get http response code body dec warning unable close object writer err g oogleapi get http response code body dec error fail run retriable func err googleapi get http response code body retry attempt dec info finish write dec info start dec info finish write dec info start dec info finish write dec info start dec info finish write dec info start dec warning unable copy object chunk err googleapi get http response code body dec warning unable close object writer err googleapi get http response code body dec error fail run retriable func err googleapi get http response code body retry attempt dec warning unable copy object chunk err googleapi get http response code body dec warning unable close object writer err googleapi get http response code body dec error fail run retriable func err googleapi get http response code body retry attempt dec warning unable copy object chunk err googleapi get http response code body dec warning unable close object writer err googleapi get http response code body dec error fail run retriable func err googleapi get http response code body retry attempt dec warning unable copy object chunk err googleapi get http response code body dec warning unable close object writer err googleapi get http response code body dec error fail run retriable func err googleapi get http response code body retry attempt dec info finish write dec info start dec warning unable close object writer err post context deadline exceed dec error fail run retriable func err context deadline exceed retry attempt dec error gcs error unable compose object context deadline exceed dec error upload upload dec error gcs error unable compose object context deadline exceed dec error unable continue backup process loss backup fail close writer chunk fail this fully clear log contain information retriable func upload compose cleanup fail but assumption close writer chunk fail retry close writer fail upload chunk exist try compose error timeout dec info finished write dec warning unable close object writer err g oogleapi get http response code body dec error fail run retriable func err googleapi error object pitr walg generation find notfound retry attempt dec error fail run retriable func err googleapi error object pitr walg generation find notfound retry attempt dec error fail run retriable func err googleapi error object pitr walg generation find notfound retry attempt dec error fail run retriable func err googleapi error object pitr walg generation find notfound retry attempt dec error fail run retriable func err googleapi error object pitr walg generation find notfound retry attempt dec error fail run retriable func err googleapi error object pitr walg generation find notfound retry attempt dec error fail run retriable func err googleapi error object pitr walg generation find notfound retry attempt dec error fail run retriable func err googleapi error object pitr walg generation find notfound retry attempt dec error fail run retriable func err googleapi error object pitr walg generation find notfound retry attempt dec error fail run retriable func err googleapi error object pitr walg generation find notfound retry attempt dec error fail run retriable func err googleapi error object pitr walg generation find notfound retry attempt dec error fail run retriable func err googleapi error object pitr walg generation find notfound retry attempt dec error fail run retriable func err googleapi error object pitr walg generation find notfound retry attempt dec error gcs error unable compose object context deadline exceed dec error upload upload dec error gcs error unable compose object context deadline exceed dec error unable continue backup process loss suggestion make sure retry fail close writer upload maybe override err return var defer func work expect
1499,75788120,1.0,redirect the secure sub dept complete acquisition step redirect fuzzit website gitlab my question infrastructure handle redirection want redirect specific page please provide guidance forward
1500,13305380,1.0,pgbouncer gprd invalid user show pools while test pgbouncer exporter gprd i notice fail hard golang column user datum ˇˇˇˇc panic label value valid goroutine run stanhu src prometheus prometheus stanhu src larseen stanhu src larseen stanhu src larseen stanhu src larseen stanhu src larseen stanhu src prometheus prometheus create if look show pools c i check production i wonder garbage come sql root sudo gitlab psql gitlab embed bin psql pgbouncer pgbouncer psql server bouncer type help help pool database user maxwait chatop transaction gitlab transaction transaction pgbouncer transaction gitlab transaction pgbouncer transaction c transaction pgbouncer pgbouncer statement row
1501,13305236,3.0,migrate artifact gcs we need migrate artifact object storage gcs this add benefit allow artifact much work we sync gcs bucket daily keep date the process complete migration follow disable artifact run final sync gcs artifact bucket change storage location point gcs verify artifact work enable artifact migrate artifact land disk object storage
1502,13304901,3.0,migrate sentry gcp once primary failover complete migrate sentry azure gcp
1503,13305333,4.0,create canary gprd gstg we canary environment gprd probably i think nice pre failover possible post failover milestone here summary current proposal do use new chef environment use exist gprd gstg environment have option create gstg cny test configuration change gstg gprd the cny name web api gstg first iteration single web node mean follow new vms we run migration canary deploy to identify prometheus use new tag stage canary will need takeoff adjustment canary long environment once canary accept traffic cookie set browser add weight haproxy configuration small percentage production traffic canary for work need x create new share terraform config canary x create new chef configuration canary role gstg gprd x modify takeoff deploy canary x create canary specific dashboard s create canary specific will track separately start move prod traffic vm s update gitlab haproxy config production traffic direct canary need we track separately
1504,13305386,4.0,monitoring metric aggregation ci cd infrastructure gcp there additional work bring prometheus server ci cd team use parity work azure in general well prometheus server iap like current one add prd chef environment
1505,13305349,1.0,update general information configuration this page appear place document general information setting update setting update environment documentation
1506,13305238,2.0,add config sentry node gstg gprd config
1507,13305171,4.0,drop influxdb entirely this issue capture effort remove influxdb infrastructure prometheus metric good obviously post gcp migration more conversation check tickbox satisfied ci runners cc gcp migration project cc gitaly cc gitlab rail cc ha proxy cc operation cc x pages cc x postgre cc prometheus cc redis cc x workhorse cc fyi folder exist broken dashboards rely influxdb currently work once once dashboard manually migrate dashboard clean delete
1508,13305343,1.0,lower size besteffort nod the current besteffort sidekiq node gprd absolutely massive they scale pretty substantially
1509,13305399,4.0,fluentd parse error gitaly log maybe server look monday warn dump error event fail empty input line column sched td agent embed lib ruby gem lib fluent plugin rescue warn dump error event fail empty input line column sched td agent embed lib ruby gem lib fluent plugin rescue warn dump error event fail empty input line column sched td agent embed lib ruby gem lib fluent plugin rescue warn dump error event fail empty input line column sched td agent embed lib ruby gem lib fluent plugin rescue warn dump error event fail empty input line column sched td agent embed lib ruby gem lib fluent plugin rescue warn dump error event fail empty input line column sched td agent embed lib ruby gem lib fluent plugin rescue for i clean tf agent log longterm need visibility alert fluentd state time disappoint knife ssh role gitlab base stor nfs sudo du log td agent sort m log td agent m log td agent m log td agent m log td agent m log td agent m log td agent m log td agent m log td agent g log td agent g log td agent m log td agent m log td agent m log td agent m log td agent m log td agent m log td agent
1510,9846487,5.0,should serve object storage raise concern redundancy node
1511,13305409,1.0,create prometheus exporter elastic cloud currently metric graph admin panel query directly we setup dedicated ha node prometheus elasticsearch exporter point elastic cloud scrape monitoring this potentially big issue split
1512,13305417,1.0,create pipeline back restore kibana setting currently password base administrative access kibana es cloud cluster we need create backup restore procedure kibana configuration add lot lose mistake
1513,13305422,1.0,write documentation runbook elastic cloud setup currently apart cookbook documentation elastic cloud setup this issue track
1514,13305433,2.0,migrate exist elastalert new elastic cloud cluster as current setup elastic cloud exist integration new infrastructure
1515,13305425,2.0,stop current logstash pipeline tear exist elastic cluster this issue track cleanup progress stop current pipeline tear current infrastructure
1516,13305222,4.0,migrate production servers we handful server notably require production usage gitlab address move stand replica gcp
1517,13305164,4.0,create runbook test manual process update staging database production we need periodically update staging database pseudoanonymization customer datum once complete step automate process
1518,13304990,1.0,copy backup configuration production all backup customer datum production duplicate staging
1519,14000565,4.0,meta structured json log postgresql structured logging easy ingest search log in particular useful search slow query spread multiple line we investigate option deem possible useful implement there step investigate deploy change x build jsonlog module manually install staging x decide destination path log x evaluate log output fix deficencie x x x x x configure elk stack ingest log x ensure log rotate clean retention period view integrate jsonlog omnibus wait omnibus jsonlog deploy production unanswered question can use mtail alternate way metric log
1520,7367111,5.0,meta new geo testbed update plan small issue azure x create terraform configuration azure x create new chef environment geo testbe x setup nfs storage geo testbe azure copy nfs shard x setup uploads attachments geo testbe azure x setup database pseudo anonimize contain project nfs shard x setup monitoring alarm azure testbe x setup automatic deploy azure testbe azure x create terraform configuration azure x create new chef environment geo testbe x setup nfs storage x setup uploads attachment x setup database disk config x setup monitoring alarming x setup automatic deploy original issue per discussion day ago like new geo testbe upgrade exist gitlab run ha mode primary secondary web worker load balancer sure geo code functionality bug ha setup
1521,6618867,5.0,set alert http queue timing carry while anticipate well way measure web request response time integration prometheus wait set alert http queue timing once timing slo i propose issue runbook tie alert describe add unicorn host step obviously reason high http queue timings start propose slo http queue timing ms propose alert alert ms minute
1522,6091522,2.0,sync app server zendesk the person contribute openid connect feature report return different key request presumably app server private key sync we sync node see implement
1523,5664260,5.0,create terraform gym as follow great create terraform gym fiddle tool isolated environment this allow team member learn experiment complete freedom platform production risk hurt to create separate azure account decide want limit exercise pure infrastructure spend time chef server set the allow coverage way provision server moment discussion comment let continue
1524,5424965,5.0,automate staging environment creation follow agree need way create staging environment test new feature this mean intermediate step remain ultimate long term goal the way achieve short mid term leverage terraform creation node this exist effort well align staging production environment configuration code to scalable number different staging environment need define schema chef role like ask deal monitoring fundamental component gather datum change would use global influxdb prometheus server should use staging environment should create last database we probably need use copy staging datum set if use staging database schema stale real quick also good opportunity test migration control way plan
1525,4737052,1.0,custom robot recipe apply multiple line the gitlab robot recipe apply disallow line multiple time production symptom tail gitlab embed service gitlab rail public disallow disallow disallow disallow disallow disallow disallow the issue probably regex disallow
1527,2472686,5.0,redirect blue moon we need create new node chef control hold redirect domain old domain point blue moon ip address redirect current website blue moon hold status site to discontinue blue moon need move status separate node provision new node host redirect update ip domain list nginx site make sure redirect work cc
1528,2472662,3.0,send operation notification email failure migrate chef repo for long time i read cron email etc operation notification email create issue thing look interesting important because i move op i decide i stop i read op notification email anymore
1529,15802836,1.0,gitter ssh proxy bastion fail connect box run ansible command gitter ssh proxy bastion fail connect box run ansible command it especially happen prod inventory lot box connect example ansible command add ssh key example another example i try add ssh key look play recap cc
1530,15802770,2.0,add runner capacity we currently runner dedicate release dedicate chatop the i think nice switch kubernete we probably create dedicated runner traffic generation the runner cookbook design multiple runner want minor update ideally scale runner type horizontally different token
1531,15797526,1.0,rake task rake appear work in setup new sre need execute rake task rake update vault user new access vault this appear work the vault update new user a quick look indicate task work
1532,15795660,2.0,meta move backup cicd job need operation switch pushgateway monitoring we probably want split follow small task x move gitlab com gl i sure gitlab restore group gitlab com mistake sure x mirror repository gitlab restore push rule project esc tool x move cicd configuration this mean use source code cicd run op replace deadmansnitch pushgateway notification alert
1533,15784725,5.0,update status page chatop it notice update status page one reason job currently manually moc overlook this need easy status update post twitter status page likely place wide community check go
1534,15765309,1.0,assign privilege admin account accord infrastructure onboarding issue provide need privilege admin account
1535,15759359,2.0,access to sidekiq rails logs for staging i try read log stage sidekiq worker job i example log i sure trigger staging log appear kibana i try search kubernete error pubsub sidekiq inf gstg show day i trigger time if forward log kibana i need access system log necessary troubleshoot issue staging i able determine log available kibana production deploy code production assume available production kibana i need access production log i read
1536,15755845,2.0,rollout prometheus prometheus now include sample flag limit memory query use x decide max sample limit x rollout staging x rollout op x rollout default
1537,15752769,1.0,log ddl statement txid in order recover drop table ddl relate incident data loss employ pitr replay right incident happen know txid incidental transaction enable precisely replay transaction incident without know txid replay certain point time loss induce early render restore unusable late so proposal log ddl statement ddl sure log prefix contain transaction d
1538,15711421,1.0,change label title because strive use violent analogy culture language need update label outage postmortem i recommend change outage rootcause outage analysis sake brevity i find label i believe i permission necessary edit
1539,15707301,3.0,add ssh key amar skarbek gitter host add ssh key gitlab com gl infra gitter gitlab com gl infra gitter need current admin run ansible distribute fleet
1540,15745749,1.0,ingest structured audit log elk with structure audit log come rail tada thank these log flow stackdriver we ingest elk once able use lot abuse detection activity way gitaly present cc cc schedule
1541,15646862,1.0,slack chatop run explain access groulot access explain database ref
1543,15613204,2.0,add alert increase active session action add alert large increase active session
1544,15612110,1.0,error web editor staging this relate sure to reproduce find repo click file repository view click edit file i time the show json rouge test update master macintosh intel mac os x khtml like gecko workhorse log show json correlation macintosh intel mac os x khtml like gecko
1545,15612015,5.0,test plan test patroni replication environment test qa patroni environment team sync replication testing quality team help functional test see project ci cd work mr work diff display correctly performance test flight automation load testing use gcp migration follow up meeting update issue env detail quality team work turn issue test plan recording plan run exist automate test new environment run new performance test automation run load test
1546,15611809,4.0,review patroni setup cookbook review totally cookbook environment setup variable patroni test environment create gitlab
1547,15608505,2.0,vacuum full bloat statistic so run vacuum full production datum restore backup the goal figure bloat currently database i export table level size statistic fact analyze well
1548,15605562,1.0,plan monitoring visibility geo create list issue validate properly monitor alert geo infrastructure consider following ensure exist monitoring solution sufficient ensure firewall rule place properly scrape metric x ensure logging place ensure alert route properly geo x ensure sentry correctly configure geo this issue close issue create
1549,15605056,1.0,plan enable geo plan require enable geo consider following should phase sync approach briefly discuss this issue close creation necessary story enable geo feature complete
1550,15604893,5.0,build geo environment utilize learn discovery proceed build necessary infrastructure choose region this issue close component geo this issue include enable geo
1551,15604805,1.0,request quota increase x region after decide region build geo proceed contact google support increase quota require discovery
1552,15604761,1.0,discover recommend installation parameter geo gather list following build geo conversation start instance size requirement disk type requirement this issue discussion discovery keep mind follow work issue we plan sync repo database datum the region place stuff customer traffic consider place reduce cost expensive infrastructure component
1553,15604577,1.0,discover current geo implementation despite geo currently disable server run configuration relate figure server geo currently compile list action item tear anything leave right purpose migrating gcp neglect this issue close issue create
1554,15600024,1.0,postgresql europe conference summary this quick recap good talk i visit pgconfeu lisbon october year i go talk obviously note i the conference schedule contain link slide my talk hyperloglog algorithm counting index type pg spatial analysis postgis training dig wal slide description in half day training look wal detail use recover certain catastrophic situation the speaker automate example recover cluster pitr understand conflict logical replication recover split brain situation apart high level discussion away log ddl statement txid tremendously helpful group commit considerably reduce fsyncs require useful script psql talk location universal foreign key past present future spatial postgresql slides description the talk give paul ramsey beautifully manage build bridge database folk gis land the spatial analysis example awesome example correlate starbuck location income datum if work spatial datum how spatial datum how visualize gqi how load analyze postgis personally good reminiscence bachelor thesis cover different spatial database talk clean crocodile tooth postgresql index story index type pg description great talk louise grandjonc citus data cover implementation detail b tree gin gist sp gist brin hash index compare talk the hyperloglog algorithm how work love description theory hyperloglog algorithm implement efficient query postgresql if wonder hll work talk read slides talk postgresql bad practice description countless bad practice present database consultant excerpt bingo play disallow access production database developer not read always use orm count precisely like place gitlab counting hard scale anti pattern implement scale related issue discussion in memory join be trend schemaless use jsonb be agile use eav talk advanced logical replication description this talk detail implement transactional message queue logical decoding postgresql base this relevant need geo design great way introduce transactional queue introduce dependency like kafka the talk cover online upgrade relevant bi directional replication talk around world with extensions description great overview available extension include cool stuff like hyperloglog count top n sharde citus talk high performance pgbackrest description details pgbackrest implement parallelism backup restore claim able push tb worth wal hour async parallel push network keep this relevant discuss replace wal e reason design pende talk advanced postgresql backup recovery method description great overview different backup recover tool available postgresql compare
1555,15583051,2.0,introduce small traffic stage cripple environment a small load staging environment web req sec api req sec git req sec start add load cause problem api git gitaly particular
1556,15582965,5.0,database reviews last milestone x x sean x douwe x x x x x x x x x recursive cte x sean x douwe x x sean x
1557,15582728,1.0,error deploy chef handler action info processing chef handler action create gitlab line date action info processing action enable gitlab line info enable prometheushandler report handler info enable prometheushandler exception handler date warn this cookbook write use resource recipe chef compatible please version pin prevent break change take effect see detail warn this cookbook write use resource recipe chef compatible please version pin prevent break change take effect see detail warn this cookbook write use resource recipe chef compatible please version pin prevent break change take effect see detail recipe compile error chef cache cookbooks cookbook customer gitlab com recipe nomethoderror undefined method nil nilclass cookbook trace chef cache cookbooks postgresql recipe block chef cache cookbooks postgresql recipe chef cache cookbooks postgresql recipe chef cache cookbooks postgresql recipe chef cache cookbooks cookbook customer gitlab com recipe chef cache cookbooks cookbook customer gitlab com recipe relevant file content chef cache cookbooks postgresql recipe this cookbook write use resource recipe chef compatible please version pin prevent break change take effect see detail true uri distribution component main key action add end platform linux running handler error run exception handler prometheushandler run handler complete error exception handler complete chef client fail resource update second fatal stacktrace dump chef cache chef fatal please provide content file file bug report error undefined method nil nilclass fatal chef run process exit unsuccessfully exit code
1558,15578262,2.0,research improvements docker registry monitoring alerting log registry come elastic search unstructure increase time take sift log we runbook registry if happen detail look troubleshoot diagnose issue utilize issue research area improve overall stature monitor alert issue relate docker registry bit infrastructure create issue completion research address potential area improvement appropriate acceptance criterion fit
1559,15572063,1.0,iterate snapshot timing hour continue let bring time snapshot hour instead current
1560,15571885,1.0,request admin access reuben pereira per edit tag proper person provision
1561,15567470,1.0,rake task update vault misconfigure example run rake gitlab org bundle exec knife vault edit dev gitlab org default client bundle exec knife vault update dev gitlab org default dev gitlab org or role dev gitlab org and ahanselka ahmadsherif alejandro jarv jjn stanhu yorickpeterse dsmith devin skarbek craig client warn no client return search get expect switching master sec pull git sec bundle exec knife download dev gitlab org updated dev gitlab updated dev gitlab git add dev gitlab org git commit dev gitlab org aborting commit commit message fail git commit dev gitlab org it pull dev pull op it properly search role dev gitlab org the dev node clearly role show knife node either fix rake task appropriately update documentation remove rake task reference
1562,15563093,5.0,design doc implement patroni the doc detailed info
1563,15543857,1.0,pagerduty sslcert expiring soon summary issue we get page follow incident certification expire gmt investigation accord runbook run command look certification detail echo openssl null openssl pem certificate datum version serial number ba ba signature algorithm issuer c gb st greater manchester l salford o comodo ca limited cn comodo rsa domain validation secure server ca validity not before nov gmt not after nov gmt try renew get error sslmate renew if account visit enter sslmate username gitlabop enter sslmate password authenticating do tip want type password time run sslmate link link system account error certificate expire tip reissue certificate run sslmate reissue tip use override error so sure need force wait will check team practice time root cause this issue a pre cautionary alert action items renew certificate
1564,15520640,1.0,access production database hello i like read access secondary production database run explain tricky query issue can set access like the hostname response exist anymore i guess access survive gcp here ssh public key ssh rsa groulot
1565,15517988,2.0,root device postgre primary run space the device gb size fill completely since disk cause problem postgre backup i weak suspicion direction can increase size gb postgre host be possible online resize gprd primary even indicate go mean it take second write gbs worth datum hit process fail clean notice prometheus
1566,15517511,1.0,create env stage equal production patroni create cluster postgresql pgbouncer consul agent server ilb use cookbook terraform possible change review ongre review environment progress
1567,15513612,2.0,wal e backup occasionally fail this surface move backup gcs apparently case it look like wal e crash reason middle backup recover
1568,15508112,1.0,redirect developer page backend engineer page after need redirect family engineering family engineering backend engineering career development junior engineering career development junior
1569,15502473,1.0,trigger fire cpu use percent extremely high past hour summary issue cpu utilization alert trigger investigation bunch wal e process appear take cpu cpu utilize time spike time in postgresql log error indicate absolute uri able load cause cpu jump a pd incident fire early postgre dr delay node delay replica node consume high cpu could relate decide wait poc emea look for detailed investigation comment proximal root cause the uri load cause retrie subsequent task step complete properly cause loop consequently cause high cpu utilization since definite root cause proximal mitigation the alert clear but suppress alarm reason pd action items monitor cpu alarm go symptom log suppress alert alertmanager inform poc emea incident thing help look root cause
1570,15501330,1.0,alert we get alert production it appear delay replica try use wal e replication slack thread
1571,15494176,1.0,staging high error rate docker registry we see increase error load balance backend http request minute check registry node one process live traffic moment
1572,15493622,1.0,complete phase end support note issue to complete nov put date remind early
1573,15491965,2.0,redirect as follow website ia need redirect place include team page the list redirect google sheet cc although js redirect place ensure people page small delay google pick route change for example page show move a redirect reliable way communicate change search engine example show incorrectly serp
1574,15491641,2.0,add gitlab bot triage infrastructure production issue queue here rule i think implement start gitlab bot rules incident issue severity label incident issue attribution label service infrastructure issue weight comment maybe close issue xxx day innactivity
1575,15466888,1.0,error deploy recipe compile error find recipe server cookbook postgresql cookbook trace chef cache cookbooks cookbook customer gitlab com recipe chef cache cookbooks cookbook customer gitlab com recipe relevant file content chef cache cookbooks cookbook customer gitlab com recipe cookbook name cookbook customer gitlab com recipe database license mit copyright gitlab gitlab vault cookbook customer gitlab com package libpq dev bash create database user user postgre code psql user with password sudo su postgre grep platform linux running handler error run exception handler prometheushandler run handler complete error exception handler complete
1576,15453153,1.0,unable publish new version gitlab postgresql when try publish gitlab postgresql cookbook fail the chef repo pin berks file in process update berksfile chef repo i unable resolve dependency conflict berk actually tell conflict lie end result inability satisfy long list requirement reference
1577,15450766,1.0,miss trace sidekiq dashboard the dashboard sidekiq stats miss panel specific trace server reference
1578,15440789,2.0,high cpu web fleet release the load web fleet increase this deploy gprd current state alarm example for ruby process cpu perf record sleep perf record wake time write datum perf record capture write mb sample perf script
1579,15421785,1.0,daily db restore job fail error could fetch resource quota exceed limit region
1580,15420977,1.0,change backup gitaly fleet actually execute backup the idea execute hour initially iteration hour in case disaster reduce datum lose effort small speak jarv any thought
1581,15413433,1.0,redirect ci page redirect gitlab ci continuous there js redirect place today key page need asap
1582,15412354,1.0,gitalylatencyoutli alert staging the follow alert come staging repeatedly it stay minute resolve if problem fix if fix alert gitaly latency gitaly listbranchnamescontainingcommit unusually high compare hour average the error rate listbranchnamescontainingcommit endpoint outside normal value hour period confidence check gitaly latency gitaly listbranchnamescontainingcommit unusually high compare hour average the error rate listbranchnamescontainingcommit endpoint outside normal value hour period confidence check label label alertname gitalylatencyoutlier channel gitaly environment gstg listbranchnamescontainingcommit monitor gstg default provider gcp region east severity warn
1583,15412296,3.0,add version constraint cookbook publisher per discussion comment chef cookbook publisher script currently insert ci pipeline simple git clone version lock mechanism place ensure change tooling inadvertently break downstream cookbook pipeline that publisher script repackage gem include gemfile appropriate version pin implement equivalent control tag reference git clone etc
1584,15410731,1.0,redirect culture remote culture i open merge request rename directory culture remote culture before merge preferably i like setup redirect affect url redirect redirect thank advance help reference
1585,15410708,1.0,staging alert stage postgres consume transaction id slowly txid s unusually low perhaps application unable connect possibly relate
1586,15410692,1.0,walebackupdelaye staging alert walebackupdelaye possibly relate
1587,15410319,1.0,disk full db gstg the disk db gstg there bunch old tmp file gig i delete old usage there leftover effect i eye
1588,15407040,2.0,pullmirrorsoverduequeuetoolarge staging state check mirror dashboard datum staging updateallmirrorsworker entry i check since staging customer job i clear set states return following import time import take long second import time import take long second nil nil error lock ref ref remote upstream unable create opt gitlab git data repository ithkuil remote upstream no space leave lock ref ref remote upstream site unable create opt gitlab git data repository ithkuil remote upstream no space leave lock ref ref remote upstream site unable create opt gitlab git data repository ithkuil remote upstream site no space leave error lock ref ref remote upstream master unable create opt gitlab git data repository rolisoft remote upstream no space leave nil error lock ref ref remote upstream unable create opt gitlab git data repository viktorbodrogi remote upstream no space leave lock ref ref remote upstream unable create opt gitlab git data repository viktorbodrogi remote upstream no space leave lock ref ref remote upstream large screen unable create opt gitlab git data repository viktorbodrogi remote upstream large no space leave lock ref ref remote upstream lessnew unable create opt gitlab git data repository viktorbodrogi remote upstream no space leave lock ref ref remote upstream localdraft unable create opt gitlab git data repository viktorbodrogi remote upstream no space leave lock ref ref remote upstream mail unable create opt gitlab git data repository viktorbodrogi remote upstream no space leave lock ref ref remote upstream unable create opt gitlab git data repository viktorbodrogi remote upstream no space leave lock ref ref remote upstream master unable create opt gitlab git data repository viktorbodrogi remote upstream no space leave lock ref ref remote upstream mediarefactor unable create opt gitlab git data repository viktorbodrogi remote upstream no space leave lock ref ref remote upstream old stable unable create opt gitlab git data repository viktorbodrogi remote upstream old no space leave lock ref ref remote upstream pagetool unable create opt gitlab git data repository viktorbodrogi remote upstream no space leave lock ref ref remote upstream unable create opt gitlab git data repository viktorbodrogi remote upstream no space leave lock ref ref remote upstream phpunit fix unable create opt gitlab git data repository viktorbodrogi remote upstream phpunit no space leave lock ref ref remote upstream retrytest unable create opt gitlab git data repository viktorbodrogi remote upstream no space leave lock ref ref remote upstream scrutinizer unable create opt gitlab git data repository viktorbodrogi remote upstream scrutinizer no space leave lock ref ref remote upstream stable unable create opt gitlab git data repository viktorbodrogi remote upstream no space leave lock ref ref remote upstream unable create opt gitlab git data repository viktorbodrogi remote upstream no space leave lock ref ref remote upstream travis unable create opt gitlab git data repository viktorbodrogi remote upstream no space leave lock ref ref remote upstream versionfixtool unable create opt gitlab git data repository viktorbodrogi remote upstream no space leave error lock ref ref remote upstream master unable create opt gitlab git data repository boost remote upstream no space leave nil nil nil nil nil nil nil nil repository path nil import time import take long second nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil import time import take long second nil fatal read username terminal prompt nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil repository path undefined method nil nilclass undefined method nil nilclass repository path repository path repository path repository path repository path repository path repository path repository path repository path repository path repository path repository path undefined method nil nilclass undefined method nil nilclass undefined method nil nilclass undefined method nil nilclass undefined method nil nilclass undefined method nil nilclass undefined method nil nilclass undefined method nil nilclass undefined method nil nilclass this indicate disk file node staging i clear set specify runbook alert clear
1589,15405079,1.0,ethan strike access per
1590,15404915,1.0,transfer domain gitlab transfer soon purchase transfer this email reference domain cc reference
1591,15402187,1.0,redirect handbook product ceo pricing we move page previously host product ceo i appreciate help set redirect catch traffic point old url see reference this redundant request i think i open issue i i go search open closed issue apology advance case thank
1592,15381864,8.0,get ephemeral environments ready postgre work the environments project nearly ready use deploy arbitrary temporary environment this task minimal change necessary project usable deploy arbitrary environment the intention use temporary workspace work chef recipe provision postgres patroni clustering in near term migrate terraform code versione module this implement iteration design document
1593,15369359,1.0,redirect setting gitlab ci android last week publish update expand version old blog post need redirect link old post new post link redirect uri thanks
1594,15364868,1.0,stackdriver pubsub metric stop report
1595,15290981,1.0,import big project customer customers potential one want import large project this isssue provide workaround timely manner background in past big import import dedicated instance help achieve large import timeout timeout hour kill sidekiq memory killer this bit frequent app grow large sidekiq process eat memory we improve import mechanism use memory cost get bit slow execute transaction single commit keep object memory this tweak way hit memory issue timeout problem the big step separate different independent worker order save memory small refactor why this help support sale current workaround for self host instance easy workaround increase sidekiq rss memory allow disable worker kill import certain timeout reach for tweak so normally rid pipeline heavy object export project order free bit memory but encounter problem example i manage import mb export do instance average rss mb peak mb note practically half load rails app the default max rss gb but work what difference the thread import run do instance share thread thing process thread free proposal for prospect customer use deploy node run script memory limit normally hit we need james provide script call i e logic provide export archive target easy james document work we need use tmux screen infra confirm ok i suggest time ping oncall support confirm identity customer verify access target namespace support ping oncall require customer alternative maybe automate hook chatops use runner memory thinking but thought relate cc
1596,15275488,3.0,ephemeral environments demo video rather try schedule bunch zoom call ci cd work ephemeral environments i create video watch convenient i issue place gather feedback please comment youtube environments environments a thing note this initial iteration context build poc environment patroni useful easy way experiment kubernete thing the worflow demo account desire git workflow branch create test environment issue merge request appropriate place conversation the code video skim visible curious actually happen pipeline output we use lot discussion work the design doc issue merge request pende comment this project probably need workspace op instance click reaction i track watch
1597,15273047,1.0,aw admin access staging new sre per comment new sre access staging aws account account id please create iam user account assign necessary able manage account
1598,15270446,2.0,staging run disk space sda the keep fill delete file lsof grep delete kill linger process remove need restart postgre great idea be ignore alert
1599,15237984,1.0,remove workaround send param elasticsearch currently strip param field indexing issue elasticsearch this issue remove workaround fix make release put milestone need push depend long take fix land
1600,15193724,1.0,aws access i think i access aw account create issue track this help pick aws access request issue go forward assign help
1601,15181465,1.0,consolidate pagerduty schedules right multiple schedule geographic region consolidate single schedule multiple layer region we clean consolidate reduce overhead ease understanding
1602,15181087,3.0,clean moved repos when use api migrate project storage node project old storage node get rename like the not clean old repos probably old move repos lie we audit repos remove
1603,15180368,5.0,rebalance repository storage shard currently storage shard i think consider rebalancing repos shard evenly spread ideally consideration
1604,15179022,2.0,create alert fail consul service check we postgresql service check pass consul postgre ha topology if definitely page oncall mean unable failover properly failure
1605,15178924,2.0,postgresql service check fail production it look like fail time what mean probably go succeed failover put database ha topology risk the consul definition postgresql service gitlab bin gitlab ctl repmgr check ha postgresql gitlab bin gitlab ctl consul watcher handle fail master production sh gitlab bin gitlab ctl repmgr check master bash gitlab bin gitlab ctl repmgr check master gitlab embed bin ruby gitlab embed bin omnibus ctl gitlab gitlab embed service omnibus ctl repmgr check master sh gitlab bin gitlab ctl repmgr check master bash gitlab bin gitlab ctl repmgr check master gitlab embed bin ruby gitlab embed bin omnibus ctl gitlab gitlab embed service omnibus ctl repmgr check master sh gitlab bin gitlab ctl repmgr check master bash gitlab bin gitlab ctl repmgr check master gitlab embed bin ruby gitlab embed bin omnibus ctl gitlab gitlab embed service omnibus ctl repmgr check master root grep repmgr check master it appear command hang production credential correct problem command jarv sudo gitlab consul gitlab embed bin psql opt gitlab postgresql gitlab consul change directory jarv permission deny psql type help help
1606,15159893,4.0,refactor pagerduty cog functions gitlab chatops there little sense put effort rewrite cog pagerduty function use require give change gem api abandonment cog rather refactor function need dispose need write format commutable ci drive chatops current requirements match functionality x it accept bare request return service x it accept service partial match return service
1607,15132778,1.0,investigation new haproxy drain deployment the new haproxy drain logic time appear successful normal spike error deployment there anomolie investigate follow these traffic change relate healthcheck account large workhorse backend traffic the spike api traffic bit mystery traffic spike api dip workhorse web traffic web web
1608,15116196,2.0,redirect partner previously redirect path fix path we need place deployment coordinate create chef mr x remove x create x create integrate deploy list x stop chef client x merge chef mr x merge website mr x when website mr close deploy start chef client deploy
1609,15116140,1.0,update comparison the rule update
1610,15098617,3.0,improve log content gitlab pages production incident it difficult look request log pages incident get hostname path remote ip request log ship
1611,15098019,2.0,create terraform setup load balancer install the load balancer rw read write traffic ro read traffic the technology suggest gcp ilb with know ip rw traffic ro traffic health configuration
1612,15097962,4.0,create chef cookbook patroni install setup install package setup node add actual parameter production cluster setup monitor metric new postgresql instance duplicate actual one define rule autofailover create pgbouncer user create mechanism restart case service die runit systemd create monitoring service node health status create process setup node get datum prod cookbook could could e wally create process create patroni node cookbook pgbouncer add actual parameter production cluster create mechanism restart case service die runit initd setup monitor metric new pgbouncer instance duplicate actual one create check cronjob updage access new user connect pgbouncer avoid direct connection authquery update watcher script consul update pgbouncer entry point pgobuncer master database the ongre document install
1613,15097912,1.0,create chef cookbook consul agent install setup install standard version consul agent create mechanism restart case service die runit initd add monitor metric consul agent health generate config connect agent master need
1614,15097887,1.0,create chef cookbook consul server install install standard version consul server create mechanism restart case service die runit initd initially setup cluster autospawn instance create server consider consensus consul add monitor metric consul server health usage vip mean ip reserve in gcp use static internal ip seem need setup startup server the node consul bootstrap good practice fix ip explore instance group make vital iteration can use terraform consul module
1615,15097847,1.0,create chef cookbook pgbouncer install install standard package pgbouncer community version production will instal box postgresql
1616,15097793,3.0,create chef cookbook postgresql install install standard package postgresql community version production create postgres user os could use image
1617,15088152,2.0,workaround github importer bug apply database level fix affect project context
1618,15087137,8.0,database reviews x x x x x x x x x x x x x x x x x x x x x x x x x x x x monday kamil x monday kamil x
1619,15065301,2.0,document setup patroni consider postgresql cluster we need create chef cookbook setup patroni environment please list step
1620,15065336,1.0,create project create minimun cluster patroni poc please create project gcp allow ongre generate poc patroni please minimize usage resourece possible
1622,15042790,2.0,github url redirect a mistake i line github com vs tool github com vs should github com vs tool github vs fix
1623,15033485,1.0,get package omnibus download count september please obtain package omnibus download count month september instruction usage statistic sheet show goal cell thank lot
1624,15012329,4.0,add alert repository mirror github assign solve reassign case appropriate cc investigate relate recent release cc
1625,14999517,2.0,dynamically populate ha proxy backends we currently consul production node iteration i propose publish service web fleet dynamically update ha proxy backend base service availability
1626,14998558,3.0,gitter ssl certificates need renewal gitter use wildcard certificate this expire november cc
1627,14971954,1.0,set redirect people operation group conversation we rename functional group updates fgus group conversations in merge request i update url people operations page group conversations attempt fix link break result i think set redirect case link point page i the page people operation functional group redirect people operation group
1628,14971594,1.0,admin access please enable admin access account ctbarrett
1629,23422847,2.0,delivery mttp metric define develop track mean time production mttp metric kpi infrastructure department scalability team iterate necessary update url health maturity next step handbook infrastructure performance indicators mttp page necessary
1630,23409359,3.0,update chef update chef client pre environment identify fix issue pre no issue identify deploy fix pre no fix require verify pre work deploy environment non breaking fix no fix require update chef client break fix environment first step
1631,23406508,3.0,rca gitaly latency slo please note incident relate sensitive datum security related consider label issue mark confidential incident gitlab com gl infra summary a file descriptor leak gitaly cause high resource consumption high latency gitaly operation host this cause high latency project host project stay unaffected some project severely block customer move node investigation affect gitaly team attribution gitaly minutes downtime degradation for calculate duration event use platform metrics dashboard look appdex slo violation impact metrics start following what impact incident very high gitaly latency project host who impact incident customer project big customer report issue how incident impact customer gitaly base operation like merge request slow how attempt access impact service feature how customer affect how customer try access impact service feature include additional metric relevance provide relevant graph help understand impact incident dynamic gitaly latency detection response start following how incident detect gitaly latency slo alert do alarming work expect gitaly latency slo alert pagerduty see eoc immediately alarming gitaly file descriptor count see issue day how long start incident detection customer report issue start fd leakage take day how long detection remediation m be issue response incident bastion host access service available relevant team memeber page able issue identify action root cause analysis some project suffer high latency gitaly relate operation like mr why because gitaly high latency project why because gitaly issue affect project host node why because gitaly process lot cpu resource why because gitaly hang cat file process why because gitaly leak file descriptor what go start follow identify thing work expect any additional out go particularly what improve alerting gitaly file descriptor make gitaly slo alert pagerduty we follow identify error day ago we clear escalation path support sre corrective action list issue create corrective action incident for issue include follow bare issue link issue label action include estimate date completion corrective action incldue name individual own delivery corrective action x prevent cat file leak x alert high gitaly file descriptor count x page gitaly slo alert guideline blameless rca guideline s
1632,23396890,5.0,rca job queue grow please note incident relate sensitive datum security related consider label issue mark confidential incident gitlab com gl infra summary ci job take long complete job sidekiq queue pile pipeline cause high sidekiq job sidekiq pipeline node maxe cpu job cause sql call pgbouncer pool sidekiq saturate rca doc affect sidekiq team attribution minute downtime degradation impact metrics start following what impact incident delay ci job who impact incident customer ci pipeline how incident impact customer prevent run ci test deploy how attempt access impact service feature how customer affect how customer try access impact service feature include additional metric relevance provide relevant graph help understand impact incident dynamic detection response start following how incident detect support report customer issue ci pipeline do alarming work expect get sidekiq alert pgbouncer saturation alert page we alert queue size clear indication issue how long start incident detection m queue start rise till alert sidekiq cpu how long detection remediation m be issue response incident bastion host access service available relevant team member page able eoc aware incident report customer support page alert it hard find help issue root cause analysis the purpose document understand reason cause incident create mechanism prevent recur future a root cause person way writing refer system context specific actor follow s blameless manner core root cause analysis for necessary start incident question happen keep iterate ask time while hard rule time help question deep find actual root cause keep min come answer consider follow different branch example usage s the vehicle start problem why the battery dead why the alternator function why the alternator belt break why the alternator belt useful service life replace why the vehicle maintain accord recommend service schedule fifth root cause what go start follow identify thing work expect any additional out go particularly what improve start follow use root cause analysis explain improve prevent happen be improve detection time detection be improve response time response be exist issue prevent incident reduce impact do indication knowledge incident place corrective action list issue create corrective action incident for issue include follow bare issue link issue label action include estimate date completion corrective action incldue name individual own delivery corrective action x increase cpu sidekiq nod review pgbouncer pool config x optimize pipelineprocessworker deduplicate sidekiq job x define sidekiq slo simplify sidekiq setup improve sidekiq observability prevent customer cause platform issue add client limit place guideline blameless rca guideline s
1633,23366845,3.0,rsyslog create system log file bootstrap notice building sidekiq nod initial bootstrap log syslog log exist rsyslog run it require restart rsyslog reboot create log this mean system log log locally ship elasticsearch manual activity occur i investigation idea simple complex fix
1634,23362896,1.0,chef client work runit do sudo chef client info fork chef instance converge start chef client version info chef info platform linux info chef client pid info run list debian gitlab org ssh git wale info run list expand gitlab plugin path gitlab gitlab utc gitlab gitlab check gitlab public ip gitlab public ip upgrade gitlab en gitlab client gitlab gitlab gitlab openssh chef client gitlab gitlab gitlab editor vim gitlab default gitlab iptable gitlab gitlab omnibus gitlab gitlab logind gitlab auto upgrade gitlab shell gitlab gitlab gitlab gitlab gitlab user info start chef run info run start handler info start handler complete info http request return not find info http request return not find info error report run start data collector url exception not find this normal chef automate resolve cookbook run list gitlab plugin path gitlab gitlab utc gitlab gitlab check gitlab public ip gitlab public ip upgrade gitlab en gitlab client gitlab gitlab gitlab openssh chef client gitlab gitlab gitlab editor vim gitlab default gitlab iptable gitlab gitlab omnibus gitlab gitlab logind gitlab auto upgrade gitlab shell gitlab gitlab gitlab gitlab gitlab user info http request return precondition fail satisfy constraint package runit solution constraint gitlab mtail solution constraint result constraint runit gitlab exporter runit gitlab mtail runit mtail packagecloud error resolving cookbooks run list cookbook dependency resolution error error message unable satisfy constraint package runit solution constraint gitlab mtail solution constraint result constraint runit gitlab exporter runit gitlab mtail runit you able resolve issue remove cookbook version depend delete cookbook remove unused cookbook version pin exact cookbook version environment expand run list gitlab plugin path gitlab gitlab utc gitlab gitlab check gitlab public ip gitlab public ip upgrade gitlab en gitlab client gitlab gitlab gitlab openssh chef client gitlab gitlab gitlab editor vim gitlab default gitlab iptable gitlab gitlab omnibus gitlab gitlab logind gitlab auto upgrade gitlab shell gitlab gitlab gitlab gitlab gitlab user platform linux running handler error run exception handler run handler complete error exception handler complete chef client fail resource update second fatal stacktrace dump chef cache chef fatal please provide content file file bug report error precondition fail fatal chef run process exit unsuccessfully exit code
1636,23350105,3.0,gitaly slo alert pagerduty gitaly latency slo alert go slack pagerduty we page alert
1637,23349885,3.0,alert gitaly file descriptor incident show alert file descriptor help detect incident early
1638,23328620,2.0,setup update terraform admin project follow slight modification pattern i propose utilize env zero project central place manage project permission service account automation as modification instead single admin level service account provision resource project create service account project environment permission scope allow access resource project once project bootstrappe relevant credential add environment specific ci variable automate terraform run project however i sure possible project specific service account terraform admin project project manage this issue track research discovery effort implement type setup include documentation bootstrap script work circular dependency project manage terraform possibly relocate consolidate state file manage child project question rationale point scope project specific service account fyi discussion
1639,23326253,1.0,rca deep dive perform walk incident dna meeting wednesday july
1640,23321292,2.0,postgres relate service nearing capacity caplan dashaboard postgre relate service nearing capacity caplan dashboard please investigate fix dashboard create issue address specific caplan concern ongre aware caplan metric calculate help link say issue issue tag obserbavility caplan
1641,23312022,2.0,move gstg wal we decide discontinue test wal g staging hinder ability test change wal e release production background from high level perspective transition use wal g staging beginning we hope able use production alleviate pressure primary wal g capable take basebackup secondary wal e however keep run wal g issue gcs particular keep wait wal g release wal g production the task actually gstg use wal e x reconfigure gstg wal e x deploy change daily backup wal push x dr replicas like resync use wal e go not need transition gapless let catch x have backup testing project use wal e staging
1642,23292414,3.0,use native instrumentation camoproxy camo native prometheus support x enable monitoring prometheus scrape config x cleanup textfile exporter x remove obsolete flag gitlab camoproxy cookbook x add metric upstream camo replace mtail metric x remove mtail watcher x create grafana dashboard
1643,23284858,2.0,right size redi persistent persistent redi sidekiq node the redi sidekiq node size previous redi persistent node safety initial migration this give cpu gb ram all redi node generally need cpu redi thing sizing ram redis redi sidekiq node currently gb res virt gb general cache gb ram total and sufficient on old redis persistent nodes running system gb gb general cache we able away gb ram i confident like usage decide
1644,23271205,3.0,performance metric per need produce performance metric reflect performance gitlab experience user something line outline service levels error budgets blueprint let possibly rough iteration pingdom thousand eye in particular start sli outline blueprint please update infrastructure performance indicators handbook page reflect health maturity url datum
1645,23174740,1.0,postgre dr db gstg replication break alertname channel database env gstg environment gstg fqdn note replication lag server currently m from log error archive exist error archive exist
1646,23167854,3.0,create runbook documentation type access request the exist how to guide severely lacking we need better documentation runbook
1647,23166908,5.0,enable external merge request diff storage on big table tb currently grow rate gb month we look enable external merge request diff storage point update the current size relevant table gb gb total database size total database size single table once migration datum live outside database reduce database size
1648,23755565,3.0,upgrade verify host run kernel os haproxy host after read i like box configure late version run underlying os kernel
1649,23136657,1.0,delete cert sslmate it auto renew from sslmate support to op notification subject sslmate certificates your certificate ready use anymore replace we need stop auto renew let expire gracefully
1650,23136455,2.0,work while work gitlab cookbooks gitlab i encounter gitlab recipe fail kitchen setup unable figure i go production mailroom box encounter issue level info listen mailbox level fatal tcp bind address use mailbox the production log log prometheus current error point exporter serve datum curl hang i recipe code
1651,23135310,3.0,anomaly base detection sidekiq queue size follow alert specific label generically give queue exceed it use anomaly detection technique establish recording rule rule alert rule general service flexible notice unusual volume queue incident try manage hard code limit
1652,23133138,4.0,update runit cookbook one error encounter testing chef client runit cookbook we update notice accord changelog version require chef this mean cookbook update update need hold deployment roll chef accord chef repo follow cookbook pin runit need update gitlab alertmanager gitlab camoproxy gitlab elk gitlab exporter gitlab monitor gitlab mtail gitlab prometheus ideally mr add rspec example miss include recipe use kitchen sure update break part
1653,23130461,2.0,plan in lieu automating access request issue access request source sre sre manager issue manually ingest approve blindly after requestor manager approve issue sre manager way know requestor grant access if issue audit trail i long comfortable add approval and continue process challenge i propose remove language altogether infrastructure manager for request involve access critical infrastructure system additional layer approval require review request approve copy paste longterm i expect roll role base access okta iam but regardless outcome aforementione approval process sre focus hamper access request short fully automate workflow i advocate follow option process request batch type start week document process instruct requestor submit merge request cookbook cc
1654,23118017,1.0,the late version prometheus operator break upgrade due lock version component in pre environment run follow test locally upgrade fail error gitlab monitor kube state metric invalid invalid value state metric field immutable error upgrade fail gitlab monitor kube state metric invalid invalid value state metric field immutable stopping tiller error plugin tiller exit error monitor master helm search stable prometheus operator name chart version app version description stable prometheus operator provide easy monitoring definition kubernetes servi monitor master this upgrade version currently run gstg use issue figure go need accomplish cleanly upgrade gstg downtime this utilize practice run determine investigate perform clean upgrade version component manage
1655,23108892,2.0,cleanup postgres runbook some postgres runbook reference repmgr let review runbook bring date
1656,23108666,2.0,google network failure detection we number failure google network recently during rca deep dive bring idea investigate determine health network look aggregate network traffic look significant deviation if help well troubleshoot track root cause
1657,23081589,1.0,rca deep dive perform walkthrough incident dna meeting please familiarize incident rca this issue process troubleshoot it great example sre execute task normally execute dbre
1658,23065560,5.0,make gke node pool production ready i notice thing go think terraform manage mainly probably go use multiple node pool multiple cluster production instance any change node disk space etc result entire node pool delete create include run pod it rolling way like expect this issue track investigate way roll update need find workaround run change downtime
1659,23055856,2.0,review postgres failover week we failover activity week let ongre review relevant failover datum report case
1660,23055597,1.0,rename scope team labels to work appropriate manager board need rename scope label i propose use
1661,23050307,2.0,availability mttr metric define develop track mean time recover mttr metric kpi infrastructure department iterate necessary update url health maturity next step handbook infrastructure performance indicators page necessary epic
1662,23050298,2.0,observability mttd metric define develop track mean time detect mttd metric kpi infrastructure department iterate necessary update url health maturity next step handbook infrastructure performance indicators page necessary
1663,23050278,2.0,reliability mtbf metric define develop track mean time failure mtbf metric kpi infrastructure department site reliability engineering team iterate necessary update url health maturity next step handbook infrastructure performance indicators page necessary
1664,23045213,2.0,rebuild instance after close leave shut unused instance we clean following rebuild remove cluster just terminate delete leave gap instance sequence tf cope
1665,23033031,2.0,more cpu sidekiq pipeline node update there intermittent report lately pipeline run timely fashion i believe cpu contention pipeline sidekiq node peak consider give node cpu original there intermittent report lately pipeline run timely fashion i believe increase load pipeline sidekiq nod expand group
1666,23010576,4.0,dr site patroni wo stay replicated the runbook resynchronize dr site database master wal replication work it take day end replicate however stay synchronize we manually resync week this work live it need stay replicated this time look like go patroni cluster member host role state tl lag mb pg ha cluster start unknown pg ha cluster start unknown pg ha cluster start unknown ongre familiar database set look recommendation
1667,23005476,1.0,cleanup dns information runbooks we stale runbook reference powerdns apparently long use discover gitlab com gl infra we need simply include information external dns comment
1668,23004685,1.0,rca deep dive gitlab com gl infra perform walk incident dna meeting please familiarize incident rca this issue process troubleshoot it great example illustrative successful collaboration development infrastructure overlap call dbre sre execute corrective action also timezone split issue cover emea americas timezone
1669,23004148,2.0,propose label cleanup i clean house delete old label issue close update long year ago my list remain issue i sure table name total number issues remove priority
1670,23002661,1.0,need elastic ip the aws instance host change ip restart stop start it elastic ip address prevent cause long outage dns manually update this issue case problem occur
1671,23002291,1.0,investigate recent demo show young pods know reason in recent demo progress implement kubernetes container registry realize pod run registry young utilize issue track create necessary documentation troubleshooting purpose
1672,23002140,3.0,kubernetes deploy perform set forget style failure when test upgrade kubernetes application deployment go fine helm think deployment successful new replicaset create come properly the pod start stick crashloop with use case deployment technically successful require roll it wise add deployment pipeline detect failure fail pipeline instance
1673,23002061,3.0,version component expose prometheus deploy kubernetes currently visibility metric version component run kubernetes we run version container registry i find this useful find metric specific version change watch upgrade roll back run cluster increase visibility run give time
1674,23000711,3.0,on fail registry upgrade downgrade alert pod fail come downgrade registry version alert k pod gitlab name ready status restarts age gitlab run gitlab complete gitlab nginx ingress t run m gitlab nginx ingress run gitlab nginx ingress run m gitlab nginx ingress default blzzq run gitlab nginx ingress default run m gitlab run m gitlab run m gitlab htttg crashloopbackoff
1675,22974838,2.0,rca elevated git error rate summary increase error rate gitaly large file upload single user affect gitaly team attribution minute downtime degradation for calculate duration event use platform metrics dashboard look appdex slo violation impact metrics start following what impact incident partial service degradation who impact incident external customer how incident impact customer prevent x incorrect display y how attempt access impact service feature how customer affect how customer try access impact service feature include additional metric relevance provide relevant graph help understand impact incident dynamic detection response start following how incident detect alertmanager pagerduty notification do alarming work expect yes how long start incident detection approx minute how long detection remediation approx minute be issue response incident no timeline utc receive alert increase error rate gitaly stor gprd utc confirm error operation user identify utc eoc initiate user block utc load average stor gprd begin drop utc alert gitaly error rate high clear utc node level alert gitaly error rate high stor gprd clear glance this duplicate recurrence root cause analysis gitaly service unresponsive why gitaly consume cpu stor gprd why gitaly sshreceivepack sshuploadpack process servicing request consume cpu resource why a user attempt upload large gb ssh what go alert allow identify root problem quickly on engineer work rotation handle respond multiple incident support engage interface affected customer request configure git lfs well handling large file what improve we need well understand operation single user able consume resource single gitaly shard corrective action do implement facility gitaly abort git operation long time threshold investigate resource constraint mechanism like cgroup limit resource git operation consume guideline blameless rca guideline s
1676,22917311,1.0,schedule rca deep dives please schedule rca deep dive dna meeting to create issue deep dive assign team member assign date match dna meeting link relevant production incident incident rca issue link deep dive issue following epic
1677,22860599,3.0,discussion what practice adopt help train new caller goal this pitch adopt structured learning path new gitlab team member sre non sre incrementally accumulate knowledge necessary successfully field issue in context structured learning path mean gradually build work knowledge system maintain begin immediately applicable knowledge iteratively expand breadth depth specific learning objective stage student able use learn answer question component interact step diagnose scenario x help investigate fix component lot folk organization experience witness work personally i feel strongly cater different learning style essential kind inclusive current future teammate i love folk contribute idea experience opinion practice adopt helpful support new caller ideally promote continuous learning benefit tenure caller starter material there way organize existing new training material whatever method choose clear progression introductory advanced topic explicitly state learn outcome tie overarching theme support duty some example intro level learn objective list major component service stack briefly describe role component learn objective know basic usage observability tool understand kind question answer describe scenario tool provide useful information learn objective know find help troubleshoot problem component scenario familiar where find list subject matter expert intermediate level learn objective know use rail console specifically gitlab object model learn objective walk curate historical postmortem gain familiarity common pattern troubleshoot stack include application regression infrastructure failure become acquaint tool tactic diagnose fix regression style collaboration engagement peer troubleshooting remediation phase learn objective what normal look like component level view vantage point use troubleshooting grafana dashboard host level command line tool perf netstat etc rail console psql redis cli etc advanced level learn objective what abnormal look like limit scope choose area focus know common critical known failure mode service component what symptom effect remedy failure scenario learn objective which component architecture currently fragile which singleton which service lack graceful failover how affect slo recovery time reliably low where quickly find document recovery procedure critical component learn objective know scale service tier terraform know infer effect helpful harmful harmful identify critical downstream bottleneck
1678,22849556,1.0,when start cluster k ctl check object non existent namespace one check k ctl validate get secret load environment this work attempt install time completely clean cluster one namespace exist fail bail script point secondly order install need certificate however namespace instal helm to work need skip validation and create namespace drop secret perform install this document bit confusing utilize issue come well way handle cyclic dependency
1679,22820679,3.0,logrotate break i get page today disk space upon investigation log datum year we send log i want delete because i clear root we need revisit log retention fix logrotate
1680,22812532,1.0,add subdomain instance we need subdomain create instance the ip find dev resource job output lb see
1681,22770473,5.0,setup network dns infra vault x choose free subnet avoid ip collision envs x setup vpc peer envs access vault x setup dns x disable external ip vault
1682,22745634,1.0,proposal adopt rabbitmq proposal adopt rabbitmq i like propose solution long term that solution option implement timeframe nearer term interim solution option intend buy long runway application scope sharding this proposal subject current status quo application use sidekiq framework asynchronous task process redi cache fifo queue lock mechanic as software developer year i experience consequence inter system inter service bottleneck introduce misuse software pattern framework of course intention good case software pattern implementation test work nearer term small scale simplistic naive test scenario however system engineer know application usage profile grow adherence application software development good practice drift code module feature release commit thing complicated sometimes software framework pattern employ default configuration seldom adjust suit application usage sometimes solution simply design feasibly certain scale rabbitmq sidekiq approach cluster redis include single node multi process distribute request proxy load balancing multi node single process distribute request load balancing multi node multi process node distribute request load balancing as far i know maybe multi node single process distribute request load balancing additional node clustered worker node perform load sharing failover node replicate datum furthermore unlikely read request get properly distribute exclusively failover node instead equally distribute node overburden master redis node solely responsible write request on hand adopt write usage master node cause failure variety application usage update miss setnx operation such concern complicate good practice pattern sidekiq framework configure task prioritization sidekiq troublesome for instance phil sturgeon point define multiple queue sidekiq config distribute work evenly nov with rabbitmq work broker send parametric info task appropriate background process asynchronous work profile distribute message categorical pool asynchronous worker cpu load automatically balance core design rabbitmq multi thread erlang platform build service require concurrency make horizontal scaling simple configuration deployment perspective this case redis build single thread single process application design this design adequate commonly market use case scale horizontally somewhat complicated explain support microservice architecture message broker technology like rabbitmq kafka support architectural transition microservice i understand gitlab head future martin fowler plenty microservice trade offs another good resource read distribute service communication coverage conventional communication technique like rpi the away microservice architectural pattern commonly accompany message broker technology technology like sidekiq certainly place distribute inter service communication orchestration place so distribute architecture star gitlab opportunity begin lay groundwork transition my intention cultivate discussion singular problem present sidekiq appear horizontal scaling sidekiq redis challenging compare option like rabbitmq rabbitmq sneakers sneakers ruby background job processing framework use rabbitmq nor i allow single point failure redis suitable highly available background processing framework i lose message worth mention redis swiss army knife pry die corpse dotan nahum jan additional advantage sneakers compare sidekiq use case i need a great perform framework limit broker speed s acknowledge persist large sneakers that use core a highly available processing framework guarantee rabbitmq offer great a familiar dsl api support advanced messaging semantic reject requeue acknowledge etc that expose gut amqp and irrelevant comparison sidekiq background processing framework i need it use ruby care content gem run c extension mri a production ready package hold allow lazy possible metric logging bake convenient deployment maintenance supervision story dotan nahum additional reading message rail part rabbitmq rabbitmq scheduling messages sneakers additional sharding prioritization queue solution like rabbitmq provide support application feature like programmatic sharding job criticality level domain rabbitmq class queue channel instead redi lock pattern contentious susceptible inconsistency error fail over another core concept job framework queue a typical app dozen queue critical default webhooks low import payment etc developer choose job as set queue mix priority base queue critical default low domain specific queue webhook import payment kir shatrov jan additional reading the state background jobs summary to summarize limited research experience inform complicated design problem scale redi rely lock acquisition mechanic asynchronous job queue it require sophisticated coordination good architectural deployment pattern programmatic adherence certain application software pattern order ensure reliability horizontal scalability personally i suspect gitlab outgrow redi back sidekiq extent i recommend incorporate rabbitmq available ruby library framework sneakers bunny gitlab rails deployment configuration necessary change codebase order interface appropriately consider queue invoke categorical asynchronous task reserve redis use purpose cache simplistic session token frequently read infrequently modify application datum falsifiability my suspicion certainly mistaken i grokke number involve profile redi usage application it currently difficult thing present usage somewhat functionally overload redis cluster this probably remedie issue soon clear picture usage dominate bandwidth exist redis system resource it split redis sidekiq redis persistent complete obvious mal pattern illuminate eliminate plenty room grow use case message broker service obviate related issue tagging team member visibility feedback solicitation
1683,22710228,1.0,shutdown instance staging we go pause postgre upgrade effort little we stop instance staging need save db gstg db gstg db gstg db gstg db gstg db gstg
1684,22701083,2.0,postgresql minor upgrade we minor version late release postgresql perform minor upgrade minor upgrade generally safe incur long downtime need restart postgre process that mean rolling upgrade restart cluster minor upgrade usually fix bug non breaking improvement changelog
1685,22674953,5.0,proposal simplify sidekiq worker pool require spawn currently number different sidekiq priority queue its unclear key differentiator different queue i assume base throughput example realtime high priority short job besteffort low priority long run job appear case example task upwards hour run realtime queue once job assign priority queue process fleet sidekiq worker dedicate queue for example sidekiq fleet realtime besteffort etc if look thing machine level node run set sidekiq worker process worker set thread handle job at point surprise each process different number worker thread process each process handle different set job queue git ruby gitlab embed service gitlab rail ee bin sidekiq cluster production gitlab embed service gitlab rail merge merge merge merge merge git sidekiq queue merge busy git sidekiq queue busy git sidekiq queue busy git sidekiq queue busy git sidekiq queue busy git sidekiq queue busy git sidekiq queue merge busy git sidekiq queue merge busy git sidekiq queue busy this mean job saturate busy worker worker process fleet sit idle it mean need able manually monitor fleet constant manual adjustment unfortunately far i tell metric alert worker certain subset fleet busy instead reactively respond worker queue length start climb proposal i propose simple approach easy manage priority queue strictly base throughput requirement job latency each priority queue strict slo requirement latency if apdex particular job consistently meet require slo development team notify job de prioritised high latency queue each priority queue fleet present each worker process process job give priority queue subset each worker number thread this approach easy manage require manual adjustment if realtime queue keep job scale process if saturation worker thread fleet drop threshold certain period fleet scale this simple deal world strategy
1686,22647225,5.0,add thano query gke rest thano infra currently gke cluster thano query datasource grafana we connect op thanos query instance metric datum infrastructure x write ability add additional query target x wire
1687,22641309,1.0,dr vs gprd file server count difference currently file server dr gprd file server this break geo feature use issue discuss miss server dr environment bolster documentation ensure scale file server occur dr we consider alert situation like way recover datum
1688,22632250,1.0,ensure zpool cleanly reimporte instance destroy recreate if i gcp destroy zfs back storage node use terraform reprovision confident correct disk reattache zpool reimporte chef run if fix we know zpool import relevant mount instance reboot
1689,22617197,1.0,register prometheus service consul necessary able dynamic inventory consul gitlab org release
1690,22616232,2.0,increase postgres dead tuple alert we see increase unactionable postgres alert shift this recover change behavior understand cause the alert dead tuples percentage day dead tuple rates day total dead tuples day autovacuum table day note day all metric
1691,22615000,1.0,renew ssl cert hello this automate message inform ssl certificate status page expire soon please login dashboard upload new certificate settings ssl tab
1692,22594425,1.0,do need provision git storage node overweighte zone region our git storage fleet provision generic stor terraform module provision node configure zone node configure region allocate zone region round robin in way file store node overweight configure zone propose break feature generic stor reason discuss mr implement require zone overweightness provision file store pool specify zone specify region round robin node allocation this issue aim address need simply provision regional pool round robin allocation overweight zone
1693,22557161,2.0,create consul server fleet op for op exclusive service alertmanager necessary
1694,22556971,2.0,register alertmanager service consul necessary able dynamic inventory consul gitlab org release
1695,22552517,5.0,import prometheus rule gke prometheus operator configuration we lot rule need import prometheus operator run gke
1696,22550222,5.0,consider removal nginx ingress gke container registry with recent implementation container registry gke go quick route utilize nginx ingress provide helm chart order quickly utilize this add thing complexity way haproxy transfer datum gke cluster add extra network hop really need current glb haproxy registry nodes propose glb haproxy nginx ingress pod ningx ordinary forward traffic with configuration let encrypt new external endpoint this add layer configuration potentially eliminate this make hard slowly roll traffic vm gke instead solution switch proposal configure container registry ingress expose container registry service internal static ip feed haproxy
1697,22499039,1.0,dead tuples stale replica these error intermittently flap additionally runbook lot information misleading date current environment postgresql dead tuple large the dead tuple ratio great postgresql replication slot fall the replication slot minimum transaction id transaction old this cause increase dead tuple primary this cause long run transaction master standby unused replication
1698,22498589,2.0,register haproxy service consul necessary able dynamic inventory consul gitlab org release
1699,22498216,5.0,git ssh error debug follow underlying issue resolve in particular regularity issue report suggest track instance specifically hopefully capture datum find source cause current status problem alleviate followup occur unicorn queue git end server
1700,22487647,1.0,gke prometheus store datum persistently our prometheus instance gke store datum persistently pod remove despite configure datum store week configuration prometheus pod remove datum old new pod lose the helm chart default emptydir instead disk we solve way add datum persistence prometheus operator finish issue i think pursue option exist potential pod disappear thano chance grab datum push datum cloud storage this replacement issue
1701,22483996,8.0,dry terraform module code with upgrade terraform complete availability hcl advantage new language feature reduce duplication terraform codebase particular module identify near identical module refactor generic sv group generic stor generic stor group create relate issue refactor work specific module create relate issue update version pin deploy change gitlab com infrastructure pipeline
1702,22453338,1.0,rca degraded performance redis cache overload please note incident relate sensitive datum security related consider label issue mark confidential summary since july utc see degraded performance elevate error web api delay ci job the imminent root cause turn maxe cpu redis cache primary expensive call redi cache application affect web team attribution minute downtime degradation m base web latency apdex impact metrics start following what impact incident degraded performance elevated error rate web api component delay ci job who impact incident all user emea business time how incident impact customer slow loading page error delay ci job pull mirror how attempt access impact service feature how customer affect how customer try access impact service feature include additional metric relevance provide relevant graph help understand impact incident dynamic detection response start following how incident detect pagerduty alert gitlabcomlatencywebcritical do alarming work expect yes how long start incident detection minute how long detection remediation patch eliminate heavy app config request redis be issue response incident bastion host access service available relevant team memeber page able detect redis cache slowly saturate early timeline utc connection queue unicorn worker latency rise web api utc pagerduty alert gitlabcomlatencywebcritical utc alert acknowledge sre utc job queue duration rise utc incident issue open utc incident open utc update we add worker utc new api web worker add lb utc support report stick ci job customer utc new incident issue open report delay ci runner utc tweet job share runner pick low rate appear stick utc update acknowledge ci pipeline delay utc incident issue close relate utc update continue investigate announce incident issue url utc additional worker remove reduce connection redi cache utc update status change monitor ci job catch utc update normal level utc incident resolve utc kernel update reboot redis utc unexpected failover redis utc redis kernel upgrade reboot utc unexpected failover redis utc patch eliminate application config request redis cache deploy utc cpu usage drop network s s metric improve utc redis kernel update reboot root cause analysis the web component slow response time why redis cache slow response time why redis cache saturate cpu why too heavy request redis application why miss awareness test expensive redis cache request generate application what go alerting work get aware web performance issue immediately a lot support engineering find root cause work remediation what improve detection redis performance issue generally detect saturation service system trend analysis capacity plan find root cause performance degradation followup degradation resolve self direct root cause sight indication deep issue trend corrective action list issue create corrective action incident for issue include follow bare issue link issue label action include estimate date completion corrective action incldue name individual own delivery corrective action x start monitor saturation metric add service slo x distribute trace instrumentation rail cache x distribute trace instrumentation redis call discuss add style limit redis call development testing environment issue discuss add size limit redis key store cache issue x stop cache junit file redis monitor cache misuse redis application team redi cli add field structure log issue x add documentation monitor redis instance x consider break redis instance current persistent cache pair example ci cache mergerequest cache etc discuss possibility move redis cluster manage redis instance eg redis labs issue x use cache markdown field calculate participant x bandaid disable juint report x move flipper cache away redis memory cache x move geo check away redis memory cache x add redis detail peek performance bar compress payload big certain threshold x cleanup improve redis cache metric useful guideline blameless rca guideline s
1703,22441524,1.0,make redi cache instance reboot safe when reboot redi cache instance kernel upgrade redis process start self we need redis process reboot safe
1704,22438562,2.0,turn junit config upload junit artifact as effort alleviate saturation redis node relate go turn junit config upload junit artifact
1706,22425562,1.0,clean production severity label some incident issue production queue use sn label use label some use use we need clean use label ensure incident severity label
1707,22400153,1.0,chef run fail user databag format the check job fail find z f xargs bash jq null echo bad json exit find z nodes f xargs bash diff cat jq null echo you format file exit you format file user you format file user error job fail exit code
1708,22357954,8.0,apply vault terraform config ci cd for consistency testing keep vault deployment secret place apply terraform change vault gke cluster ci cd pipeline other job run ci keying add new pgp key repo trigger keying pgp encryption unseal key change basic vault config account role policy
1709,22354600,3.0,metric gke environment label apply unlike majority infrastructure apply env label metric capture inside gke this pose problem heavily rely label alert greatly w dashboard utilize issue track add label metric
1710,22354510,6.0,add gcp provider alternative dev resource deprecate do recently see increase random error dev resource past couple month it source frustration support team create instance reproduce issue handle interview i think do scale some error appear response do api we consider add gcp provider start migrate do maybe eventually deprecate relate
1711,22334879,2.0,camoproxy url blacklist tooling to implement url blacklisting propose
1712,22325891,3.0,gitlab com repo pull master branch chart page the gitlab com repo pull version specify env variable default master determine version chart want prior switch helm tiller rely version specify file setup repo pull specific version define provide file ensure file update match master deploy lately consider fact helm chart run gitlab page should page reason secondary place grab chart
1713,22325710,1.0,our ci images pull master helm component consider lock both installation helm helm tiller plugin late master build process consider pin version specific release prevent issue breakage
1714,22273197,3.0,build staging kubernetes cluster container registry utilize issue track progress create container registry staging environment keep update ensure fully document proceed the end goal issue registry traffic destine staging environment send inside gke platform
1715,22261446,2.0,upgrade gitlab package postgre dr host this propose action we employ dr replica run standard omnibus package however upgrade package regular deploy current version this rarely use codebase host care postgre far however especially dr context beneficial late codebase instal this allow easily run rails console talk delay replica example this helpful recover accidentally delete project it currently possible rails console old codebase start anymore instance the dr replica question gprd similar gstg postgre dr db gprd postgre dr db gprd
1716,22261178,1.0,start rail console read database session default this propose action we employ console host sv gprd people use start rails console ssh console user start console automatically excecute gitlab rail c host proposal by default rails console read database session this allow use rails console risk alter database datum accidentally when read write access necessary session promote read write console implementation in postgre database session set read set session characteristics as transaction read only when start rails console set attribute this possible configure accordingly postgre user dedicate console access default in order promote session read write simply set set session characteristics as transaction read write this ship codebase convenience
1717,22260511,8.0,validate establish backup strategy it possible slight time alignment consistency difference disk snapshot raidz array result umountable zfs dataset if restore raidz array gcp snapshot need propose alternative periodically ship zfs snapshot day git storage node zfs lose ability recover datum hour day ago cc
1718,22249328,2.0,replace use monitoring lb as soon slightly generic version monitoring lb prove work camoproxy fine prove circle replace use monitoring lb remove archive monitoring lb
1719,22246545,4.0,make possible whitelist ip api currently able whitelist ip rate limit api way site in order facilitate internal request whitelisting need implement way whitelist ip rate limit entire site
1720,22240869,1.0,registry alert numerous obvious problem resolve the sre receive alert indicate high number error registry service but obvious service problem registry serve request slowly etc repair the issue recover action take either alert error registry stringent legitimate issue registry need fix one criterion close issue do slo registry error threshold if current alert enforce level level high create merge address change alerting if alerting correct underlie issue create issue issue resolve problem
1721,22217251,4.0,create environment migration testing create easily creatable sandbox environment test gitaly node migration one back one back zfs ideally deliverable repository include terraform gitlab environment minimal gitaly shard need test initially gitaly shard use integration chef consider worth chef organisation script fill gitaly node dummy datum a readme explain set use tear use resource isolation possible separate gcp project
1722,22213445,6.0,blueprint encrypting tls internal gitaly traffic during work evident write blueprint design doc our work think advance the implementation document customer refer provide pin point iterate future will work mr
1723,22211442,8.0,service inventory catalog product background in deliver service inventory catalog design doc production app effort help centralize tribal knowledge production service reduce time search information reduce onboarding time benefit give service inventory catalog sic take step direction address problem common team organization company gitlab interest explore option solution product customer run similar need in believe individual team building requirement design ownership dependency documentation configuration runbook playbook security aspect document maintain sic aim find home information just like good practice version control project file service inventory catalog good practice objective the objective issue track work investigate feasible deliver sic product explore want deliver think perspective customer use scope effort identify dependency resource implement announce references service inventory catalog design doc service catalog yml file team yml file deploy version source code cc
1724,22194004,2.0,use common ci image workload gitlab com turn registry metric now common project ci image ci config utilize gitlab com this issue track follow x use common ci image run pipeline op x adapt pipeline similar job name apply update x adjust configuration start scrape metric
1725,22192027,2.0,alert redi process we alert redis process slave
1726,22183983,3.0,move gitlab cookbook group gl infra what transfer gitlab cookbook group subgroup gitlab com gl infra op why permission inheritance gl infra organisation how prepare mr chef repo update berkshelf reference use new subgroup path move group move group op merge chef repo mr notify infrastructure team people update remote this issue think idea iirc discuss have i miss consideration section
1727,22171958,2.0,fix remove redis role portion shell prompt i notice custom shell prompt break redis server the prompt show string that string set local bin redis host try determine current role primary secondary local redis instance connect instance unix socket gitlab embed bin redis cli opt gitlab redi role null echo return fi master echo primary redis echo secondary redi fi however redis instance currently configure accept unix socket connection in opt gitlab redis set unixsocketperm unixsocket also redis configure require authentication requirepass ask sre dbre team fix remove portion shell prompt if decide fix helper script local bin successfully open redis cli session that follow pattern use psql access postgres host we need decide enable unix socket listener use tcp listener tcp simple fix require bounce redis
1728,22146023,4.0,investigate api whitelist work we add whitelist entry chef apply reload work i spend investigate today i decide issue as process try restart haproxy resolve issue otherwise i pretty stumped
